Big Data

1. 01/06/2025 - Introduction
    - Course Objectives
        - Big Data Concepts
        - Technologies
        - Processing and Techniques
    - What is Data Mining?
        - Big Data - no standard term
        - Generators - almost everyone and every device
        - Dumo - Data has changed over time
        - Goal - extract knowledge and model from data
        - Goal - discover patterns and models that are valid, useful, unexpected, and understandable
    - Three V
        - Volume - size (gigabytes, terabytes, petabytes)
        - Velocity - speed
        - Variety - types
    - Directions in Modeling
        - Statistical Modeling - describes the distribution of data e.g. Gaussian is mean and standard deviation
        - Predictive Methods - use some variabels to predict unknown or futrue values of other variables
        - Descriptive Methods - find human interpretable patterns that describe data
            - PageRank - represent each website with a score, if a website has many hyperlinks to it, the score increases
            - Clustering - create groups of clusters of data points, with each cluster revealing something about the group
        - Feature Based Model - look for the most extreme example of phenomen and represent the data by these items
            - Frequent Itemsets - makes sense for data that consists of baskets of small sets of items e.g. market basket
            - Similar Items - find pair of sets that have a relatively large fraction of their elements in common
    - Machine Learning Classification
        - Topic - movies and tv shows
        - Data - Instance - Record - Entry - Data Point
        - Features - length, genre, seasons
        - Goal - map rows and columns into open space, where every column is an axis in your space, orthogonal to each other
        - Decision Boundary - separation between classifications based on features
        - Prediction - input feature values and map them to the open space and then based on decision boundary return classification
        - Training Set - produces decision boundary
        - Test Set - unknown data that will be classified
        - Overfit - performance on test set is poor, and performance on training set is extremely high
    - Analytic Answers
        - Total Information Awarenmess - a plan to mine all data, including credit card receipts, hotel records, travel data, and many other
        - Bonferroni's Principle - if you look in more places for interesting patterns than your amount of data will support, you are bound to find crap
        - Example
            - Goal - we want to find unrelated people who on two different days were both at the same hotel
            - 10^9 peopple being tracked i.e. 10,000,000,000
            - 1,000 days are being reviewed
            - Each person stays in a hotel 1% of the time
            - A hotel holds 100 people
            - There are 10^5 hotels
            - What is the probability that a person stays in a hotel on any day? 0.01
            - What is the probability that two people stay in a hotel on any day? 0.01 * 0.01 = 0.0001
            - What is the probability that these two people stay in the same hotel on any day?  0.0001 / 10^5 = 1x10^-9
            - What is the probability that these two people stay in the same hotel on two days? 10^-9 squared which is 10^-18
            - n chose m, n days how many m combinations? (n! / (n-m)!m!)
                - m is 2 that is two people
                - n! / (n-2)!2!
                - n(n-1)(n-2)(n-3).../(n-2)(n-3)....*2
                - n^2-n/2
                - n^2/2
                - (10^9)^2/2 = 5*10^17
                - (10^3)^2/2 - 5*10^5
                - Goal - number of possible combinuations of two days, times number of possible combinations of two people, times probability
            - 250,000 suspicious pairs of people
    - Things Useful to Know
        - TF.IDF - the formal measure of how concentrated into relatively few documents are the occurences of a given word - if you see how frequent a word is e.g. the or a.
        - Inverse Document Frequency
            - Frequency TF - for word I in document j, frequence of i in document j divided by max kfkj i.e. most frequent word in the document 
            - IDF - log(total number of documents divided by how many documents have the word i in them)
            - For the article  "the", T.IDF should be high, logn(1) is 0, not important
        - Hash Functions - takes a hash key value as an argument and produces a bucket number, which is an integer. normally in the range 0 to B-1, where B is the number of buckets
            - What is the ideal value of B? if all even numbers, the bucket is the same as results are 0
            - Prime number, which cannot be factored, are ideal for number of buckets
        - Base E of Natural Logarithms (Review)
        - Power Laws (Review)

2. 01/13/2025 - Hadoop
    - Things Useful to Know
        - Power Laws - linear relationship between logarithms of two variables, if x is the horizontal axis and y is the vertical axis, then the relationship is log base 10 y = 6-2 log base 10x
            - X axis - rank of the book, Y axis - number of sales
            - How to find the formula?
                - 1a. Determine slope, delta y over delta x (y=ax+b) [LINEAR]
                - 1b. Determine slope, log y = alogx + b, log delta y over log delta x [LOGARITHMIC]
                    - log(1) - log(1,000,000) / log(1000) - log(1)
                    - 0 - 6 / 3 - 0 = -6/3 = -2
                - 1c. Determine y Intercept b
                    - logy = -2logx + b
                    - log(1) = -2log(1000) + b
                    - 0 = -2(3) + b
                    - 0 = -6 + b
                    - 6 = b
                - 1d. Form Equation
                    - logy = -2logx + 6
                - 1e. Formula is linear relationship between logarithms of two variables
            - How do we take advantage of e to get the actual relationship
                - e to the power of log y = e to the power of (-2logx + 6)
                - y = e to the power of -2logx X e to the power of c (CONSTANT)
                - y = e to the power of logx to the power of -2 X e to the power of c
                - y = cx^-2
                - 1,000,000 = c(1)^-2
                - 1,000,000 = c
                - y = 10^6x^-2
    - Data
        - Challenges - usage, quality, context, streaming, scalability
        - Data Modalities - ontologies, structured, networks, text, multimedia, signals
        - Operators - collect, prepare, represent, model, reason, visualize
    - Data Mining Cultures
        - Databases - large scale data, simplex queries
        - Machine Learning - small data, complex models
        - Theory - randomized algorithms
        - Culture
            - DB - analytic processing i.e. queries that examine large amounts of data
            - ML - inference of models
    - Learning Objectives
        - Working with high dimensional data
            - Locality sensitive hashing, clutering, dimensionality reduction
        - Working with infinite/never ending data
            - Filtering data streams, web advertising, queries on streams
        - Working with labeled data
        - Working with MapReduce
        - Working with Data Streams
        - Working with Machine Learning
            - Support Vector Machine, decisions trees, perceptron, kNN
    - Applications and Tools
        - Recommender Systems
        - Market Basket Analysis
        - Duplicate Document Detection
        - Linear Algebra (Recommender System)
        - Dynamic Programming (Frequent ItemSets)
        - Hashing (LSH, Bloom Filters)
    - MapReduce and the New Software Stack
        - What is Hadoop? open source data storage and processing platform
        - Setting Up Hadoop
            - Hadoop Binaries - local, Cloudera's DEMO VM, Cloud
            - Data Storage - local filesystem or hadoop distributed file system, cloud
            - Map Reduce - local, cloud
            - Other Libraries and Tools - vendor tools, libraries
        - Philosophy
            - Objective - complete work on a large set of data
            - Actions
                - Divide work i.e. data across machines
                - Perform action on data
                - Combine results to produce final value
        - Architectures
            - Single Node Architecture - CPU, memory, and disk are on the same device, data loaded from disk to memory, algorithm is run against all data, results are written to storage
                - Issue(s) - What if you cannot store all data on disk? What if you cannot store all data in memory?
            - Clsuter Architecture
                - Rack - 16-64 nodes (CPU, memory, disk), 1Gbps between nodes
                - Switch - connects racks and other switches, 2-10 Gbps between racks
        - Motivation
            - Data - 20+ billion web pages, each web page is 20kb = 400TB of data
            - Read - if reading data 30-35MB/sec from disk it would take ~4 months to read the web
            - Today's Paradigm - cluster of nodes with ethernet connecting them
        - Large Scale Computing
            - Challenges
                - How do you distribute computation?
                - Which machine should do what work?
                - How do we write the code for this problem?
                - How do we deal with machine failures?
            - Idea and Solution
                - #1 - Redundancy for Reliability
                    - Issue - copying data over a network takes time
                    - Idea - bring computation close to the data, store files multiple times for reliability and redundancy, algorithm runs on this machine
                    - Solutions - MapReduce, Google File System, Hadoop Distributed File System                
                - #2 - Storage Infrastructure
                    - Issue - if nodes fail, how to store data persistently?
                    - Idea - Distributed File System i.e. global file name space
                    - Usage Pattern - huge files, data rarely updated in place, reads and appends are common
                    - Note - Hadoop isn't necessarily great for real time
                - #3 - Distributed File System
                    - Idea - Chunk Servers - file is split into contiguous chunks 16-64mb, each chunk is replicated, try to keep replicas in different racks, each chunk server is a compute server
                    - Idea - Master Node - stores metadata about where files are stored, might be replicated
                    - Idea - Client Library - talks to master to find chunk servers, connects directly to chunk servers to access data
                    - Why store on different racks? if a switch fails, access to no data stored on a particular rack
        - Programming Model: MapReduce
            - Example Task
                - Objective - count the number of times each distinct word appears in the file
                - Data - huge amount of data, assume all web pages
                - Case 1 - file is too large for memory, but all word,count pairs fit in memory
                - Case 2 - storing words and counts would be large, task is naturally parallelizable
            - Overview: MapReduce
                - Essential Actions
                    - READ DATA
                    - Map - extract something you care about (keys)
                    - GROUP BY KEY - sort and shuffle
                    - Reduce - aggregate, summarize, filter, or transform
                    - WRITE RESULT
                - Map - Function (YOU)
                    - Input Key Value Pairs - Document, key = file name, value = lines in file
                    - Output Intermediate Key Value Pairs - key = word, value = 1
                - Reduce - Function (YOU)
                    - Sort to Key Value Groups - the, the, the, game, game, game, name, name
                    - Reduce to Output Key Value Pairs - aggregate the values for each key i.e. the - 3, game - 3, name - 2
                - Other - System (HADOOP)
            - Overview #2: Map Reduce
                - Input - Set of Key Value Pairs
                - Map(k,v) - takes input key value pair and outputs a set of intermediate key-value pairs (k',v') (Key = Document, Value = Words)
                - Reduce - takes intermediate key-value pair (k', <v'>) and ouputs a a set of key value pairs (k', v'') (Key = Word, Value = 1)
                - Reduce(k, v) - takes key value pair and outputs a set of output key value pairs (key = word, value = aggregate count)
            - Example Word Counting: MapReduce
                - #1 - Obtain Big Document and split into chunks
                - #2 - Map takes file and outputs word and value of 1
                - #3 - Reduce - Group by Key, collect all pairs with the same key
                - #4 - Reduce - take grouped keys and aggregate them together
            - Note on Global Count
                - Issue - how do I aggregate values together to determine global count?
                - Solution - hash function will take intermediate key value pairs and send them to reduce nodes, and in the reduce nodes grouping is done and reduce is applied
                - Each word is the input to the hash function, producing a unique number
                - X MOD B, x is intermediate key, B is number of buckets or reduce nodes
                - X MOD 3, output is either 0, 1, or 2, intermediate key goes to a particular node
                - Whichever map node the X occurs on, due to the hash, it will be sorted to the same reduce node, all unique words must go to same bucket which is same reduce node
            - Pseudo Code
                - map(key, value) - key = document, value = text
                    - for each word in value, emit(word, 1)
                - reduce(key, values) - key = word, values = iterator
                    - result = 0, for each count in values: result += v, emit(key, result)
            - MapReduce Environment Features
                - Partitioning the input data
                - Scheduling the program's execution across a set of machines
                - Performing the group by key step
                - Handling machine failures
                - Managing required inter-machine communication
            - MapReduce a Diagram - Slide 25
            - MapReduce Node Diagarm - Slide 26
                - Map Nodes - 3, Reduce Nodes - 2
        - Data Flow - Map Reduce
            - Input and Output are stored on Distributed File System i.e. Hadoop Distributed File System
            - Intermediate results are stored on local File System of Map and Reduce workers
                - Why? It would involve duplicating and take processing power and network bandwidth
            - Final Output is often input to another MapReduce task
        - Coordination Master
            - Task Status (idle, in progress, completed)
            - Idle Tasks - get scheduled as workers become availbale
            - When a map task completes, it sends the master location and size, Master pushes this info to reducers
            - Ping workers periodically to detect failures
        - Failures
            - Map Worker Failure - all map tasks are reset
            - Reduce Worker Failure - only in progress tasks are reset to idle
            - Master Failure - MapReduce task is aborted and client is notified
        - How many Map and Reduce Jobs?
            - Make M much larger than the number of nodes in the cluster
            - One Distributed File System chunk per map is common
            - Usually R is smaller than M, because output is spread across R files, if R is large, intermediate map files explode and cause high network traffic
        - Task Granularity and Pipelining
            - Master - Assign Tasks
            - Worker 1 - Map 1 and Map 3
            - Worker 2 - Map 2
            - Worker 3 - Reduce 1
            - Worker 4 - Reduce 2
        - Refinement: Combiners
            - Opportunity - perform aggregation earlier
            - Action - on the same Map, combine values of all keys of a single mapper to reduce data needing to be copied and shuffled
            - Note - combiners can only be used if Reduce is commutative and associative
                - Commutative - A + B = B + C (Addition, Subtraction)
                - Associative - (A + B) + C = A + (B + C) (Addition, Subtraction)
                - Average - Sum = 1 + 2 + 3 / 3 and Sum = 4 + 5 / 2 can be S1 + S2 / C1 and C2
                - Median - no...
        - Refinement: Partition Function
            - Sometimes it is useful to override the hash function
            - Example; Host Size
                - Host = url, size, date
                - Goal - for each host, find the total number of bytes of all URLs
                - Override the hash function use the hostname of the url instead of the entire URL, to ensure all urls with same host name are sent to each machine
        - Assignment: Language Model
            - Statistical Machine Translation - need to count number of times every 5 word csequence occurs in a large corpus of documents
                - Map
                    - Key1 - the dog and the cat, value = 1
                    - Key2 - dog and the cat played, value = 1
                - Reduce
                    - Combine the counts
    - Problems with MapReduce
        - Difficulty of programming directly in MR, many problems arent easily described
        - Performance bottlenecks or batch not fitting use cases, writes everything to disk
    - Data Flow Systems
        - MapReduce uses two ranks i.e. Map and Reduce
        - General
            - Allow any number of tasks/ranks
            - Allow functions other than Map and Reduce
    - Spark
        - Benefits
            - intermediate results are kept in memory instead of disk
            - Caches data for repetitive queries
            - RDDs spread across the cluster
        - Design
            - Directed Acyclic Graph (DAG)
            - Resilient Distributed Datasets (RDD)
                - Partitioned Collection of Records
                - Can only write once, need new RDD each time to make a change
    -
