Deep Learning

1. 01/09/2025 - Introduction
    - Algorithmic Agent
        - Goal - deliver from San Francisco to New York
        - Solution - Djikstra's Algorithm
    - Reasoning Agent
        - Goal - predict the most likely next output given the input
        - Solution - Large Language Model
    - Machine Learning - machine learns from experience and data input
        - Image Classification
        - Image Generation
        - Prediction
    - Deep Learning
        - Discriminative Model Application - learn and model the decision boundary between different classes in input data
        - Generative Model Application - model the underlying distribution of the data and then generate new samples that are similar to training data
        - Application
            - Regression
            - Image Classification
            - Object Detection
            - Image Segmentation
            - Language Translation
            - Image Retrieval
            - Visual Q&A
            - Image Captioning
            - Text to Image
            - Mixed Text-Image Generation
        - Model 
            - Linear Models
            - Fully Connected Neural network
            - Convolutional Neural Network
            - Recurrent Neural Networks
            - Transformer Block
        - Training Tools
            - Gradient Descent
            - Data Normalization
            - Data Augmentation
            - Regularization
            - Dropout
            - Weight Decay
            - Weight Initialization
            - Batch Normalization
            - Activation Function
            - Learning Rate Decay
            - Transfer Learning

2. 01/16/2025
    - Regression and Classification
        - Regression - predict continuous outcomes e.g. predict the size of a house based on size
        - Classification - predict categorical outcomes e.g. identify house type
    - Building A Model
        - Regression - model is linear line, given any value, predict the price
        - Classification - model is plot with categorical distinctions
    - Make a Model Prediction
        - Regression - given house size is 550 sqft, we predict the price will be
        -  Classification - given hous esize is 550 sqft, we predict the type is apartment
    - Linear Regression
        - Linear Model
            - Equation: f(x)=a0​+a1​x1​+…+an​xn
            - Inputs: 1, x1, x2, ... , xn
                - 1 dimensional input to 1 dimensional output (line)
                - 2 dimensional input to 1 dimensional output (plane)
            - Prediction: f(x) = a0 + a1*x1
                - Example f(x) = 5 + 10x, f(x = 3) = 5 + 10*3 = 5 + 30 = 35
                - Example f(x) = 5 + 10x1 + 10x2 = 5 + (30) + (30) = 65
        - Linear Model Training
            - Perfect Linear Model - y: prediction, a1: slope, ao: intercept, only need two points needed
            - Reality - noise data i.e. input data is not perfect 
            - Goal - Data driven optimization
            - Steps
                - 1. define an error term i.e. the distance from a particular point to the linear line
                - 2. define a loss function i.e. considers all the errors for all datapoints in the dataset, summation of all errors
                - 3. minimize the loss function with respect to the parameters i.e. how do I reduce the overall summation error by adjusting my linear equation parameters i.e. a0, a1, etc.
                - 4. Set gradient as 0, error term is quadratic, property is a local min or maximum, at that point, derivative of the function should be 0
                - 5. Once you have a0 and a1, training is complete
                - 6. Convert loss functions
                - 7. Make prediction using results to produce a value
            - Notes
                - more data inpoint more rows, and more variables more columns
            - Example
                - x1 = [2,2], x2 = [3,4], x3 = [4,5], x4 = [6, 8]
                - y1 = 10, y2 = 12, y3 = 15, y4 = 18
                - What is A i.e. Amxn where m = rows and n = columns
                    - M = number of datapoints i.e. 4
                    - n = 1 + x1 A and x1 B
                    - A = [1, 2, 2]
                      [1, 3, 4]
                      [1, 4, 5]
                      [1, 6, 8]
                - What is y? Y is vector 4x1
                    - y = [10]
                        [12]
                        [15]
                        [18]
                - What is a? a = (At * A)^-1*At*y
        - Linear Model Notes
            - Solution; One, None, Multiple
                - Example: single data point, results are infinite that grow through that point
                - Rule: number of knowns equal number of data points
    - Linear Classification
        - Binary Classification - 0 or 1, predict is a single value
            - Logistic Model - probability of Yes and No i.e. p(YES) + p(NO) = 1 or p(No) = 1 - p(YES)
            - Sigmoid Function f(x) = 1/(1+e^-x), upperbound is one on y axis, lower bound is 0
                - Input linear equation Ax as x in the equation to produce a probability
                - If predicted value is greater than 0.5 then yes, otherwise no
            - How do we get a0 and an?
                - Dataset, X1, x2, ... Xn with a y1, y2, y = 0 or 1
                - 1. Define an error term, why was natural log added?
                    - If y = 1, p(y) is large than 1
                    - If y = 0,  1 - p(y) is small
                    - Cross Entropy - way to define how similar your predicted distribution to your ground distribution
                - 2. Define a loss function, average of error terms
                - 3. Minimize loss function
        - Multi-Class Classification - multiple categorical outcomes e.g. an image contains cat or a dog
            - Image Classification - input image -> category label
                - Image - two dimensional input
                    - 2x2 pixels - four pixels
                - How to use linear classifier?
                    - Stretch or flatten the image into a single vector [10 5 7 3] each number a pixel
                    - We have a linear classifier, we need three equations to represent each category
                - Weight x Input Image + Bias = Class
                - Output - largest, or instead probability
                - How to perform classification?
                    - Obtain s1, s2, s3 and convert to probabilities P1, P2, P3. Summation of P1 + P2 + P3 = 1
                    - Normalization - 21 / (21 + 8 + 11) = .525, 8 / (21 + 8 + 11) = .2, 11 / (21 + 8 + 11) -.275
                    - Softmax - p1 = e^s1 / (e^s1 + e^s2 + e^s3)
                        - How do we get W and b?
                        - 1. Define the error term - cross entropy
                        - 2 Define a loss function - mean error
                        - 3. Minimize the loss function - 

3. 01/23/2025
    - Optimization in Machine Learning
        - Objective - minimize loss functions i.e. adjust model parameters to make a good prediction
            - Quadratic Functin - minimization involves setting the gradient as 0
                - Equation is 10x^2 + 3x + 4
                - Derivative is 20x + 3
                - 0 = 20x + 3
        - Issue - loss function can be very complicated and solving gradient = 0 is very challenging
        - Technique #1 - Grid Search - global search method
            - Loss Function - L(theta) 
            - Search Boundary - [x_left, x_right]
            - Step Size - delta, increase step to determine next sample to evaluate for the loss function
            - Goal - what is smallest value of L? theta that produces that is your model parameter
            - Issues
                - Disccretized steps i.e. you miss continuous values
                - Dimensionality curse - 1D = 10, 2D = 10 * 10, 3D = 10^3
        - Technique #2 - Gradient Descent
            - Loss Function - derivative of the loss function is the gradient
            - Start Point - where you begin accessing
            - Learning Rate - alpha, how much you change your input value to test the loss function
            - Stop Condition
            - Which direction?
            - Calculations
                - Analytic Approach - if you have a function, take derivative, plug in value to get gradient
                - Numeric Approach - use a formula, plug in a tiny delta, verifies analytic gradient
                - One Dimensional - If a derivative is positive, the loss decreasing direction is negative i.e. you should reduce your x value to produce a smaller value i.e. head towards 0
                - In N-Dimensional - the gradient of the function is the vector of partial derivatives along each dimension
                    - L(x,y) = 2x^2 + 3y^2, Gradient is 4x+6y
                    - L(1,1) = 2+3 = 5, L'(1,1) = [4, 6] 
            - Recommendation - always us analytic gradient, but check implementation with numerical gradient
            - Tool - PyTorch i.e. automatic gradient descent tools
            - Issue
                - #1 - this technique is subject to starting point
                - #2 - too large a learning rate and you might jump back and forth
                - #3 - subject to local minimums
    - Optimizing a Linear Classifier for Image Classification
        - Model Parameters - S = wx+ B
        - Calculate Gradients
            - The total gradient with respect to theta is the average of all the gradients for each error term
            - Parameter One - width
            - Parameter Two - bias
            _ Example
                - Image = x1 and x2
                - Score = Weight Matrix times Image Matrix plus Bias Matrix
                - O - e^sn
                - P1 - probability, what you are tring to increase 
            - As a Tree
                - Error Term - dependent on P1 - Root
                - P1 - dependent on O1 and O2
                - O1 - dependent on S1
                - O2 - dependent on S2
                - S1 - leaf
                - S2 -leaf
            - Gradient Calculation - Chain Rule
                - Derivative of e with respect to s1 is three parts
                    - Derivative of e with respect to p1
                    - Derivative of p1 with respect to o1
                    - Derivative of o1 with respect to s1
                - Error Term with Respect to Weight - 36:00
                - Error Term with Resepct to Bias - 36:00
            - Derivative of Natural Logs - derivative of e^x with respect to x is e^x
            - Derivative of Exponential Function - derivative of lnx is 1/x
            - Derivative of Quotient - derivative of u(x)/v(x) = u'(x)v(x) - u(x)v'(x)/v(x)^2
        - Generalization - suppose you have k classes that is p1 through pk, probability for each k
            - Only the ground truth key row do you minus 1 to get gradient with resepct to your weight matrix
            - One Hot Vector - 1k is  vector that contains all zeroes except in the kth position
            - Goal - average of all error terms is your gradeint, for each data sample you will have probability and one hot vector which will give you gradeient with resepct to W
            - Note - if you have N data samples we need to calculate this N times and average them
        - Involved Example - 42:00 - Optimizing a Linear Classifer for Image Classification
            - Key Steps
                - Image Classification - S = Weight * Samples + Bias
                    - W - Matrix where rows = Classes and columns = features
                    - S - Matrix where columns = for a particular sample, each feature value for that sample
                    - B - W * X results in a m x n matrix, Bias must have the same number of rows to perform addition AND bias must have the same number of values as they are classes i.e. m of W.
                - Calculate Gradients of Error Terms
                    - s = wxn + b - score
                    - p = softmax(s) - probabilties as a function of s
                    - error = ln(pn)
                - Calculate Total Loss from Gradient of Error Terms and Current Weight and Bias
                - Adjust Weight and Bias to reduce Total Loss
                    - Derivativer of Error with Respect to S (e1 and e2)
                    - Derivative of Error with Respect to Weight (e1 and e2)
                    - Derivative of Error with Respect to Bias (e1 and e2)
                - Determine Gradient of Weight
                - Determine Gradient of Bias
                - Update Weight
                - Update Bias
                - Calculate New Total Loss
    - Regularization
        - Issue - sometimes the model overfits
            - Example - 5 degree polynomial model first the 5 points perfectly
            - New point will produce a large error given the overfit model
            - Notes - training set low errors, poor performance on new data
        - Data Split
            - Training Data - adjust model weight and tune hyper parameters
            - Validation Data - check model performance, tune hyperparameters, check if overfitting
            - Testing Data - final test
        - How can we detect model overfitting?
            - Look at the training loss, if it goes down, while the validation loss is increasing, it means model is overfit
        - How to prvent overfitting?
            - Weight Regularization Loss
            - Dropout
            - Batch Normalization
        - Regularization Loss
            - Loss function is a weighted combination of data loss (fit to training data) and regularization loss (prevent overfitting)
            - We want to mimize data loss and at the same time minimize regularization loss
            - Note - prefer small weights and spread weights
                - W1 [0.5, 0.5] is preferred to W2 [1, 0]
    - Steps to Train a Linear Classifier Using Cross-Entropy Loss
        - Compute class scores using S = W*X + b
        - Apply the softmax function to obtain class probabilities.
        - Compute the cross-entropy loss.
        - Compute the gradient of the loss with respect to weights and biases.
        - Update weights and biases using gradient descent.
        - Repeat for multiple iterations (epochs) until convergence.
        
4. 01/30/2025
    - Neural Networks & Backpropogation
        - Issue - Linear classifiers are not that powerful
        - Options
            - Feature Transformation - convert or modify values to produce a distribution whose classes are more easily bounded by a linear line
                - Issue - how do you find a good feature transformation?
            - Neural Network - learn the feature transformation and build a classifier for the data
        - Linear Classifier - s = wight * inuts + bias, probability = softmax(s), tune with gradient descent
            - Each score output is a combination of input and weights, weight and bias can be learned
        - Two Layer Neural Network
            - Layer One - width and bias
            - Layer Two - width and bias
            - Activation
            - Network
                - Input x times weight one and add bias one to produce h1
                - Input h1 times weight two and add bias two to prduce s
                - Softmax to produce probabilitie 
        - Activation Functions
            - No matter how many layers you have, if you do not have an activation functions it will continue to function as as linear classifier
            - Activation Function - non linear e.g. sigmoid
            - Each new layer has as input the previous layers output
        - Deep Neural Networks - many many layers
        - Return to Activation Function
            - Rectified Linear Unit or ReLU, if value is larger than 0 get x, if value less than 0 get 0
            - Sigmoid - not good for deep neural networks
        - Back Propogation - How do we adjust parameters in our neural network?
            - Parameters - W1, W2, ..., WN and B1, B2, ..., BN
            - Gradient Descent - given learning rate, intial W and B, adjust them to reduce loss
            - Idea - backpropogate loss to weight gradient
            - Computation Graph Example -
                - x = 2, y = 1, z = 0
                - f(x,y,z) = x * (y + z)
                - Gradient
                    - Derivative with respect to x = (y+z)
                    - Derivative of x with respect to y = x
                    - Derivative of x with respect to z = x
                    - Result is 3x1 vector
                - Idea - use graph structure to denote the function
                - Forward Pass - given input, produce intermediate and final ouputs
                    - y + x = v
                    - x * v = f
                - Backward Pass - Calculate Gradient
                    - Derivative of f with respect to f - 1 why?
                    - Derivative of f with respect to x - 1 why?
                    - Derivative of f with respect to v - 2 why?
                    - Dervivative of f with respect to y is
                        - Derivative of f with respect to v - 2 why?
                        - Derivative of v with respect to y - 1 why?
                    - Derivative of f with respect to z is
                        - Derivative of f with respect to v - 2 why?
                        - Derivative of v with respect to z - 2 why?
                - Why Forward and Backward? Allows you to move through each step forward or backward
                    - Downstream Gradient = Local Gradient * Upstream Gradient
            - Another Computation Graph Example - l = max(wx +b, 0)
                - Goal derivative of l with resepct to bias
                - Goal Derivitave of l with respect to v
                - Goal derivative of l with respect to weight
                - Goal derivative of l with respec to x
                - Goal derivative of l with respect to l = 1
                - Goal derivative of ReLU if s > 0 = 1 else 0
            - Compuation Graph - calculate all derivatives for all parameters using forward and then back in order to update the function - DO BY HAND
                - Note - you must cache forward passing results to calculate the gradient later
                - Add = Gradient Distributor - i.e. it will always take your upstream gradient value and send it backwards
                - Multiple = Swapper and Multiplier
                    - Upstream Gradient multiplied by Lower Gradient
                - Max - Router
                    - Gradient becomes 0 or the same
                - Copy = Gradient Adder, gradient of downstream is gradient of both upstream gradients
        - Calculate Gradient for Weight and Bias of Neural Network for Layer One
            - Goal
                - loss with respect to b
                - loss with respect to w
            - Steps
                - Go through first layer to produce the end value
                - Gradient of l with respect to h - upstream gradient with respect to ReLU
                - Gradient ReLu is 1 if greater than 0 and 0 otherwise
                - Gradient of l with respect to s - 0 or 1
                - Gradient of l with respect to b
                    - Gradient of l with respect to s times gradient of s with respect to b (1)
                - Gradient of l with respect to v
                    - Gradient of l with repect to s times gradient of s with respect to v (1)
                - Gradient of l with respect to w
                - Gradient of l with respect to x
        - Produce Probabilities Layer Two
            - Goal
                - Loss with resepct to b
                - Loss with respect to w
                - Loss with respect to h
            - Once we have the gradient with respect to h, we can contionue to calculate the gradients for the first layer
        - How to adjust parameters?
            - Stochastic Gradient Descent -> Backpropogation
            - Basic Idea - calculate forward pass and Pytorch will do back propogratrion
        - Autograd PyTorch - REVIEW

5. 02/09/2025
    - Deep Learning Software PyTorch
        - Tensor - multi dimensional rays of numerical data
        - Features (Review and Practice)
    - Convulational Neural Network
        - Fully Connected Layer
            - 4 X input nodes
            - 3 S output nodes
            - Output is a factor of all 4 x inpute nodes
        - Data Dimensions
            - Previously DxN, D = rows, N = columns
            - Now - AxT, A = rows and number of data samples, T = number of columns
            - Now - NxCxHxW
                - N - number of data samples in a batch
                - C - number of channels
                - H - image height
                - W - image width
        - Convulational Layer
            - Problem - grey image, one channel, channel value is 0 through 255
                - H = 32
                - W = 32
                - Convulational Filter - channel x H x W - 1x5x5
                - Basic Idea - do a dot product of the filter with respect to a portion of the image, produces one value, add bias
                - Shift - produce another value using filter, move across the entire imnage
                - Note - filters can have h/w greater than 1
            - Problem - RGB-D, red, green, blue, and depth
            - Problem
                - Setup
                    - Image Size - 4x4
                    - Filter - 3x3
                    - Output Size - 2x2
                - Action
                    - Multiply square by square
                    - Sum
                    - Add Bias
                - Shift Filter
                - Repeat Action [1-3s]
                - Output Size - Image Size - Filter Size + 1
                    - 4 - 3 + 1 = 2
            - Problem
                - Stage One Setup
                    - Channel - Three RGB
                    - Image Size - 32x32
                    - Filter - 3x5x5
                    - Output Size = Image Size - Filter Size + 1 = 32 - 5 + 1 = 28
                    - Possibility - multiple filters each with different weights
                - Action
                    - Dot Product - (RxF1 + BxF1 + GxF1) + Bias = 1 Pixel on one output images
                    - Dot Product - (RxF2 + BxF2 + GxF2) + Bias = 1 Pixel on another output images
                - Stage Two Setup
                    - Channel - Two
                    - Image Size - 28x28
                    - Filter 
                    - Output Size =  Image Size - Filter Size + 1 = 
                    - Question - HOW DID YOU GET SECOND FILTER?
            - Problem
                - Padding - add pixels around input image, normally values of zero, this increases size of image output, making it possible to produce an image of the same size
                - Stride - how far you move before applying fitler again
                - Output Size w Padding AND Stride
                    - Output Size = (Input Size - Filter Size + 2 * Padding) / stride + 1
                - Special Case - Padding Size = Filter Size - 1 / 2 will produce an output imge of same size
                - Receptive Field
                    - Each time you do a convulation you produce a new ouput, resulting in a pxie.
                    - RF - region of the imput image that influences a particular neuron in a layer
                        - As you go backward from the pixel in convolutional layers, the region of influence increases
                    - Receptive Field Size = 1 + Number of Layers * (K-1)
                        - Number of Layers Before= 3
                        - K = Kernel or Filter Size = 3
                        - RFS = 1 + 3*(3-1) = 7 i.e. 7x7 Receptive Field
            - Problem - Stacked Convolutional Layer
                - Input Image: N x C1 x (S1 x S1)
                - Filter: C2 x C1 x (K1 x K1)
                - Issue - if you only use one layer, it becomes linear
                - Solution - add ReLu i.e. a non-linear activation function, to produce results that are non-linear
            - Problem
                - Channels - 3
                - Image Size - 32x32
                - Filter Size - 10 x 3 x 5 x 5
                - Stride - 1
                - Pad - 2
                - Output Size = (Input Size - Filter Size + 2 * Padding) / stride
                    - (32 - 5 + 2 * 2) / 1 + 1 = 27 + 4 + 1 = 32 or 10 x 32 x 32 = 10240 pixels
                - Learnable Parameters?
                    - Number of Filters x Size of Filter
                    - Size of Filter = 3 x 5 x 5 + 1 = 76
                    - Number of Filters = 10
                    - LP = 10 * 76 = 760
                - Number of Multiple-Add Operations?
                    - Filter Size - 3 x 5 x 5 = 75
                    - Number of Outputs = 10 x 32 x 32 = 10240
                    - Total = Filter Size * NUmber of Outputs
                        - 75 * 10240 = 76800
                - PyTorch Conv2D ((input, output, kernel size, stride, padding))
                    - Filter - 3 x 5 x 5
                    - Layers - 10
                    - Input Channel - 3
                    - Output Channel - 10
                    - Kernel Size = 5
                - Problem - Large Images require Computations
                    - Solution - Pooling Layer
                        - Pooling - Take image and downsample, produicing a smaller image
                        - Max Pooling - take the maximum value for each pooling region
                        - Average Pooling - computes the average value of each pooling region
                        - Global Average Pooling
                        - Adaptive Pooling
                - Problem - Classic Convultional Layers
                    - Input Image -> Convultional Layer -> ReLu -> Pooling -> Output Image
                - Problem - Classic LeNet-5
                    - Architecture - [Convultional Layer -> ReLU -> Pool] x N -> Flattern (Stretch to Single Vector) -> [Linear Layer -> ReLU] x N -> Linear Layer -> Softmax
                    - Notes
                        - Spatial Size Decreases HxW
                        - Channels Increase 1 -> 20 -> 50
                        - Design - reduce spatial size but increase channels, less spatial size to reduce memory, icnrease channels to learn more features    
                - Batch Normalization
                    - Normalize the output so thaty have zero mean and unit variance
                    - Why? stabilization the optimization process so we can train a deeper network
                    - We have N x D matrix i.e. N = samples, D = features
                    - Do normilzation on each feature dimension i.e. column
                        - Extract column, add all values in the column and do average over n
                        - Extract column, and calculate standard deviation
                        - For each column, take the columns mu and sigma to normalize each value
                    - Learnable Scale and Shift Parameters - What is zero mean, unit variance is too hard to constrain?
                    - What values should we use during Test Time? Keep a running average mu and sigma

6. Convolutional Neural Network Architectures
    - Review 
        - LeNet-5 - used for hand written digit recognition
        - Normalization Layer - REVIEW
    - Architectures
        - AlexNet
            - Image -> Convoution -> Pooling -> Convolution -> Pooling -> Convolution -> Convolutional -> Convolutional -> Pool -> Fully Connected Neural Network -> FC -> FC -> Softmax
            - Output Size - REVIEW - (Input Size - Filter Size + 2 * Padding) / stride
            - # of Parameters - REVIEW - Number of Filters x Size of Filter
            - Memory - REVIEW - 
            - FLOPs - REVIEW
            - Notes
                - Most of the memory usage is in the early convolutional layers - WHY?
                - Most of parameters are in the fully connected layers - WHY?
                - Most of FLOPs are in the convolution layers - WHY?
        - VGG
            - Design Rules
                - All convolutional layers are 3x3 with stride 1 and pad 1 i.e. same receptive field with less params, less FLOPs and more nonlinearities
                - All max pool are 2x2 with stride 2, after pool, double # channels i.e. less memory usage, same FLOPs, and more parameters
            - Stages
                - c -> c -> p
                - c -> c -> p
                - c -> c -> c -> p
                - c -> c -> c -> p
                - c -> c -> c -> p
                - FC -> FC -> FC -> Softmax
        - GoogleNet
            - Problem Solved - first layer takes too much memory for VGG, solution is to aggressively down sample
            - Notes
                - Layers - 22
                - Inception Modules - use different convolution filters of different sizes will handle objects at multiple scales better
                - Global Average Pooling - reduce fully connected network layers
        - ResNet
            - Notes
                - Layers - 152
                - Issue #1 - deeper model seems to be underfitting i.e. deeper model is harder to optimize
                - Solution #1 - make it learn identity functions easier
        - ResNext
            - Notes
                - Group Convolution - each filter has the same number of channels as input, divide filters into groups and each group looks at a subset of inputs
                - Depthwise Convolution - each filter in each group applies to one channel
        - PyTorch
            - Grouped Convolution - torch.nn.Conv2d
    - Model Trade Offs
        - Memory Usages vs. Number of Parameters vs. Number of FLOPs
    - Essential Questions to Answer for Exam
        - Why is pooling used?
        - Why does output size matter?
        - How do you produce an output size equal to inpuyt size?
        - How do you calculate essential paramters e.g. output size, # of parameters, memory, FLOPs, etc.
        - How does AlexNet compare to VGG?
        - VGG to GoogleNet?
        - GoogleNet to ResNet?
        - What is a convolutional layer?
        - Why is ReLu used?
        - Why is Fully Connected Layer used?
        - Why is softmax used?
        - What is a skip connection?
        - What is Bottlenext skip connection and why is it used?
        - What is down sampling?
        - What is up sampling?

7. Object Detection and Image Segmentation
    - Computer Vision Tasks
        - Classification - cat, dog, (no spatial information)
        - Semantic Segmentation - grass, cat, tree, sky, no object, (pixels)
        - Object Detection - dog, cat, dog (multiple objects)
        - Instance Segmentation - dog, dog, cat (multiple objects)
    - Classification to Object Detection
        - Definition - set of detected objects
            - For each object predict
                - Category Lablel
                - Bounding Box
        - Challenges
            - Multiple Outputs
            - Multiple Types of Outputs (What/Where) (Label/Box)
            - Large Images
    - Object Detection
        - Bounding Boxes - axis aligned or object oriented
        - Modal - bounding boxes cover only the visible portion
        - Amodel - box covers the entire extent of the object, even occluded
        - Intersection Over Union - how can we compare our predition to ground truth?
            - Area of Intersection / Area of Union
            - IOU > .5 decent
            - IOU > .7 pretty good
            - IOU > .9 perfect
        - Single Object
            - Goal - output label and bounding box
            - Action - treat localization as a regression problem
            - Technique - Sliding Window
                - Try to classify each crop as object or background
                - Parameters - # of boxes, size, and stride
                - Issue - if you have a large image you m ay have a large number of boxes
            - Technique - Region Proposal - Region Based CNN
                - Goal - try to find a small set of boxes that cover the image
                - Action - resize images from regions of interest and make a classifcation for each region
            - Technique - R-CNN's Box Regression
                - Model - Transform
                    - 
                - Region Proposal
                    - Center (x,y)
                    - Width (w)
                    - Height (h)
                - Output Box
                    - x is x + (w*tx)
                    - y is y + (h*ty)
                    - w is w * exp(tw)
                    - h is h * exp(th)
                - Ground Truth Transform Equation - Reduce Error
            - Training: R-CNN Training
                - Action - categorize each region proposal as positive, negative or neutral
                - Action - negative class no box target
                - Action - class target
                - Action - box target
                - Order
                    - Run Proposal Method to Get Region Proposals
                    - Runn CNN on each proposal to get class scores, transforms
                    - Threshold class scores to get a set of detections
                - Problems
                    - Overlapping
                        - Solution - non-max suppression
                            - Select next highest scoring box
                            - Elminate lower scoring boxes with IoU > threshold
                            - If any boxes remain, GOTO 1
                    - R-CNN - speed of execution due to forward passes
                        - Solution - Fast R-CNN
                            - Run ConvNet on image to generate image features
                            - Produce regions of interest from image features
                            - For each region, run CNN to get class cores
                            - Runn CNN on each proposal to get bounding boxes
                    - Fast R-CNN Feature Crop
                        - How do you crop features correctly?
                        - Solution - perform linear projection to scale image
                        - Solution - snap images to cell
                        - Solution - form regions to standard feature size
                    - Fast vs Slow
                        - Fast - fraction of a time but region is most of the time
                        - Solution - Region Proposal Network to predict proposals from featurets
                            - Produce Image Feature with Anchor Point
                            - Put Box at Each Anhor Point
                            - Classify each as Positive (Object) or Negative (No Object)
                    - Dealing with Scale
                        - Multi Stage Multiscale Features
            - Technique Single Stage Detector
                - RetinaNet - directly predict object or background
            - Technique: Single Stage Anchor Free Detector
    - Semantic Segmentation
        - Definition
            - Things - object categories that can be separate into instance
            - Stuff - object categories that cannot be
        - Technique: Fully Convolutional Network
        - Technique: Downsampling and Upsampling
            - Downsampling - High to Low Res for Feature Detection
            - Upsampling - Low to High Res for Feature Area Detection
    - Instance Segmentation
        - Object Detection for Bounding Boxes
        - For Each Bounding Box Pixel - Predict

8. Recurrent Neural Network
    - RNN - Process Sequence
        - One to One - Image Classification
        - One to Many - Image Captioning
        - Many to One - Video Classification
        - Many to Many - Machine Translation
    - Internal State
        - new state = a function of old state and input vector at some time step
    - Vanilla Recurrent Neural network
        - State - single hidden vector h
    - RNN Computational Graph
        - h -> weight and new input -> h -> weight and new input continue
    - RNN Computational Graph - Many to Many
    - RNN Computational Graph - Many to One
    - RNN Sequence to Sequence
        - Encoder -> RNN -> RNN -> Decoder
        - Hidden Middle State - code context information
    - RNN Next Token Prediction
        - Token - convert string into numerical value or vector
        - Take numerical vector input and use layers to predict next letter
        - Combine together
        - Take sample output and go through process again, continuoulsy predicting next letter
        - Learnable Weight - Embedding, Hidden, Output
        - Action - encode inputs as one hot vector
        - Backpropogation Through Time - forward through entire sequence to compute loss, then backward through entire sequence to compute gradient
    - RNN Image Captioning
        - Image -> Convolutional Network to Produce Feature Vectors -> RNN to Identify Captions
    - RNN LSTM - Long Short Term Memory
        - Input Gate - whether to write to cell
        - Forget Gate - whether to erase cell
        - Output Gate - how much to reveal cell
        - Gate Gate - how much to write to cell
    - RNN Layers - Multi Layer
        - As layers increase...
    - RNN Many to Many Cross Attention
        - Review...
    - RNN Cross Attention for Image Captioning
    - RNN Attention Tranformer
        - Similarity Function - use dot product + softmax to measure similarity
        - Query, Key -> Similarity
            - Dot Product Query and Key
        - Similarity - Attention Weights
    - RNN Attention - Self Attention
        - Review...


            
