Operating Systems CIS 450

1. 08/31/2022 - Introduction
    - System Basics
        - Hardware - CPU attached to main memory
            - Buses connect components e.g. CPU, memory, graphics, peripheral devices
            - Questions
              - How close should each component be?
              - How do we deal with limited space?
        - Software
            - Questions
                - What happens when a program runs?
                    - It executes instructions, many millions/billions of times every second. The CPU
                        - fetches an instruction from memory
                        - decodes it i.e. figures out which instruction
                        - executes it i.e. does the thing
                        - moves to next instruction F-D-E
                - What else happens in the background when a user program runs?
                    - Body of software, OS Kernel, runs all the time to
                        - make it easy to run programs
                        - allow programs to share memory
                        - enabling programs to interact with devices
        - Basics of Von Nuemann Model of Computing
    - What is an OS?
        - Language, Command Interpreter, Library of Commands, Set of Utilities - NO
        - Definitions
            - Bottom-Up Perspective - Resource Manager
                - Resources - CPU, Memory, Disks, I/O, Timer, Battery, Touch Screens
                - Layers - User -> Application -> Operating System -> Hardware
                - Collection of software that manages computer hardware, intermediary
                - Benefits
                    - Sharing/multiplexing - more than 1 app/user to use resource
                    - Protection of apps from each other
                    - Performance - efficient/fair access to resources
                - Challenges
                    - Managing all kinds of resources
            - Top-Down Perspective - Hardware Abstraction Layer
                - OS as a virtual machine providing a standard library
                - Applications issue system calls to use OS abstractions
                - Benefits
                    - Ease of use
                    - Reusability - provide common functionality
                    - Portablity
                - Challenges
                    - What are the right abstractions?
        - Example - Android
            - Upward Compatability - app developers can write a single app that works across over a billion devices

2. 08/31/2022 - Three Important Pieces
    - Review of OS C Programming Examples from OSTEP
    - Illusion
        - The system has a very large number of virtual CPUs
        - Each running program has allocated memory at the same address, each program acts as it has its own private memory
    - Virtualization
        - Idea - take a physical resource and transform it into a more general, powerful, and easy to use virtual form of itself
        - Virtualization of the CPU - Central Processing Unit
            ./cpu A & ./cpu B &
        - Virtualization of the Memory
            - ./mem 100 & ./mem 1000 & - the memory address location is the same, this is the illusion demonstrated
    - Concurrency
        - Threads
            - ./thread 100 - produces expected value
            - ./thread 100000 - doesn't produce expected value... why?
            - Issue
                - This relates to how instructions are executed
                - A key part of the porogram, where the shared counter is incremented, takes three instructions
                    - Load the value
                    - Increment the value
                    - Store back to memeory
                - These DO NOT execute atomically all at once, and the results become indeterminate
        - Idea - How to manage multiple things at once
    - Persistence
        - Goal - store data for a long period of time
        - Hardware and Software Support
            - Hardware I/O Device
            - Software - File System
        - Unique - OS does not create a private, virtualized disk, instead we create a virtual to physical mapping

3. 09/07/2022
    - Virtualization
        - OS - takes a physical resource (CPU or memory) and transforms it into a more general, powereful, and easy to use resource
        - Abstraction - Process
            - One of the most fundamental abstractions - basic unit of resource virtualization from the perspective of an application
        - Process - CPU acitivities and memory usage
            - What is a process?
                - Informal - a running program, exeuction stream: stream of instructions
            - Examples
                - Web browser, execute compiled program, shell
            - Process vs. Program
                - Program - lifeless thing, code, instructions
                - OS takes program and runs them
                - Program - passive
                - Process - active
                - Different processes may be associated with the same program
            -  Linux/Unix Commands
                - ps - list running processes - excluding some background
                - top - tabular view of running processes
        - Abstraction - A Process
            - What constitutes a process?
                - Machine state of a process
                    - What a program can read or update when it is running
                    - Take inventory of the different pieces of the system it accesses or affects during the coures of its exeuction
            - Machine State
                - Memory - address space
                - Program Code - instructions
                - Stack - local variables
                - Heap - dynamically alocated memory
                - Data - global variables
                - Design - stack can grow, heap can grow, split to increase the size of each, naturally boundary has to exist for a process
                - Registers
                    - Program Counter - indicating the instruction of the program that is currently being executed
                    - Stack Pointer and Frame Pointer - managing the stack for function parameters, local variables, and return addresses
                - I/O Information - a list of open files the process currently has open
            - Process States
                - Initial
                - Ready - ready to run
                - Running - executing instructions
                - Blocked/Waiting/Sleeping - not ready to run until some other events takes place
                - Zombie
                - Running depends on number of CPUs
            - CPU Bound and I/O Bound
                - CPU Bound - Majority time on CPU
                - I/O Bound - Majority time on I/O
            - Data Structures
                - Process List - list of processes
                - Process Control Block - struct object representing each process
                    - Identifier, state, cpu registers
                    - Scheduling, memory management, accounting, and I/O information
            - Xv6 Proc Data Structure
                - proc.h and proc.ch
                    - UNUSED - initial state
                - ptable is the process list inside of proc.c

4. 09/08/2022
    - Process APIs
        - Pattern
            - Code -> Store on Disk -> Load into Memory -> Run
        - Important Process APIS - man fork, wait, exec
            - fork() - child begins execution of code immediately at line after fork(), entire parent code is duplicated and given to child
                    - if rc negative, something went wrong
                    - if rc = 0, child process was created and has control
                    - else, parent logic
                - Each process has its own memory allocation
                - Processes become concurrent running processes, non-deterministic who runs first
                - QUIZ - if fork() n times, how many processes? 2^n
                - Practical
                    - Parent makes a duplicate copy of its memory, including code segment, into antoher memory location for the child process
                    - Child has same code, but in a different location
                    - Child wont change code until exec() is called
                - fork() in xv6
                    - When the shell sh forks a new spin process, the new process name remains as sh until exec() is called
            - wait() - instruction to parent to wait until child process execution has finished
                - delays parent exeuction until ONE of its children finishes executing
                - waitpid(pid_t pid) - wait for specific child
                - Zombie state in xv6
                    - When child process is completed, state is changed to zombie state
                    - When the parent process sees the child process change to zombie state, it stops waiting
            - exec()
                - variants - execl(), execle(), execlp(), execv(), execvp(), execvP()
                - int execvp(const char *path, char *const argv[])
                    - path identifies the location of the new process image within the hierarchical file system
                    - argv is a pointer to an array of pointers to null-terminated character strings
            - Separation of fork() and exec()
                - Why? without separation, we dont have flexibility in the shell
                - Shell Pattern
                    - figures out where executable is
                    - calls fork() to create a new child processs
                    - calls some variant of exec() to run the command
                    - waits for the command to completed by calling wait()
                    - returns from wait() and prints out a prompt again, ready for your next command
                - The Trick - right after fork() and before exec(), the shell can run code that can alter the environment of the about to be run program, and enables a variety of interesting features
            - Unix Shell - list, redirection, and pipes
        - Create
        - Destroy
        - Wait
        - Status
    
5. 09/09/2022 - Session One
    - Virtualization - CPU Mechanisms
        - Time Sharing - share a resource in different time
        - Space Sharing - share a resource in different parts
        - Challenges
            - Performance - how can we implement virtualization without adding excessive overhead
            - Control - how can we run processes while retaining control over the CPU
        - Separation of Mechanism/Policy in System Design
            - Mechanism - low level machinery - part that controls the authorization of operations and the allocation of resources
            - Policy - high level intelligence - where decisions are made about which operations to authorize and which resources to allocate
            - Separation Achieves
                - modularity - allows one easily to change policies without having to rethink the mechanism
        - Low Level Machinery - Mechanism
            - How does a program run on CPU?
            - How does the OS stop running progam and start running another?
        - Simple Mechanism
            - Direct Execution - just run program on CPU
                - OS 
                    - Create entry for process list
                    - Allocate memory for program
                    - Load program into memory
                    - Set up stack with argc/argv
                    - Clear registers
                    - Execute call main()
                - Program
                    - Run main()
                    - Return 
                - OS
                    - Free memory of process
                    - Remove from process list
            - Direct Execution Benefits
                - Process runs natively on hardware
            - DE Problems
                - How can the OS make sure the process doesn't do anything that we don't want it to do?
                - How does the OS Stop it from running and switch to another process for time sharing?
            - Problem #1 - direct execution prevenets the construction of many kinds of desirable systems
            - Problem #1 Solution - Restricting Operations
                - Dual Processor Modes - User and Kernel
                    - User Mode - application runs with restrictions, no I/O, exceptions are raised
                    - Kernel Mode - kernel runs with privilege
                - System Call - user application makes call to get access to protected reources or privileged operations
                    - Limited direct execution with restriction
                    - Most modern OSes exposes a few hundred such operations
                    - Examples - See Slide 
                - How does a system call work?
                    - A process executes a special trap instruction, jumps into the kernel, and raises kernel mode
                    - Kernel performs privileged operations
                    - Kernel calls a special return from trap instruction, returns into the calling user program, and reduces to user mode
                - Three Potential Sources for Interrupt
                    - Hardware
                    - Processor
                    - Software
                - Harware Support
                    - Special instructions
                        - to trap into the kernel
                        - return from trap
                    - Assist the OS to provide user and kernel mode
                    - Careful when executing a trap, save enough of caller's register state to return correctly
                    - x86
                - Trap Handler
                    - How does the trap know which code to run?
                        - OS must carefully control what code executes upoon trap
                    - OS sets up a trap table or interrupt descriptor table at boot time
                    - OS informs the hardware of the locations of different trap handlers
                - System Call Number - number assigned to each system call
                    - User code is responsible for placing the desired system call number in a register or at a specified location
                    - OS examines number, ensures it is valid, and if it is, executes the corresponding code
                    - Indirection serves as a form of protestion, user code cannot specifiy an exact address to jump to
                - xv6 - see slides for example
        
5. 09/09/2022 - Session Two
    - Virtualization - CPU Mechanisms
        - Switching Between Processes
            - Problem
                - If a process is running the CPU, it means the OS is not running
                - How can the OS regain control of the CPU so that it can switch between processes
            - Cooperative Approach - wait for system calls
                - OS trusts the process of the system to behave reasonably
                - OS regains control of the CPU by waiting for a system call or an illegal operation
            - Non-Cooperative - OS takes control
                - Timer Interrupt - a timer device can be programmed to raise an interrupt every so many milliseconds
                - When the interrupt is raised, the running process is halted, and a preconfigured interrupt handler in the OS runs
            - Saving and Restoring the CPU Context
                - Decision - continue process or switch to a new one, made by high level policy the scheduler
                - Save general purpose registers, PC, and kernel stack pointer
                - Restore regsiter, PC, and switch to kernel stack
            - xv6 Context Switch
                - Save P1's user mode context and switch to kernel mode
                - Handle system call or interrupt
                - Save P1s kernel context and switch to scheduler context
                - Select another process P2
                - Switch to P2's address space
                - Save Scheduler CPU context and switch to P2's kernel context
                - Switch from kernel to user mode and load P2's user mode context
            - xv6 - Timer Interrupt Workflow
                - Scheduler - two infinite loops
        
                
6. 09/15/2022 - Policies #1
    - Focus - which process should the OS run?
    - CPU Scheduling - motivation for multiprogramming
        - Typical Execution Profile
            - Start -> CPU Burst -> I/O -> REPEAT
        - CPU scheduler is managing the execution of CPU bursts
        - Histogram of CPU Burst Times
            - Most CPU bursts is quite small < 2ms
        - Scheduling Structure
            - Ready queue of running processes
            - Scheduler chooses which to pick
    - Workload Assumptions
        - Job - processes
        - Each job has - arrival time, cpu burst time or running time, completion time
    - Scheduling Metrics
        - User
            - Turnaround time - time between submission and completion
            - Waiting time - sum of periods waiting in ready queue
            - Response time - time from submission and first response, first waiting time
        - System
            - CPU Utilization - percentage of time CPU is executing
            - Throughput - number of jobs completed per time unit
        - Good Scheduler
            - maximize CPU utilization anbd throughpout
            - minimze turnaround time, response time, and waiting time
    - First Come First Served - First In First Out (FCFS or FIFO)
        - Order of arrival in queue determines execution order
        - Waiting Time
            - P1 = 0 runs for 7
            - P2 = 2-7 so 5, runs for 4
            - P3 = 4 - 11 so 7, runs for 1
            - P4 = 5 - 12 so 7, runs for 4
            - 0 + 5 + 7 + 7 / 4 = 19 / 4 = 4.75
        - Features
            - Pros - simple and fair
            - Cons - long waiting times, poor dynamic behavior e.g. convoy effect
            - Convoy Effect
                - Happens when a set of processes need to use a resource for a short time, and one process holds the resource for a long time, blocking the other proceses
                - Cases long waiting times and poor utilization of other resources in the system

7. 09/15/2022 - Policies #1 - Shortest Job First (SJF)
    - Shortest Job First
        - Focus - reducing turnaround time
        - Action - whenever CPU is idle, pick process with shortest next CPU burst, non-preemptive, must wait for jobs to complete
        - Issue - how do we know how long a job will run?
        - Example
            - Break Tie - FCFS - P1, P3, P2, P4
            - Review example on slides!
        - Features
            - Pro - minimize average waiting time, optimal
            - Cons - starvation for long jobs and it is hard to determine length of next CPU burst
        - How to prove SJF is the best for minimizing average waiting time?
            - Induction
                - Moving a short proecss before a long one decreases the waiting time of the short process more than it increases the waiting time of the long process
            - Proof
                - Plong -> PShort
                - Swap Jobs, Delta W < 0, subtracting P1 length minus P short length to determine delta
                - Review Slides!, Swapping technique to reduce waiting time
        - SJF in Practice
            - How do you determine execution time of next CPU burst?
                - Guess? Inspection?
            - Forecasting Exponential Average
    - Shortest Remaining Time First (SRTF)
        - Optimal among all preemptive scheduling policies in terms of minimizing waiting time, overlook overhead
        - Can lead to starvation, and is not practically
        - Review slides!
    - Quiz - FCFS, SJF, SRTF
        - REVIEW
    - Priority Scheduling - Non-Preemptive, Preemptive
        - Pick the process with the highest priority
            - Priority - class, urgency
            - Preemptive or Non-Preemptive
        - SJF and SRTF is special priority scheduling
        - Problem - starvation of low priority
        - Aging - gradually increase the priority over time
        - Example - smallest number is highest priority
    - Round Robin
        - Circular Queue - with pointer to next job
        - During each turn, run for time quantum
        - After turn, move pointer to next job in a circular manner
        - Work Conserving - relinquish quantum of time if larger than remaining time on process
        - Pros - time sharing and response time
        - Cons - long average waiting time, large context switch overhead
        - Choice of Quantum
            - Large
            - Small - processor sharing illusion
            - Larger quantum, more cpu utilization
            - Rule of thumb - 80% of cpu bursts should be shorter than the time quantum
    - Quiz
        
8. 09/19/2022 - Multi-Level Feedback Queue
    - Summary
        - First Come First Served
            - PRO - Simple and Fair
            - CON - Long Turnaround/Waiting Time, Convoy Effect
        - SJF/SRTF
            - PRO - Minimum Turnaround/Waiting time
            - CON - Starvation, Not Practical
        - NP-PS/P-PS
            - PRO - Serves Urgent Jobs
            - CON - Starvation
        - RR
            - PRO - Short Response Time
            - CON - Long Turnaround/Waiting Time
            - Large Overhead for Context Switchg
    - Question - Can we achieve short response time and short turnaround time?
    - Goals
        - Short Turnaround/Waiting Time
            - SJF/SRTF
            - OS doesn't generally know how long a job will run
        - Short Response for Interactive Jobs
            - RR
            - Long waiting time
        - Incorporating I/O
        - Avoid Starvation
    - Multi Level Feedback Queue - Hybrid Version
        - Compatible Time Sharing System CTSS - Corbato
        - Use Multiple Queues
            - Student
            - Batch
            - Interactive
            - System Process
        - First Level - Which Queue - Interqueue
            - System -> Interactive -> Batch -> Student
        - Second Level - Within Queue - Which Job? - Intraqueue
            - RR
        - Building Blocks
            - Rule 1 - Priority(A) > Priority(B) - A Runs - Intraqueue
            - Rule 2 - Priority(A) = Priority(B) - A and B run in RR
            - Rule 3 - When a job enteres the system, it is placed at the highest priority
            - Rule 4 - if a job uses up an entire time slice, its priority is reduced - LONG RUNNING
            - Rule 5 - If a job gives up the CPU, it stays at same priority level - I/O
        - Problems
            - Starvation given too many interactive jobes
            - Rule #3 - can be abused by gaming the scheduler
        - Solution
            - Priority Boost - after sometime, move all jobs to the highest queue
        - Solution
            - Perform better accounting at each level
            - Instead of forgettting how much a time slice a process used at a given level, the scheduler should keep track
            - Update Rule 4 - Once a process has used its allotment, demoted to next queue
        - Rules
            - If P(A) > P(B), run A first
            - IF P(A) == P(B), A and B run in RR
            - When job enters, place at highest priority
            - If a job uses its time allotment at a given level, reduce its priority
            - After soe time, move all jobs to the highest priority queue
        - Tuning
            - How many queues?
            - How big should the time slice be per queue
            - How often should priority be boosetd
        - Common Practice
            - High Priority Queue - short time slices
            - Lower Priority Queue - longer time slices
        - Two Hawks? More?
            - How does it reduce turnaround and waiting time? - SRTF Mimic with Rule 3
            - How does it reduce response time? - Mimic RR with short time quantum
            - Use Boost Priority to avoid starvation
            - Better Accounting to avoid gaming the scheduler
        - Issues with MLFQ
            - MLFQ uses history to approximate the future
                - Use the age of a job to approximate the remaining size...
            - Not Perfect ^^
                - The job with the smallest remaiing size is likely to be the job with the largest age
        - Case Study Solaris
            - 1st Column Priority Queues 0 - 59 - highest number is highst priority
            - 2nd - time quantum at each priority
            - 3rd - run one round done with time quantum, which priority queue you are demoted to
            - 4th - I/O - during execution I/O request, you are promoted to a higher queue

9. 09/21/2022
    - Multiple Processor Scheduling
        - Sharing
            - Time Sharing
            - Space Sharing
        - Single vs. Multi CPUs
            - Single: CPU - Memory
            - Multiple:
                - CPU + Cache - Memory
                - CPU + Cache - Memory
                - Bus
            - Problems with Multiple CPUs
                - Cache Coherence - CPUs have different versions of variables locally cached
            - Solution to Cache Coherence
                - Bus Snooping - each cache pays attention to memory udpates by observing the bus
            - Locality
                - Temporal Locality - when a peice of data is accesssed, it is likely to be accessed again
                - Spatial Locality - if a program accesses a data item at address x, it is likely to access data items near x as well
        - Cache Affinity
            - Often advantageous to run a process on same CPU it was running on before
        - Scheduling
            - Single Queue Scheduling
                - Put all jobs that neede to be scheduled into a single queue
                    - Queue - A - B - C - D - E - Null
                    - Round Robin Rotation to Different CPUs
                - Pro
                    - Simple to implement
                - Cons
                    - Does not maintain cache affinity
                    - Lacks scalability
                    - Issues with lock for concurrent access to single queue
            - Multi-Queue Scheduling
                - Each queue follows a particular scheduling discipline
                - When a job enters, it is placed on exactly one scheduling queue according to some heuristic
                - Then job is scheduled essential independently
                - Pros
                    - Scalability
                    - Cache Affinity
                - Cons
                    - Load Imbalance
                    - Solution - occasionally migrate from CPU to different CPU
            - Linux Multiprocessor Schedulers
                - Single Queue BFS
                - Multi Queue Q1
    - XV6 Scheduler
        - Design - One global queue across all CPUs
        - Design - Per-CPU scheduling algorithm: RR with quantum size = 10ms
        - Scheduler in proc.c
            - Per-CPU Scheduler
            - Each CPU calls scheduler() after setting itself up
            - Scheduler never returns and it loops doing
                - Choose a process to run
                - Switch to start running that process
        - Timer interrupt in trap.c
            - trap -> yield -> sched -> swtch
        - XV6 Context Switch
            - Save P1s user mode context and switch from user to kernel mode
            - Handle system call or interrupt
            - Save P1s kernel context and switch to scheduler context
            - Select another process P2
            - Switch to P2's address space
            - Save scheduler CPU context and switch to P2's kernel context
            - Switch from kernel to user mode and load P2's user mode context
    - Program #2 - Revised MLFQ Scheduler
        - Note - Current is RR
        - Action - Change to rMLFQ
        - Note - Three rounds is 270 ms
        - Note - Why 10, 30, and 90 ms?
            - Interrupt is 10MS
            - Have a counter, sequence of timer interrupt to meet queue quantim size
        - Note - all changes should be done inside of proc.c 277
        - Note - DO NOT CHANGE switchuvm
        - Note - our job is to pick the right next proc = p
        - Note - use existing global array with additional value queue type (p->queuetype)
        - Note - do not change the yield in the trap

10. 09/23/2022
    - Evolution of Computer Systems
        - Early systems were easy
        - Systems became increasingly complex
        - Users expectations increased as well
    - Early Systems
        - OS with Routines + Current Program
    - Multiprogramming - jobs were batched and multiple processes were ready to run at a given time and OS would switch between them
        - Benefits - increased effective utilization of the CPU
    - Time Sharing - interactivity became important, as users might be concurrently using a machine, each waits for a timely response
    - How to implement time sharing?
        - Run one process for a while, give it full access to all memory
        - Stop process, save it its state to disk
        - Load another process and run it for awhile
        - Problem - saving the entire contents of memory to disk is brutally non-performant
    - How to implement time sharing?
        - Leave processes in memory while switching between them
        - Problem - need to prohibit a process from accessing some other's mmeory-protection, also requires abstraction of physical memory
    - Example mem.c - run multiple process at time, illusion that they have the same memory address, really virtual representation
    - Virtualizing the Memory
        - Abstraction - Address Space
        - An abstraction of a private address space for multiple running processes in a single, physical memory
        - Logically, each process in memory always starts at addresses 0 and end at the maximum size
    - Generating Address Space
        - Compiler - generates an object code file that describes the instructions and data used and created
        - Linker - combines object code into an executable file that defines how the program will be loaded into memory
        - The loader in the OS reads the instructions in an executable file and sets up the logical address space to run the program
    - Address Space
        - All processes have similar address space structure
    - Address Space Structure
        - 0-1KB - Program Code
        - 1KB-2KB - Heap
        - 2KB-15KB - Free (Heap and Stack can both expand)
        - 15KB-16KB - Stack
    - Unix Memory API
        - Stack Memory - declare memory on the stack and the compiler does the rest
        - Heap - all allocation and deallocation
        - malloc(size_t size) - asks for room on the heap
        - free() - frees memory on the heap
        - malloc() and free() are built on top of system calls
    - How to implement memory virtualization?
        - Goal - attaining performance while maintaining control
        - Crux-  How can we build an efficient and flexible virtualization of memory
    - Address Translation
        - Hardware Based Adddress Translation
        - Hardware transforms each memory access e.g. fetch, load store, from virutal to physical address
        - OS must keep track of which locations are free and which are in use, and maintain control over how memory is usedx
    - Simple Scenario
        - Contiguous allocation - processes' address space must be placed contigulousy in physical memory
        - Small address space - less than the physical memory space
        - All address spaces are the same size
    - A Memory Trace
        - Fetch instruction at address 128
        - Execute instruction, load from address 15KB
        - Fetch instruction at address 132
        - Execute
        - Fetch instruction at address 135
        - Execute
    - A Memory Trace
        - Virtualize memory with transparency
    - A simple relocated process
        - Code, Heap, Free, Stack - 16KB
        - Place inside of space with OS and Free Space
        - Allocate from 32KB to 48KB
        - Name - Base and Bounds
        - Need
            - Physical Address = Virtual Address + Base
            - Virtual address is within the bounds
    - Memory Management Unit
        - Bounds Register
        - Base Register
    - Example
        - Base Register Value = 320KB - Physical Memory Start
        - Bounds Register Value = 64KB - Size of process memory and indicates 320KB + 64KB = 384KB is second bounding value
    - OS Issues
        - Find space for its address space in memory when a process is created - solution = maintain a free list
        - Reclaim all of its memory when a process is terminated
        - Perform context switch - save/restore the values of the base and bounds registers to/from memory in PCB
        - Access to the base and bound registers is privileged, special hardware instructions required to access i.e. system call
    - Limitation of Base and Bounds Dynamic Allocation
        - Simple
        - Potentially a lot of free space between the stack and the heap
        - Internal Fragmentation - wate of memory that has been reserved/allocatd to a process but never used

11. 09/23/2022
    - Segmentation
        - Generalized Base and Bounds
        - Logical segments of address space, apply base and bounds for each to different parts of physical memory
        - Beneit - avoid filling physical memory with unused virtual address space
        - Issue - extra to maintain base and size information for each segment
    - How to address segments?
        - Explicit Approach - bit
            - Segment Bit 
            - Offset Bit
        - Implicit Approach
            - If the address was generated from the program counter then the address is within the code segment
            - If the address is based off of the stack or base pointer, it must be in the stack segment
            - Any other address must be the heap
        - What about the stack?
            - Trick - stack grows backwards or negatively
        - The hardware needs to know which way the segment grows
        - Support for Sharing
            - Basic support adds a few bits (protection bits) per segment
                - Indicates whether or not a program can read or write a segment, or perhaps execute code
            - By setting a read only, the same code can be shared across multiple processes
        - OS Support
            - Context Switch - registers must be saved and stored
            - Manage Free Space - when new address space is creatd, OS has to be able to find space in physical memory
    - Fine Grained Segmentation
        - More segments
            - subroutine, symbol table, individual methods
        - Segment Table - maintaing the address translation
    - Simulation - OSTEP - segmentation.py
        - Split
            - seg 0 is code
            - seg1 - stack
            - unallocated
        - Virtual to Physical
            - Virtual - 0-512-1024
                - Seg 0 - 472 maps to Seg 0 Physical 6890
                - Seg 1 - 450 maps to Seg 1 Physical 4692
            - Physical
                - Seg 1 - 4692
                - Seg 0 - 6890
        - Address Translation
            - Determine Violation Range
                - Boundary
                    - Start is 472
                    - End is 1024 - 450 = 574
                    - Violation Range - [472-574] - MEMORY CANNOT BE IN THIS SPACE
            - 202 - VALID
                - Seg 0 is 0 - 472
                - Seg 0 Start in Physical is 6890
                - 6890 + 202 = 7092 - PHYSICAL MEMORY ADDRESS
            - 523 - INVALID
            - 662 - VALID
                - Seg 1 is 574 to 1024
                - Seg 1 start in Physical is 4692
                - Offset from base = 1024 - 662 = 362
                - 4692 - 362 = 4330
            - 414 - VALID
                - Seg 0 is 0 - 472
                - Seg 0 Start in Physical is 6890
                - 6890 + 414 = 7304 - PHYSICAL MEMORY ADDRESS
            - 802 - VALID - TRICKY
                - Seg 1 is 574 - 1024
                - Seg 1 Start in Physical is 4692
                - Offset from base = 1024 - 802 = 222
                - 4692 - 222 = 4470
            - 778 - Valid + Tricky
                - Seg 1 is 574 - 1024
                - Seg 1 start in Physical is 4692
                - Offset from base = 1024 - 778 = 246
                - 4692 - 246 = 4446
            - 310 - VALID
                - Seg 0 is 0 - 472
                - Seg 0 Start in Physical is 6890
                - 6890 + 310 = 7200 - PHYSICAL MEMORY ADDRESS
            - 488 - INVALID
    - Quiz
        - Information
            - Virtual Seg 0 is 0-7554
            - Physical Seg 0 is 441001
            - Virtual Seg 1 is (16384 - 7200) = 9184 to 16384
            - Physical Seg 1 is 278693 and grows backwards
            - Violation Range is [7554-9184]
        - 8376 - INVALID
        - 6634 - VALID
            - Physical Address is 441001 + 6634 = 
        - 12841 - VALID
            - Offset is 16384 - 12841 = 3543
            - Physical Address is 278693 - 3543
        - 4969 - VALID
        - 7808 - INVALID
    - Segmentation Issue
        - External Fragmentation
            - The physical memory quickly becoems full of little holes of free space
            - Each individual is too small for a new segment, too small for allocation
        - Reducing - appraoch to use a free-list managment algorithm that tries to keep large extents of memory available for allocation
            - First Fit - search for first hole that is big enough, starting from beginning
            - Next Fit - search for next hole that is big enough, in circular manner
            - Best Fit - search for smallest hole that is big enough
            - Worst Fit - search for largest hole and see if it fits
        - Example
            - Self Explanatory - don't get cocky about it though
        - Reducing
            - Compact physical memory by rearranging the existing segments
            - Compaction is expensive, as copying segments is memory intensive and thus would use a fair amount of processor time
        - Can we relax the contiguous allocation restraint?

12. 09/28/2022
    - Paging
        - Instead of logical segments, we break up our address space into fixed sized units
            - Each unit is called a page
            - Each page of physical memory is called a page frame
        - Page Table
            - Used to record mapping betwqeen virtual page and physical frame
            - This is a per process data structure
        - Overhead
            - Index, Physical Page Frame, Page Size, dont need to maintain Virtual Page Number
        - Features
            - Pros
                - No external fragmentation
                - Support the abstraction of an address space effectively
                - No assumptions about how the heap and stack grow and how they are used
            - Cons
                - Large maintenance with page table
                - Internal fragmentation for last page
        - Address Translation with Paging
            - To translate this virtual address that the process generated, we split into two components
                - The virtual page number and the offset within the page
                - 6 overall bits for the 64 bytes address space
                - Translate VPN to PFN
        - Page Table Size
            - Page tables can get large, much bigger than segment table or base/bounds pair
            - Example - 32bit address space (20 bit VPN and 12 bit offset
                - 4KB (2^12) page size
                - Maximum 2^20 page table entries for each process
                - 4MB(2^20*4) = page table size with 4 bytes per PTE
                - 400MB of memory of page tables for 100 processes running
        - Where are page tables stored?
            - Too big to be placed in MMU
        - Implementation
            - Linear Page Table - VPN <-> PFN per PTE
            - Extra Bits
                - Valid/Invalid
                - Protection
                - Present
                - Dirty
                - Reference
            - Example Page Table Entry from Xv6
                - Present Bit (P)
                - R/W - Read Write Bit
                - U/S - user/supervisor bit
                - PWT, PCD, PAT, and G - determine how hardware caching works for these pages
                - A - accessed reference biut
                - D - dirty bit
                - the PFN
        - Managing Free Frames
            - Bitmap
                - Each bit is used to indicate whether the frame is free or in use
            - Linked List
                - Each entry specifies a hole or process, the address at which it starts, the length, and a pointer to the next entry
        - Context Switch in Xv6
            - switchuvm(p) = switch to use p's page table
            - switchkvmn() = switch to use kernel's page table
        - Address Translation Workflow
            - Copy virtual memory into register - logical address
            - Split virtual address into VPN and offset
            - Find PFN and offset
            - VPN points to Page Table, return PGN
            - Go to PFN and find value with offset
        - A Memory Trace
        - Simulation
            - paging-linear-translate.py from OSTEP
            - Map Virtual Address to Physical Address
                - Figure out VPN
                - Figure out Offset
            - Example
                - How many bits for VPN?
                    - How many entries in page table? 4, so 2^2 = 4 so 2 bits
                    - How large is address space size
                        - 16k / 4K = 4, so 2^2 = 4 so 2 bits
                - How many bits for offset?
                    - Take your page size and determine number of bits to produce page size
                    - Page Size is 4K i.e. 2^12 = 4096 = 4K so 12 bits
                - How many bits for address space?
                    - 16k i.e. 2^14 = 16384 so 14 bits
                - Convert Hexadecimal to Binary
                    - 0x00003229 = 0011 0010 0010 1001
                - Identify Bits to Ignore
                    - We have 16 bits, but only 14 is neede for addresss space
                - From left, determine VPN
                    - 2 bits for VPN, so 11 is 3 or page table 3
                - Remainder 12 bits is offset
                    - 0010 0010 1011
                - Determine Bits for Physical Mem Size
                    - Phys Mem = 64k so 2^16 = 65536 so 16 bits
                - Take VPN starting address i.e. 0x80000006 or 6 and add offset bits
                    - 0110 = 6
                    - 0010 0010 1011 = offset
                    - 0110 0010 0010 1011 = final address
            - Example
                - 0x00001369 - Convert Hexadecimal to Binay
                    - 0001 0011 0110 1001
                    - VPN is 2 bits
                    - 01 -> points to Page Table 1 which is invalid
                    - Memory address is invalid!
                - Translation Steps 
                    - How many bits for VPN?
                        - 16k / 2k = 8 = 2^3 = 3 bits
                    - How many bits for offset?
                        - 2K = 2^11 = 2098 - 11 bits
                    - How many bits needed?
                        - 3 + 11 = 14
                    - Convert to Binary - 3481
                        - 0011 0010 1000 0001
                    - Take Bottom 14
                        - 110 = 6 = VPN
                        - 010 1000 0001 = Offset
                    - Take Existing - 1D
                        - 0001 1110
                    - Add 5 to Make 16 with Address bottom
                        - 1 1110 + 010 1000 0001 = 1111 0010 1000 0001
                    - Convert to Binary - 3629
                        - 0011 0110 0010 1001
                        - VPN 110 = 6
                        - 1111 0110 0010 1001
                        - 15629
            - REDO EXAMPLES

12. 10/03/2022
    - Improving Paging
    - Issues with Paging
        - How many bits for VPN?
        - How many bits for offset?
        - Page Table Base Register - initial location is begging address of page 1
        - Fetch the content once you have address
        - Validate
    - Faster Translations
        - Translation Lookaside Buffer
            - Part of MMU
            - Cache of popular address translations
        - Upon each virtual memory reference, hardware first checks TLB
        - TLB Hit
        - TLB Miss
        - Worst case is worse than before, TLB and table look
    - Effective Access Time / Average Memory Access Time
    - Expected Value
        - Value One * Probability of One + Value Two * Probability of Two
        - Example
            - TLB Access Time - 2ns
            - Memory Accces Time - 20ns
            - TLB Hit
                - (TLB + Memory) = (2 + 20) = 22ns
                - p1 - hit ratio = a
            - TLB Miss
                - p2 - miss ratio = 1-a
                - (TLB + Memory x 2) = (2 + 20*2) = 42ns
            - Effective Access Time
                - 22 * a + 42 * (1-a) = 22a + 42 - 42a = 42-20a
    - Three Value
        - Access TLB with 100%
        - Access page table in memory 1-a
        - Access Physical location with 100%
        - Example
            - EAT - 2 - TLB
            - EAT = 20 * (1-a) - PAGE TABLE
            - EAT = 20 - Physical Location of Page in Memory
            - EAT = EAT1 + EAT 2 + EAT3 = 32-20a
    - QUIZ - What is the minimum hit ratio for EAT is no longer than 40ns?
        - TLB Access Time is 4ns
        - Memory Access Time is 30ns
        - Expectation - X * p(X) + Y * p(Y)
        - TLB Hit
            - (4 + 30) = 34 ns
            - a
        - TLB Miss
            - (4 + 30*2) = 64ns
            - 1-a
        - Expectation
            - 34ns * a + 64 * (1-a)
            - 34a + 64 - 64a
            - 64 - 30a
        - Answering the Question
            - 64-30a <= 40
            - 24 <= 30a
            - 24/30 = .80 = a
    - TLB Issue - Context Switches
        - The contents of the TLB contain virtual to physical translations that are only valid for the current running process
        - When one prcoess is running, another process amy also be ready in system and the OS might be context switching between them
    - TLB Solution
        - Provide an address space identifier, identified by a few bits
    - TLB Issue - Replacement Policy
        - What entries should be kept?
        - Policy - Least Recently Used, take advange of locality and evict the least used
    - Reducing Page Table Size
        - Issue - large tables, filled with mostly unused or invalid entries
        - Solution - Bigger Page
            - Doubling the page size can cut the page table size in half
            - Issue - it leads to waste within each page, a problem known as internal fragmentation
            - In practice, most systems use relatively small page table 
        - Solution - Paging + Segmentation
            - Use Base and Bounds Segmentation
            - We use the base to hold the physical address of the page table of that segment
            - The bounds register is used to indicate the end of the page table
            - Have one pagee table per logical segment, e.g. three page tables one for the code, heap, and stack parts of the address space
            - Issues - external fragmentation, page table can be of arbitrary size
        - Solution - Multi-Level Page Tables
            - Split Page Table into page sized units
            - If an entire page of page table entries is invalid, don't allocate that page of the page tablea at all
            - Use a page directory to tell you
                - Where a page of the page table is or
                - that the entire page of the page table contains no valid pages
            - Page Directory
                - Each page is stored in the page directory - 1, 2, 3, 
                - If everything in page is invalid, don't provide PFN
                - Page 1 - PFN - something is valid
                - Page 2 - NO PFN - No Allocation
                - Page 3 - NO PFN - No Allocation
                - Page 3 - PGN - something is valid
                - Compress the empty
            - Example
                - Page Size 16B
                - Address Space is 256B
                - Pages are 256 / 16 = 16 Pages
                - What is Physical Address of Virtual Address 0xe4?
                    - Convert to Binary - 1110 0100
                    - How many bits for directory?
                        - 2 bits - Page 0, 1, 2, 3
                    - How many bits for second level?
                        - 2 bits, 4 entries within a page table
                - 11 - Page Directory - Valid 
                - 10 - Page of Page 11 - 2 - Valid
                - PFN is 86
                - Convert 86 to decimal 56 base 16 address
                    - 86 - 0101 0110
                    - Add to last four of address 0100
            - Trade off Between Time and Space
                - More Levels
                    - Smaller Page Table
                    - Slower Paging
                - Fewer levels
                    - Larger Page Table
                    - Faster Paging
                - Prefer two levels of paging

13. 10/05/2022
    - Virtualization: Memory (Beyond Physical Memory)
        - What to do if we run out of memory?
            - Bad Solution - dont allow it to happen, the programmer needs to take care of it
            - Bad Solution - run one process, give it full access to memory, stop it, save all of its state to disk, then load another process
                - Issue - extremely slow particulary as mmeory grows
                - Issue - saving the entire contens of memory to disk is brutally non-performant
        - Storage Hierarchy
            - Disk -> Memory -> Cache -> Register
            - Larger <- Size -> Smaller
            - Slower <- Speed -> Faster
        - Swap Space
            - Create a swap space (page size units) to
                - Swap out pages that are currenlty not in great demand
                - Swap some into memory from the swap space
                - Such as virtual memory in hard disk in Windows
                - The OS will need to remember the disk address of a given page in the swap space
            - Example
                - 4 Page Physical Memory
                - 8 Page Swap Space - 7 Occupied, 1 Free
                - 4 Processes (3 active, 1 inactive)
            - Features
                - Pros - addition of swap spaces creates the illusion of a large virtual memory for multiply concurrently running processes
                - Cons - overhead of slow swapping
        - Page Fault
            - If the page is not in physical memory, we must load from swap space
            - Present Bit - used to indicate whether the page is present in physical memory
                - 1 - Page Hit
                - 0 - Page Fault
            - Demand Paging
                - Initially most of the page is not present in physical memory
                - On demand, a non-present page will be loaded into physical memory
            - Recall: Paging with TLB - If Present Bit is 0 at Physical Memory Location
            - Upon a page fault, the OS is invoked to service the page fault by swapping the page into memory
            - How will the OS know where to find the desired page?
                - Use the PFN of the PTE in the page table for a disk address
            - The process will be in the blocked state while the page fault is being serviced
            - Paging with TLB + Swap Space
                - CPU has Virtual Page Number and Offset
                - Check Translation Lookaside Buffer
                - If TLB Hit
                    - Find Page Frame Number and Offset
                    - Use PFN and Offset to access page from physical memory
                - If TLB Miss
                    - Go to Page Table
                    - Check Present Bit
                    - If 1
                        - Use PFN and Offset to access page from physical memory
                    - If 0
                        - Enter swap space
                        - Allocate Page Frame
                        - Swap page into Physical Memory
                        - Return PFN and Offset
                        - Access Page from Physical Memory
            - Effective Access Time with Page Faults
                - Page Fault Rate 0 <= p <= 1.
                    - if p = 0, no page faults
                    - if p - 1, every page reference is a fault
                - Access Time
                    - p = 0, x1 = 2 * memory access time
                    - p = 1, x2 = 2 * memory access time + swapping time
                - Calculating EAT (without TLB)
                    - EAT = (1-p) * x1 + p * x2
                - Example
                    - 9 out of 10 reference in memory
                        - Hit Rate = 90%
                        - Page Fault = 10%
                    - Access Time
                        - memory access time = 100 nanoseconds
                        - Swapping = 10 milliseconds
                    - EAT
                        - (2 * 100ns * .9) + (2 * 100 + 10000000) * 0.1 = 1,000,200 ns
                    - EAT
                        - Page Fault - .1%
                        - (2 * 100ns * .999) + (2 * 100 + 10000000) * 0.001 = 10,200 ns
                - Disk Access is magnitude of difference from RAM
            - Quiz Page Fault
                - Memory Access Time = 200 ns
                - Swapping Time = 15ms
                - Question: What is the maximum page fault rate for achieveing maximum EAT time 1ms?
                - (2 * 200ns) * (1-p) + (2 * 200ns + 15,000,000ns) * p <= 1,000,000
                - 400 * (1-p) + (400 + 15,000,000) * p
                - 400 - 400p + 15,000,400p
                - 400 + 15,000,000 * p <= 1,000,000
                - 15,000,000 * p <= 999,600
                - p <= 999,600 / 15,000,000 = 6.664%
            - What is memory is full?
                - The os needs to page out one or more pages to make room for the new page
                    - Page Replacement
    - Page Replacement
        - How can the OS decide which page (or pags) to evict from memory
        - Page Replacement Policies
            - Performance - want lowest page fault rate
            - Evaluation method - eval algorithm by running it on a particular string of memory references, and computing the number of pages faults on that string
            - Policies
                - First In First Out - First Come First Served
                    - Reference String - 1 2 3 4 1 2 5 1 2 3 4 5 (Pages)
                    - Three Free Frames
                        - 1,2,3 are Cold Start Misses - Lower Cost, No Swapping as Free Frame
                        - Add 1 so [1] - X
                        - Add 2 so [1, 2] - X
                        - Add 3 so [1, 2, 3] - X
                        - Evict 1 so [4, 2, 3] - X
                        - Evict 2 so [4, 1, 3] - X
                        - Evict 2 so [4, 1, 2] - X
                        - Evict 4 so [5, 1, 2] - X
                        - 1 is a hit
                        - 2 is a hit
                        - Evict 1 so [5, 3, 2] - X
                        - Evict 2 so [5, 3, 4] - X
                        - 5 is a hit
                        - Page Faults - 9
                        - Total Accessses - 12
                    - Four Free Frames
                        - Page Faults - 10
                    - Belady's Anomaly - more frames more page faults - not always, anomalous value occurs
                - OPT - Optimal Approach - Waiting Time is Longest
                    - Replace page that will not be used for the longest period of time
                        - How do you know this?
                        - Used for measuring how well your algorithm performs
                    - Example
                        - Four Frames
                            - Add 1 so [1] - X
                            - Add 2 so [1, 2] - X
                            - Add 3 so [1, 2, 3] - x
                            - Add 4 so [1, 2, 3, 4] - X
                            - 1 hit
                            - 2 hit
                            - 5 - MIss - Look At List so [1, 2, 3, 5] - X 
                            - 1 hit
                            - 2 hit
                            - 3 hit
                            - 4 - Miss - Look at List (If list end, just evict) so [4, 2, 3, 5] - X
                            - 5 hit
                - Least Recently Used - Approximates OPT

14. 10/07/2022
    - Swapping Policy - Least Recently Used
        - Replace the page that has not been used for the longest period of time
        - Action - look backward to approximate the futuree, whichever hasn't been referenced
        - Principle of Locality
            - Programs tend to access certain code sequences and data structures quite frequently, temporal locality
            - Use history to figure out which pages are important
        - LRU Example
            - Reference String - 1 2 3 4 1  5 1 2 3 4 5
            - Each step, look back, out of what it is in the frame, which has been used leat recently?
        - LRU Implementation
            - Timestamp
                - Must keep record of what has been recently accessed, each page has a timestamp, sort by oldest time
                - Issue - requires a lot of bits, reliant on clock
            - Counter
                - Increment each time
                - A logical clock for every page table entry and a global clock in the clock register
                - For every memory reference, global counter is incrementeds and copied to the counter for the page table entry of memoery reference
                - When a page needs to be changed, choose the page with the smallest logical clock
        - LRU Approximation
            - Few computer systems provide sufficient hardware support for true LRU due to the overhead
            - Approximation Approach
                - Basis - Reference Bit
                    - When a page is brought in, its bit set to 0
                    - When a page is referenced, its bit set to 1
                    - Replace the one which is o (if one exists)
                        - Issue - many 0s, we do not know which one
                - Second Chance or Clock Algorithm
                    - Date Structure - Circular Lst
                    - Pointer p: next potential victim
                    - if p* = 0, replace this page and new p* = 0
                    - Otherwise, *p = 0, p++ until *p == 0, iterate till you find 0
        - Quiz Page Replacement
            - FIFO
            - OPT
            - LRU
            - Second Chance
        - Other Options
            - Counting Based
                - keep a counter of the number of refrence that have been made
                - Least Frequently Accessed - replace page with smallest count
                - Most Frequently Accessed 
            - Random Page Replcaement
                - Simply pick a random page, similar to FIFO
            - 80-20 Workload
                - 80% of the references are made to 20% of the pages (hot pages)
                - The reamining 20% of the references are made to the remaining 80% of the pages (cold pages)
                - It has locality within it
                - Results
                    - X Axis - Cache Size (Blocks)
                    - Y Axis - Hit rate
                    - No Locality
                        - LRU, FIFO, and RAND same
                    - 80-20%
                        - OPT Best
                        - LRU Second
                        - FIFO and RAND
                        - Second Chance? Approximates LRU well
            - Other Techniques
                - Page Buffering - keep a pool of free frames to avoid waiting for teh victim page to be swapped out
                - Page Pre-fetching - guess that a page is about to be used, and thus bring it in ahead of time
                - Demand Paging - bring the page into memory when it is accessed
                - Clustering of Writing - collect a number of pending writes together in memory and write them to disk in one (more efficient) write
    - Thrashing
        - Issue with Page Replacement
            - Initial - CPU utilization is very low
            - Increase # of processes
            - Memory is used up, then use page replacement
            - This leads to more page faults
            - Actual lower CPU utilization
        - Graph
            - Degreee of Multiprogramming X Axis
            - CPU Utilization Y Axis
            - As x increases, y increases, until a point where you run out of memory, not page eviction results and CPU utilization drops
        - Thrashing
            - Under Page Replacement
                - Move frames from one process to another process
                - No process has enough frames to execute
                - Getting worse as more processes admitted
            - Idea - let some process hold enough frames but the others wait?
            - Approach - allocate enough frames to a process to accomodate its current locality and disallow page faults to use page in the locality as victim
            - How to find locality?
                - Working Set Strategy
                    - Page Reference Table
                    - Working Set - Approximation of locality
                    - Keep the most recent delta page references
                    - If a page is in active use, it will be in the working set

15. 10/12/2022
    - Concurrency - Thread
        - Multi-Threading
            - Example
                - Web Server w/ multiple clients connected through the internet
                - Problems
                    - Thousands of concurrent access
                    - Enormous waiting time for some clients with server running as a single process
                - Original Solution
                    - Create multiple process to serve all accesses
                    - Problem - time consuming and resource intensive to create a new process, process competing for CPU
                - Better Soltion - light weight process called a thread
            - New Abstraction - Multi-Threading
                - Traditional Process - a single poiint of execution within a prorgram
                - Multi-Threaded - more than one point of execution, multiple program counter
                    - Threads in a process share the same address space
                - Features
                    - Unique for Each Thread
                        - Program Counter, Register Set, Stack
                    - Shared Address Space
                        - Code, Data, Heap, Open Files
                - Benefits
                    - Resource Sharing - instructions, global variables, files, etc.
                    - Responsiveness - fast thread creating
                    - Parallel Executing
                        - Utiliizing multi cores e.g. adding two large arrays together in parallel
                        - Avoiding blocking program progress due to slow I/O e.g. loading web pages with images
                - Multithreaded Web Server
                    - Dispatcher Thread - manages allocation
                    - Worker Thread - handles individual requests
                    - Web Page Cache - shared
            - Thread Control Block
                - Program Counter
                - Thread State
                - Register Values
                - Stack Pointer
                - Pointer to Process Control Block
            - Context Switches
                - Same - address space remains the same, must change registers
                - Differences
                    - Context switch is performed between threads as compared to processes
                    - Need one or more TCBs to store the state of each thread instead of PCBs
            - Process Exeuction
                - A process with two threads of execution on a single processor
            - Summary Process vs. Thread
                - Process
                    - memory isolation for protection and reliability
                    - Chrome Browser - individual tab is process, to protect and isolate data
                - Thread
                    - Fast to create threads
                    - Saving memory space by sharing among different threads from the same process if possible
                    - Web Servers - manage multiple requests at once
            - Thread Library
                - Library provides API for creating and managing threads
                - PThreads
                    - IEEE POXIS strandard, Unix and Linux
                    - Defined as a set of C language programming types and procedure calls
                - APIs
                    - pthread_create() - creates a new thread
                    - pthread_exit() - terminates the thread
                    - pthread_join() - blocks the calling thread until the specified tid thread terminates
            - Race Condition
                -  Issue - counter++, three instructions, load, increment, store
                    - These three instructions to not execute atomically (all at once), causing strange things to happen
                - Uncontrolled Scheduling - Interleaved Operations
                    - Simulation x86.py
                    - Exercise - 17:00 minutes
            - Why in OS?
                - The OS was the first concurrent program, and many techniques were created for use within the OS
            - Key Concurrency Terms
                - Critical Section - segment of code, in which multiple threads may be changing shared variables e.g. a counter
                - Race Condition - multiple threads executing the critical section can result in a race condition, e.g. two threads doing counter++, not necessarily parallel
                - Mutual Exclusion - if one's thread PC is already inside critical section, other will be prevented from entering theirs
            