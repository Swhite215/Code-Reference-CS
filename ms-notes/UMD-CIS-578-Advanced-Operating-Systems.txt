Security and Privacy in Cloud Computing

1. 09/05/2024 - Summary of Lecture
    - Introduction
        - Operating System - intermediary between a user of a computer and the computer hardware
        - System Structure
            - Hardware - basic computing resources e.g. CPU, memory, I/O devices
            - Operating System
            - Application Programs - compilers, browsers, database, video games
            - Users - people, machines, other computers
            - Bottom Up - Hardware -> Operating System -> Application Programs -> Users
        - System Definition
            - Resource Allocator - manage all resources, decides between conflicting requests for efficient and fair resource use
            - Control Program - controls execution of programs to prevent errors and improper use of the computer
            - Kernel - program running at all times on the computer
                - Goal - handle core functions that are really important e.g. scheduler
        - Computer Start-Up (Multi-Step)
            - Problem - OS needs to interface with hardware it does not know about
            - Solution - Firmware, enough data to identify locations to load OS
            - Bootstrap Program - BIOS - ROM or EPROM - initialize all aspects of system
        - Organization
            - Memory
            - Bus
            - Components - CPU, Disk, USB, Graphiucs
        - Interrupt
            - Trap or Exception to generate an interrupt
            - Control transfers to the interrupt service routine through the interrupt vector, address of all service routines
            - Alternative - Polling
            - Interrupt Handling
                - Preserve state of the CPU by storing registers and program counter
                - State - values in registers, program execution point, stack, heap, program code
        - Storage Hierarchy
            - Registers (Fast, Expensive, Small)
            - Cache
            - Main Memory
            - Solid State Disk
            - Hard Disk
            - Optical Disk
            - Magnetic Tapes (Slow, Cheap, Large)
            - Access Time - .n nanoseconds to 5m nanoseconds
        - Caching
            - Storing important, frequently used data, closer to the CPU
            - If data in cache, cache hit, if not, cache miss, must load data to cache
        - Direct Memory Access
            - For certain devices, driver should talk directly to memory, improving speed of system
            - Data can be transferred in a block
            - CPU is not involved
        - How a Modern Computer Works - Von Neumann
            - Program, Data - Sequence of Bytes
            - Simplify to handle instructions
        - Processing
            - Multiprocessor Architecture - single main memory, multiple CPUs
            - Asymmetric - each processor is assigned a specific task
            - Symmetric - each processor performs all tasks
        - Multi-Programming
            - Organization of jobs so CPU always has one to execute
            - Jobs are run via job scheduling
            - A single CPU can be multi-programmed
        - Multi-Threading - single program whose tasks are split amongst multiple threads
        - Timesharing - multiple userrs on a single system, keep CPU busy
        - Modes
            - User - abstracted layer through which user's request access to computer resources
            - Kernel - privileged access to important resources and system calls
            - Transition
                - User Process -> System Call -> Trap to Kernel Mode -> Execute System Call -> Return from Trap
        - Process Management
            - Process - mental model for a program
            - Requirements - CPU, memory, I/O, files, data, program counter, registers
        - Protection and Security
            - Protection - mechanism for controlling access of processes or users to resources defined by the OS
            - Security - defense of the system against internal and external attacks
            - Users
                - User Identities - name, id, number
                - User ID assocaited with all files
                - Group Identifier - set of users to be defined and controls managed
                - Privilege Escalation - allows user to change to effective ID with more rights
        - Model of Computer Networks/Systems
            - Client/Server - client communicates with hosted servers
            - Peer to Peer - clients directly communication with each other
    - OS Structure
        - Operating System Services
            - user interface, program execution, I/O operations, file system, communications, error detection
            - resource allocation, accounting, protection and security

2. 09/12/2024
    - View of Operating System
        - Kernel - Mono vs. Micro
            - Monolithic - shared, + performance, - security
            - Micro - isolated, message passing, +security, - performance
        - Application Programming Interface
            - System Call Implementation
                - Name = Unique ID
                - Unique ID - optimized look up of call through index into an array
            - Parameter Passing
                - Registers - fast, normally only a few registers are available
                - Block Memory - store all, pass starting in a list, immediate access by memory address multiple times
                - Stack - push and pop, must push and pop to target value
            - Types of System Call
                - File Management
                - Memory Management
                - Communication
                - Parallel Programming
            - Background Services/Processes
                - Background - process started, memory allocated, put process to sleep
        - Design
            - Policy - what will be done
            - Mechanism - how will it be done
        - Layered Approach
            - Layer 0 (Hardware) -> Layer X (Kernel) -> Layer N (User)
        - Microkernel System Structure
            - Positive - separation of concerns, modularity, reduction of bugs
            - Negative - communication between microkernels,
            - Goal - essential kernel mode functionality is encapsulated as independent kernels that communication with each other
            - Other - Loadable Kernel Module
    - SMP vs. AMP
        - Multiprocessing/Multiprocessors
            - Why:
                - Limitation of a single CPU
                - Multiple Users
                - Multiple Applications
                - Multi-Tasking
                - Responsiveness and Throughput
            - Issue: how do I make use of multiple processors?
        - Symmetric Multiprocessing - all cores are treated equally
        - Asymmetric Multiprocessing - one or more cores are specialized
        - Instructions and Data Streams
            - Single Instruction Single Data - single instruction on single piece of data (traditional)
            - Multiple Instruction Multiple Data - multiple instructions for multiple pieces of data
            - Single Instruction Multiple Data - same instruction on multiple pieces of data (graphics processing)
            - Multiple Instruction Single Data - redundancy, verification
        - Processor Coupling
            - Tightly Coupled - CPUs are connected over a bus
                - + access to a shared central memory
            - Loosely Coupled - processors interconnected via a high speed communication system
                - + isolation
        - Architectures
            - Message Passing
                - - latency
                - + isolation
            - Shared Memory - multiple programs acceessing same memory
                - + low latency, easy, difficult to scale
        - Memory Access
            - Bus Based Uniform Memory Access - access time for each CORE is uniform
            - Non Uniform Based Memory Access - access time to local memory for process is faster than access to another processors's remote memory
        - Operating System Distribution
            - Option #1 - OS on all CPUs
            - Option #2 - OS on one CPU, system calls passed to Main CPU for processing
            - Option #3 - Kernel is on everything
    - Von Neumann Model
        - What - program instructions and data are both stored as sequences of bits in computer memory
        - Memory Interface
            - Memory Address Bus - memory addres for the addres bus
            - Memory Data Bus - ?
            - Address Space - amount of data that can be stored
            - Addressability - number of bits stored in each memory location
        - Processing Unit
            - Arithmetic and Logic Unit - arithmetic, boolean, logical
            - Word Length - number of bits processed by the ALU
        - Control Unit
            - What - directs execution of the program, prevent bus conflicts and timing/propogation problems
            - Program Counter - points to next instruction
            - Instruction Register - currently executing instruction
            - Status Register - last instruction executed and system parameters
        - Reduced Instruction Set Computer - one instruction per clock cycle
        - Complex Instruction Set Computer - multiple clock cycles per instruction
        - Bottleneck - all instructions and data has to be fetched from memory, path to memory is bottleneck
    - Process Continued
        - What - program in exeuction
        - Program - passive
        - Process - active
        - Parts
            - Code
            - Program Counter
            - Stack - temporary data, grows down
            - Data - global variables
            - Heap - dynamic memory, grows up
        - States - new, running, waiting, ready, terminated
        - Diagram - new -> ready, ready -> running, running -> ready, running -> waiting, running -> terminated
        - Process Control Block
            - Process State
            - Process Number
            - Program Counter
            - Registers
            - Memory Limits
            - Open Files
        - Scheduling
            - Job Queue - set of all processes in the system
            - Ready Queue - all processes residing in main memory, ready and waiting to execute
            - Device Queues - set of processes waiting for an I/O device
            - Schedulers
                - Short Term - selects which process should be executed next and allocates CPU
                - Long Term - selects which processes should be brought into ready queue, controls multiprogramming, mix of CPU and I/O
                - Mid Term - remove process from memory, store on disk, bring back in from disk i.e. swapping
            - Bounds
                - I/O Bound - spends more time doing I/O than computations, many short CPU bursts
                - CPU Bound - spends more time doing computations, very few long CPU bursts
        - Context Switch
            - Context = PCB
            - What - when CPU switches to another process, the system must save the state of the old process and load the saved data for the new process
        - Process Creation
            - Parent - initial process
            - Children - created by parent
            - Process Identifier - PID
            - Resource Sharing
                - Parent and Children share all resources
                - Children share subset of Parent's resources
                - Parent and Children share no resources
            - Execution Options
                - Parent and Children execute concurrently
                - Parent waits until Children terminate

3. 09/19/2024
    - Multitasking in Mobile Systems
        - Foreground Process - controlled via user interface
        - Background Process - in memory, running, but not on display, cost on resources if background process number grows unbounded
    - Queues
        - Ready Queue - linked list of PCBs
        - Device Queue - for each device, a linked list of PCBs, offloaded work for file access
    - Process Creation - fork() and exec()
        - Tree of Processes in Linux
            - Create first process - must load this process directly from memory on init, must be coded and defined e.g. assembly language
            - Process ID 1 - starts other procesess
            - Question - every process is a child of what process? init
        - fork() - system call to create a new process, why, duplicate of parent, change PCB to new contents i.e. new program
        - exec() - system call used after fork() to create a new process
        - exit() - system call used to end execution
        - wait() - system call used by parent to wait for child
        - If parent or child never terminate, they never release their resources
        - Improvement - dont copy all data if all is not going to be used, waste of energy
    - Process Termination
        - Cascading termination - all children etc
        - How is Chrome like a mini operating system? Multiple processes that handle specific tasks
    - Interprocess Communication
        - Message Passing
            - Good - process are isolated, security
            - Bad - messages add overhead
        - Shared Memory
            - Good - fast, easy to setup, good performance
            - Bad - if any process corrupts memory, it affects the other, security
        - Cooperasting Processes
            - Independent - process cannot affedct or be affected by exeuction of another
            - Cooperating - process can affect or be affected by execution of another process
        - Direct Communication
            - Process A -> Process B
            - A communication link must be established
        - Indirect Communication (Mailbox)
            - Messages are directed and received from mailboxes
            - Link established if process share a common mailbox
        - Communication
            - stdin, stdout, stderr
            - dir | sort > output.txt
    - Synchronization
        - Blocking (synchronous, order is preserved, wasting time)
            - Blocking Send - sender is blocked until the message is received
            - Blocking Receive - receiver is blocked until a message is available
        - Non-Blocking (asynchronous)
            - Non-Blocking Send - sender sends the message and continues
            - Non-Blocking Receive - receiver receives valid or null
    - Buffering
        - Queues of messages attached to the link
        - Zero Capacity - no messages are queued
        - Bounded Capacity - sender must wait if link full
        - Unbounded Capacity - sender never waits
    - Socket
        - Endpoint for communication, IP and Port
        - < 1024 are well known ports
        - 127.0.01 - loopback
    - Pipe
        - Conduit allowing two processes to communicate
        - Ordinary Pipe - parent child relationship, unidirectional, producer writes to one end, consumer reads from other end
        - Direction: Unidirectional vs. Bidirectional
    - Signals
        - Events emitted, that can be processed with a signal handler to perform a certain task
        - Security - how do you avoid signals emitted from programs from affecting others? - user control
    - Threads
        - Concurrency vs. Parallelism
            - Concurrency - multiple processes on single core
            - Parallelism - multiple concurrent processes on multiple cores
        - Single Thread vs. Multi Threaded
            - Threads can share code, data, and files
            - Threads have their own registers and stack
            - Context Switch between threads in the same process is faster than context switch between two processes

4. 09/26/2024
    - Threading
        - Single and Multithreaded Processes
            - Why? - share relevant context data of the process with multiple workers and avoid context switching
            - Issue - sharing data requires sophisticated technique
            - What - thread includes regsiters, staack, and shares code, data, and files
        - Amdahl's Law
            - The serial portion of the program constraints the speed up granted by multiple cores
            - Key - you CAN'T always parallelize all of the program
            - SpeedUp = 1 / (S + ((1-S)/N)), as N approaches infinity i.e. # of cores, speed up approaches 1/S
        - User Threads and Kernel Threads
            - User Threads - management done by user-level threads library
                - POSIX Pthreads
                - Window Threads
                - Java Threads
            - Kernel Threads - supported by the kernel
        - Models
            - Many User Threads to One Kernel Threads
                - Pro - one kernel space thread dedicated to process
                - Con - any thread using kernel causes all to block
            - One User Threads to One Kernel Threads
                - Pro - more concurrency than many to one
                - Con - number of threads per process may be restricted
            - Many User Threads to Many Kernel Threads
                - Con - difficult to implement
            - Two Level Model
                - Pro - combination of benefits from different models
                - Con - difficult to implement
        - Thread Libraries
            - User Space Library
                - Pro - issues do not affect kernel
                - Con - if thread needs access to kernel
            - Kernel Space Library
                - Pro - can map one to one
                - Con - issues affect entire system
            - Pthreads - standard mechanism API for thread creation, not necessarily successful
            - Implicit Threading - creation and management of threads done by compilers and runtime libraries
                - Thread Pools - benefit for short running threads with overhead, instead leave created threads unallocated till needed, can create upper limit on threads
                - OpenMP - identify parallel region, create as many threads as there are cores
                - Grand Central Dispatch - identify parallel sections, add to dispatch queue and assign available thread when removed
                    - Serial - FIFO order remove one at a time
                    - Concurent - FIFO order remove as group
        - Linux Threads Clone - more control
        - Signal Handling
            - Signal - notify a process that a event has occurred
            - Signal Handler
                - Signal is generated by event
                - Signal is delivered to a process
                - Signal is handled by one of two signal handlers
                    - Default
                    - User Defined
            - Signal Handling
                - Deliver signal to the thread which the signal applies
                - Deliver signal to every thread in process
                - Deliver signal to certain threads
                - Assign a thread to receive all signals
            - Thread Cancellation
                - Deferred - thread finishes
                - Asynchronous - terminate thread immediately
            - What - misbehaving thread, signal sent, is single thread or entire process killed?
            - What - stopping thread and losing intermediate values
                - Thread Local Storage - allows each thread to have its own copy of data, maintain state across invocation
    - Remote Procedure Call
        - What - take advantage of doing calculations on a remote system, returning results to the client
        - Client/Server Communication
            - Server - program or collection of programs that provide a service
            - Client - program that uses the service, sends requests to perform actions and server responds with data
        - Messages
            - Issue - manual, lack of standardization, scaling
        - Procedure Calls
            - What - servers export a set of procedures that can be called by client programs
            - How - clients do a local procedure call, as though they were directly linked with the server
        - Issues
            - How do we make the remote part of RPC invisible?
            - What are the semantics?
            - How do we connect to servers?
            - How do we handle heterogenity?
            - How do we make it fast?
        - RPC Model
            - Server defines interface using Interface Definition Language i.e. names, parameters, and types for all client callable server procedures
            - Stub Compiler - reads IDL declarations and produces stub procedures
                - Server Programmer implements service procedures and links to server side stubs
                - Client Programmer implements client procedures and links to client side stubs
                - Stub manages communication between client and server
        - Marshaling - packing of procedures parameters into a message packet
            - Conversion of Big-Endian to Little-Endian and reverse
            - When data is sent, convert from host to standard network format
            - When data is sent, convert from standard network format to host
        - Binding - process of connecting client to the server
            - Server Start Up - export interface, identify iteself to name sever, tells RPC it is up
            - Client Start Up - import server, RPC runtime finds location and establishes a connection
        - Goal - make Remote Procedure Call look like a local call
        - Issues
            - Remote Service address space is different from clien 
            - Machines and Networks fail
            - Passing Reference Parameters
                - Replace with Pass by Copy/Restore
                - Need to know size of data to copy
            - Remote Process Loop
            - Partial Failures
            - Latency
            - Memory Access
        - Partial Failures
            - Strawman Solution - reboot every time
            - Break Transparency
                - Exactly Once - impossible
                - At Least Once - idempotent, run function again and answer does not change
                - At Most Once - zero, dont know, or once
                - Zero or Once - transactional, makes sense in practice

5. 10/3/2024 - Process Synchronization
    - Problem: Producer and Consumer
        - Producer - creates data
        - Consumer - consumes data
    - Race Condition
        - counter++ and counter--, if you can't guarantee the order of execution, the value in the register may not be what is expected
        - What - when two or more threads or processes attempt to update the same shared value
        - Critical Section Problem - when one process is in critical section, must communicate this to other processes to ensure only one enters
        - Critical Section - make as small as possible i.e. enter, change value, and exit. Larger the critical section, more likelihood that other processes will be impacted
    - Solution
        - Mutual Exclusion - no other prcoess can be executing in critical section
        - Progress - all process need to make progress
        - Bounded Waiting - limit to waiting
        - Preemption - allows preemption of process when running in kernel mode
    - Peterson's Solution
        - Turn - which process can enter?
        - Flag - are you ready to enter?
        - Check Flag -> If Flag and Your Turn -> Enter -> Exit
        - Issues - how does hardware implement the checks, etc.?
    - Locks
        - Acquire -> Critical Section -> Release Lock
    - Synchronization Hardware
        - Test and Set - Atomic - Test Target Value, Update Target, Return New Value
        - Compare and Swap - Atomic - Compare memory with value, if same, update memory location
    - Mutex Locks (SpinLock)
        - Software Abstraction - Atomic - acquire and release are implemented in hardware
    - Sempahore
        - S - integer value
        - wait(), while resources <= S, wait to be available, decrement S and enter critical section
        - signal(), increment S
        - Counting Sempahore - s can be range of values
        - Binary Semaphore - mutex
        - Can violate bounded waiting while in critical section... 
    - Issues
        - Deadlock - two or more processes waiting indefinetely
        - Starvation - process never removed from semaphore queue in which it is suspended
        - Priority Inversion - lower priority process holds a lock needed by higher priority
    - Problems
        - Readers-Writers Problem - many readers at once, only one writer at a time
    - Condition Variables - wait() - suspend, signal() - resume one process\
    - CPU Scheduling
        - Basic Concepts
            - Goal - keep CPU as busy as possible
            - CPU is mixed with processes that are CPU bound or I/O bound
            - CPU burst times are mostly small
        - Dispatcher - givers control of the CPU to the process selected by the short term scheduler
        - Criteria
            - CPU Utilization
            - Throughput - number of procesese that finish in a unit of time
            - Turnaround Time - time to execute a process
            - Waiting Time - amount of time a process has been waiting in ready queue
            - Response Time - amount of time between request and first response
    - Scheduling Algorithms
        - First Come First Served - prcoess executes in entirety in the order that they arrive
            - Convoy Effect - short process behind long process
        - Shortest Job First - optimal, however requires knowing the next burst time of a process, length can be an aspect of burst history
            - Exponential Averaging - use estimate for burst times for shortest job first
        - Shortest Remaining Time First - preempty process when a job of a shorter burst time arrives
        - Priority Scheduling
            - Priority - lower number = higher priority
            - Preemptiion - higher priority replaces lower priority
            - Nonpreemptive - lower pririty is not removed
            - Starvatio - low priority processes may never execute
            - Solution to Starvation - Aging - as time progressses, increase the priorty of the queue
        - Round Robin - same time quantum for all in queue, a lot of processes and short quantum can lead to thrashing, one process and long quantum that is FCFS


6. 10/10/2024 -
    - CPU Scheduling
        - Multilevel Feedback Queue
            - Types of Processes
                - System - high priority, low quantum
                - Interactive - Foreground - RR
                - Batch - Background - FCFS
                - Student - low priority - high quantum
            - Queues
                - Small Quantum - High Priority
                - Medium Quantum - Medium Priority
                - Large Quantum - Low Priority
            - Action - move processes through queues by increasing priority
        - Thread Scheduling
            - User Level Thread
            - Kernel Level Thread
            - Note - when threads are supported, threads are scheduled instead of processes
            - Process Contention Scope - fair allocation per process, priority set by programmer
            - System Contention Scope - all threads in contention with all other threads
        - Multiprocessor Scheduling
            - Symmetric Multiprocessing - each processor is self scheduling, all processses in common ready queue
            - Processor Affinity - process has affinity for processor on which it is currently running
                - Soft Affinity - will run on another processor, attractive for trying to maximize processor use
                - Hard Affinity - will only run on dedicated processor, may result in idle processor
                - Switching
                    - Cache is an issue when switching to new processor, which will slow down the CPU
            - Load Balancing - attempt to keep workload evenly distributed across all processors
                - Push Migration - push task from overloaded CPU to another CPU
                - Pull Migration - idle processors pull waiting tasks from busy processor
        - Multithreaded Multicore System
            - Thread - rotate between compute and memory to ensure CPU and I/O bound jobs are always 
        - Real Time CPU Scheduling
            - Soft Real Time - no guarantee as when critical real time process will be scheduled
            - Hard Real Time - task must be serviced by its deadline i.e. fixed time interval
            - Latencies
                - Interrupt Latency - time from arrival of interrupt to start of routine that services interrupt
                - Dispatch Latency - time for schedule to take current process off CPU and switch to another
                - Time Frame - Interrupt Processing + Dispatch Latency + Real Time Process Execution
            - Priotity Based Scheduling
                - Scheduler must support preemptive, priority based scheduling, and ability to meet deadlines
                - Periodic - ones require CPU at constant intervals
            - Rate Monotonic Scheduling
                - Priority - assigned based on the inverse of its period
                - Short Period = High Priority
                - Long Period =  Lower Priority
                - Missed Deadlines
                    - As you mix odd integer intervals may result in missed deadlines, we need integer multiples
                - Earliest Deadline First Scheduling - priorities are assigned according to deadlines
                    - Earlier the deadline, higher the priority
                    - Later the deadline, lower the priority
        - Linux Scheduling
            - Version 2.54 - Constant Order Scheduling - Priority
                - Realtime != Meeting Deadline
                - Lower numerical jobs has a higher priority i.e. scheduled more quickly
                - Nice, allowing to give up priority, to reduce priority
                - Issue = poor response time for interactive processes
                - Red Black Trees - grab highest priority job in as little time as possible
                    - When CPU is available, can rebalance the tree for the highest priority
                - Virtual Run Time - tracked, scheduler picks task with lowest virtual run time to run
        - Windows Scheduling
            - Priority Based Preemptive Scheduling
            - Highest priority thread runs next
        - Algorithmic Evaluation
            - What - How to select CPU scheduling algortithm for an OS?
            - Deterministic Modeling - analytic evaluation, take a predetermined workload and define performance of each algorithm for that workload
                - For each algorithm, calculate minimum average waiting time
                - Problem - how to scale this simulation to large numbers of processes and threads
            - Queueing Models
                - Describe the arrival of processes, and CPU and I/O bursts probabilistically
                - Commonly exponential and described by mean, statistical distribution
                - Computes average throughput, utilization, waiting time, etc.
            - Simulations
                - Generate random CPU or I/O bursts and arrival times
                - Run simulation with particular scheduling and evaluate performance statistics
    - Deadlock 
        - Four Simultaneous Necessary Conditions
            - Mutual Exclusion - only one process at a time can use a resource
            - Hold and Wait - a process holding at least one resource is waiting to acquire additional resources held by other processes
            - No Preemption - a resource can be released only voluntarily by the process holding it
            - Circular Wait - set of processes each waiting for a resource held by another process
        - Deadlock - NONE of the tasks can make progress on their individual job
            - Reality - if deadlock occurs, quit and give up
        - Resource Allocation Graph
            - Process - running process
            - Resource - critical section resource
            - Request Edge - directed edge from process to resource
            - Assignment Edge - directed edge from resource to process
            - Example One - not deadlock, if process 3 finishes and relinquishes R3, the remaining processes will run
            - Example Two - deadlock, circular wait
            - Example Three - cycle but no deadlock, one process has a resource and needs nothing else
            - Facts
                - If no cycles -> no deadlock
                - If cycle ->
                    - if only one instance per resource type, then deadlock
                    - If several instances per resource type, possibly deadlock
        - Handling Deadlocks
            - Deadlock Prevention
                - Remove mutual exclusion
                - Remove hold and wait - when requesting a resource, cannot be holding another
                - Add Preemption - release all resources if requesting a held resource
                - Remove circular wait - impose a total ordering of all resource types, must request in particular order
            - Deadlock Avoidance
                - A Prioi - all processes need to know resources required, impossible
                - Safe State - for all processes the resources needed can be satisfied with currently available resources
                - Facts
                    - If system in safe state -> no deadlocks
                    - If system in unsafe state -> possibility of deadlock
                - Algorithms
                    - Resource allocation graph
                    - Banker's Algorithm
                        - Each process makes claim
                        - When requesting process must wait
                        - When all resources obtained, must return in finite time
            - Allow Deadlock and Recover
                - Detect Deadlock
                - Wait for Graph - search cycle, if cycle there is exists a deadlock, algorithm is O(n^2 x m)
                - What do you do when a deadlock is detected? Recent, long running, short?
            - Ignore and Pretend - Life
    - Networking
        - Review computer_networking_a_top_down_approach

7. 10/17/2024
    - Networking
        - Packet Switching - large messages are broken down into smaller segments called packages
            - Why - individual packets can traverse the internet in different paths
            - Why - users are bursty, which makes dedicated circuits less attractive
            - Why - individual packets can adapt to network conditions
            - Why - data arriving without error, reduce size of what is sent to reduce possibility of error
            - Issue - congestion possible
            - Issue - packet delay
            - Issue - packet loss, indirect knowledge requiring resend
        - Circuit Switching - dedicated end to end resources
            - Why - large telecom industries if constantly sending data
        - Network of Networks
            - Tier 1 Internet Service Providers, Regional Internet Service Providers, Content Provider Network, Access Internet Service Provider
        - Metrics
            - Packet Loss
            - Throughput - average rate from end to end, smallest communication channel limits throughput
        - Internet Protocol Stack
            - Application - HTTP, SMTP, FTP
            - Transport - TCP, UDP - process to process data transfer
            - Network - routing datagram
            - Link - data transfer between neighboring network elements, Ethernet, WiFi
            - Physical - bits on the wire
        - Open System Interconnection Model
            - Physical
            - Data
            - Network
            - Transport
            - Sessions
            - Presentation
            - Application
        - Encapsulation
            - Structure
                - Application - prepare message
                - Transport - break message into segments w/ TCP or UDP Header
                - Network - add IP header
                - Link - transfer to next device
                - Physical - bits on the wire
            - Idea
                - Choosing size of M is important, should be larger than headers to make trip worth it
        - Attacks
            - Distributed Denial of Service - overwhelm resource with bogus traffic
            - Packet Sniffing - capture and analyze packet traffic
            - IP Spoofing - send packet with false source address
    - Applications
        - Client Server Architecture
            - Structure
                - Server - always on host, permanent IP address
                - Client - communicate with server, dynamic IP address, does not communicate directly to other clients
            - Benefits
                - Security of server
            - Issues
                - Scaling requires adding servers - linearly
        - Peer to Peer
            - Structure
                - End User - Client and Server
            - Benefits
                - Scales as new peers are added - logarithmic
            - Issues
                - Security is reduced in this situation, a malignant peer can alter and send bad data directly to others
        - Process Communication
            - Processes
                - Client - initiates communication
                - Server - waits to be contacted
            - Sockets
            - Addressing
            - Protocols
                - Message Type e.g. request, response
                - Message Syntax - fields in message
                - Semantics - meaning of information in fields
                - Rules - when and how processes send and respond
        - Transport Service
            - Data Integrity, Timing, Throughput, Security
        - Internet Transport Protocol Services
            - TCP
                - Reliable Transport - was something sent or not
                - Flow Control - sender wont overwhelm receiver
                - Congestion Control - throttle send when network overloaded
                - Connection Oriented - setup required between client and server processes
                - NO: timing, minimum throughput guarantee, security
            - UDP
                - Unreliable Data Transfer
                - NO: reliability, flow control, congestion control, timing, throughput, guarantee, security
        - Securing TCP
            - Secure Socket Layer - provides encrypted TCP connection
        - HTTP
            - Persistent - maintain a connection
            - Non-Persistent - establish connection, complete request and response, close connection
        - Web Caches - satisfy clients without involving origin server
            - How do we store information?
            - How do we track information?
            - How do we know the freshness of data?
            - Conditional Get - if modified since - 304 Not Modified or Modified, faster than downloading entire asset
    - Distributed Name Service DNS
        - What - hostname to IP address translation
        - What - distributed to avoid single point of failure, handle request volume, and perform maintenance
        - Structure
            - Root DNS Servers - 13 root level
            - Top Level Domain Servers - com, org, edu
            - Authoritative Domain Server - organizations own DNS server
        - Issue - when creating initial link, entries must be entered manually to server
        - Iterated - local dns repeatedly queries domains upward until it finds an answer
        - Recursive - dns servers query their higher tier and return responses, a lot of pressure on root server
        - Records
            - A - hostname to IP address
            - NS - hostname of authoritative name server for domain
            - CNAME - canonical e.g. closer versions ford.com to ford.jp
            - MX - mailserver to name
        
8. 10/31/2024
    - Peer to Peer
        - File Distribution - files divided into chunks, peers in torrent send/receive file chunks
        - Tracker - tracks peers participating in torrent, becomes bottleneck
        - Torrent - Group of peers exchanging chunks of a file
        - Distributed Hash Table - removes tracker, peers only aware of immediate successor and predecessor
    - Socket Programming
        - Socket - IP and port, what host and what process
        - Reliable - data arrives correctly or I know it did not get delivered
        - User Datagram Protocol - unreliable datagram
            - Connectionless - no connection between client and server, ip destination and port attached to each packet
            - Unreliable - data may be lost or out of order
        - Transmission Control Protocol - reliable byte stream
            - Connection - establish readiness and connection
            - Reliable - in order stream delivery
            - TCP Connection - Source IP and Port, Destination IP and Port
    - Transport Layer
        - What - logical communication between processes
        - Multiplexing/Demultiplexing
            - M - handle data from multiple sockets, add headers
            - D - review header and deliver segments to correct socket
    - User Datagram Protocol
        - Characteristics - unreliable, connectionless, best effort service, no flow or congestion
        - Uses - streaming multimedia, DNS, SNMP, cost of error is not significant
    - Pipelined Protocols
        - Sender allows multiple in-flight yet to be acknowledge packets, contrast with one packet and accept single response
        - Go-Back-N - cost of transmission is low, send n unacked packets, receiver only sends cumulative ack, timer for oldest unacked packet, dont buffer packets resends are cheap
        - Selective-Repeat - cost of retransmission is high, send n unacked packets, receiver sends individual ack for each packet, timer for each unacked packet, buffer packets to avoid resends
    - Transmission Control Protocol Overview
        - Characteristics
            - Point to Point - one sender one receiver
            - Reliable, In-Order Byte Stream
            - Pipelined
            - Full Duplex Data
            - Connection-Oriented
            - Flow Controlled - sender will not overwhelm receiver
        - Segment Structure
            - Source Port #
            - Dest Port #
            - Sequence Number - byte stream number
            - Acknowledgement Number
        - Three Way Handshake
            - Client Send Connect Request - SYN,
            - Server Sends ACK, Increment SEQ - SYN, ACK
            - Client Sends ACK, Incremenet SEQ - SYN, ACK
        - Round Trip Time, Timeout
            - How do you set the TCP timeout value?
            - How do you estimate the Round Trip Time?
        - TCP Reliable Data Transfer
        - TCP Flow Control - receiver controls sender so sender won't overflow receiver's buffer by transmitting too much, too fast
            - Receiver advertises free buffer space with rwnd value
            - Sender limits amount of unacked data to receiver's rwnd value
        - Congestion Control
            - Cost - delay increase exponentially, when packet dropped upstream capacity is wasted
            - End to End Congestion Control
                - Additive Increase - each successful packet sent, add a single additional packet
                - Multiplicative Decrease - cut packets sent in half after loss
                - Congestion Window - dynamic function of perceived network congestion
                - TCP Slow Start - 1 MSS, double every RTT
                - Loss due to slowdown i.e. 3 duplicate ACKs TCP Reno, reduce congestion window
                - TCP Tahoe - Slow Start, Then Linear Increase, Drop to Start if Congestion
                - TCP Reno - Slow Start, Linear Increase, Drop to Half if Congestion
            - Network Assisted Congestion Control
    - Main Memory
        - Base and Limit Registers - define the logical address space for a process, cpu must check every memory access generated in user mode is between base and limit
            - Base - Start, Limit - How Much
            - CPU - is address >= base and is address < base + limit, issue adds overhead
        - Address Binding - how to tell what address is available to Process
        - Binding of Instructions and Data to Memory
            - Absolute Address - memory location known, issue is space cant be used when not in use
            - Relocatable Code - ability to move address space after context switch
            - Logical vs. Physical
                - Logical - generated by the CPU i.e. virtual address, set of all logical addresses generated by a progam
                - Physical - address seen by the memory unit i.e. set of all physical addresses generated by a program
            - Memory Management Unit - maps virtual to physical address
                - Relocation Register - add to logical address to produce physical address in MMU
                - Dynamic Linking - linking occurs at exeuction time, load only a single time
                - Swapping - move process memory out of base and limit, copy to hard drive in backing store, load another process
                    - Swapping normally disabled, started if more than threshold amount of memory allocated
                    - Issue - swap must go from user space to kernel space, another delay
                    - Double Buffering - user space to kernel space, kernel space to user space to avoid corruption
                    - Mobile Systems - doesn't swap, ask apps to relinquish or force them
            - Variable Partition - Size to Process's Needs
            - Dynamic Storage Allocation problem
                - First-Fit - allocate first hole that is big enough, fast, may waste
                - Best-Fit - allocate the smallest hole that is big enough, slow, less waste
                - Worst Fit - allocate the largest hole
            - Fragmentation
                - External - total memory space exists to satisfy a request but it is not contiguous
                - Internal - allocated memory may be slighlty larger than requested, internal to partition, not used
                - Compaction - collect all available free memory and form one large chunk (external fragmentation)

9. 11/07/2024
    - Main Memory
        - Shared Pages
            - Shared Code - can have a single page in physical memory, add multiple references to this frame in the page table
            - Private Data - each process keeps copy and data is spread out in pages
        - Two Level Page Table Scheme
            - Outer Page Table -> Page Table -> Memory
            - Benefit - two smaller searches can be faster than one
            - Issue - now three requests and extra memory to manage
        - 64 Bit Logical Address Space - 2^12
            - Two Level Scheme is Not Sufficient - Three Level Paging Scheme
            - Hashed Page Tables - search through a set of hashes
            - Inverted Page Table - search through available frames
    - Virtual Memory
        - Important - the entire program code is not needed at the same time, can improve CPU utilization and reduce I/O
        - What - separation of user logical memory from physical memory, logical address space can be much larger than physical address space
        - Memory Management Unit - dedicated hardware to translate virtual memory to physical memory
        - Goal - minimize cache misses
        - Page Fault - page is needed but it is not in memory
            - Find Free Frame
            - Trap
            - Page in on Backing Store
            - Bring in Missing Page
            - Reset Page Table
            - Restart Instruction
        - Aspects of Demand Paging
            - Pure Demand Paging - do not load page into physical memory until you actually need it
            - Locality of Reference - load page and next N pages
            - Actions
                - Interrupt
                - Read Page (bulk of time)
                - Restart Process
            - Effective Access Time - REVIEW
            - Optimizations
                - Pool of Zeroed Out Memory Pages/Frames
                - Full on Memory - Unload, Load, Reset
        - Page Replacement
            - Track - modified/dirty bit, only modiifed pages are written to disk
            - Steps
                - Find Location of Desired Page
                - Find Free Frame (Use Free, Replace Victim Frame, Write Victim Frame to Disk if Dirty)
                - Bring in Desired Page
                - Restart Instruction
            - Frame Allocation Algorithm - how many frames for each process and which frames to replace
            - Paeg Replacement Algorithm - goal is lowest page fault rate on both first access and reaccess
            - Note - as frames increase, page faults drop until they plateau
            - Algorithms
                - First In First Out, can allocate more frames and get more page faults Belady's Anomaly*
                - Optimal Algorithm - can do if we have perfect knowledge, remove page that will not be used for longest period of time
                - Least Recently Used - replace page that has not been used in the most amount of time
    - Segmentation - memory management schema that supports user view of memory
        - Unit - main program, procedure, function, method, object, local variables, common block stack, symbol table arrays
        - Goal - keep memory address of units together to optimize
        - Table - maps two dimensonal physical addresses, each with base i.e. starting address, limit i.e length of segment
        - Segment Table Base Register - segment tables location in memory
        - Segment Table Length Register - indicates number of segments used by a program
    - Paging
        - Divide physical memory into fixed size blocks called frames
        - Divide logical memory into blocks of same size called pages
        - Keep track of all free frames
        - To run a program of size N pages, need to find N free frames and load program
        - Setup a page table to translate logical to physical addresses
        - Size is power of 2, between 512 bytes and 16 Mbytes
    - Address Translation Schema
        - Page Number P - index into page table which contains base address of each page in physical memory
        - Page Offset D - combined with base address to define the physical memory address that is sent to the memory unit
        - Flow - Logical Memory Page Number -> Index to Frame Number in Page Table -> Frame Number is Location in Physical Memory
        - Issue - Translation, must be optimized for speed
        - Benefit - can dynamically move pages between frames while updating table, fixed size you get no external fragmentation
        - Tradeoff - Smaller Framesize = Larger Table - How to determine?
    - Translation Look Aside Buffer - Cache for Page to Frame
        - Translate Page Number to Frame Number
        - Effective Access Time - Review Formula
        - Goal - increase to high as possible likelihood to find translation in TLB
        - Memory Protection
            - Valid-Invalid Bit - VALID - associated page is in the process logical address space
            - Valid-Invalid Bit - INVALID - page is not in the process logical address space
            - Goal - reduce how much needs to be changes to accurately and quickly track known relationships

10. 11/14/2024
    - Virtual Memory
        - Goal - take chunk of hard drive and make it look like memory, separate user logical memory from physical memory
        - Action - reading from memory requires reading more than intended due to constraints of physica hard disk drive
        - Pattern - Virtual Memory (Page) -> Memory Map -> Physical Memory Address -> Data
        - Virtual Address Space
    - Shard Library Using Virtual Memory
        - Keep shared libraries referenced in memory as they may be used by multiple prcoesses
    - Demand Paging
        - Goal - bring page into memory only when needed
        - Lazy Swapper - never swap a page into memory unless call
    - Locality
        - It is likely you will yse near memory addresses when accessing one - Spatial
        - It is likely you will use the same data multiple times - Temporal
    - Page and Frame Replacement Algorhtms
        - Frame - How many frames to give each process and which to replace
        - Page - how can we reduce page fault rate
        - Insight - as frames increase, number of page defaults decrease, up to a point
    - Optimal Algorithm - replace the page that will not be used for the longest time... impossible
    - Least Recently Used Approximation Algorithm
        - Use past knowledge, replace the page that has not been used in the most amount of time
        - How do you measure which page has been accessed?
            - Counter - every time page is referenced, copy clock into counter, look at smallest counter, overhead is challenging
            - Stack - double link form, page referenced, move to top of stack, bottom of stack is least recently used
            - Reference Bit - initial 0, when referenced, set to 1, replace page with 0, but doesnt know oldest...
            - Second Change Algorithm
                - Whenever page is not available, move through where you are in circular queue and find next page
                - If page to be replaced has RB = 0, replace
                - If page to be replaced has RB = 1, set to RB = 0 and keep in memory
                - Insight into system use of pages
                - Worst Case - go through entire list one O(n)
            - Enhanced Second Change Algorithm
                - Dirty Bit - 1 when a page is modified (written to)
                - If dirty, page has change and must be written back to disk
                - If clean, page can be removed without writing to disk, saving time
                - Keep 1,1, Eject 0,0
    - Page Buffering Algorithms
        - Goal - keep a pool of free frames
        - Reality - OS guesses about future page access
            - This does not work for systems that are optimizing their use of memory e.g. relational database management systems
            - In some cases, application can be promoted acces to the disk to take advantage of optimizations
            - Double Buffering - 
    - Allocation of Frames
        - Minimum or Fixed Allocation - always allocate X to any process
        - Equal Allocation - split available frames into equal amount
        - Priority Allocation - allocate according to the priority
        - Proportional Allocation - allcacte according to the size of process
    - Global vs. Local
        - Global Replacement - replace a frame from all sets of frames, optimizes overall throughput
        - Local Replacement - each process replaces a frame from its own set of allcoated frames
    - Thrashing - degree of multiprogramming leads to higher cpu utilization, up to a point where you are spending too much time context switching
    - Working Set Model
        - Each process has a number of pages
        - If number is too small, can incur page faults
        - If number is too large, can load pages not used by process
        - If number is infinite, entire program
        - Requires per process tracking of pages in memory and page faults (overhead)
    - Page Fault Frequency - Approximation to Working Set
        - Establish Page Fault Frequency Boundary Upper and Lower
        - Keep track of page faults per process
        - If rate is low, reduce frames allocated to process
        - If rate is high, allocate additional frames to process
        - If rate is in boundary, dont do anything
    - Memory Mapped Files
        - Goal - file access speed is treated as RAM
        - Action - load DLL in shared memory and keep it there, issue loss of power and loss of data
    - Allocating Kernel Memory
        - Buddy System - fixed power of 2 allocator
        - Large Chunk -> Divide by 2 (REPEAT)
        - Efficient allocation and deallocation

11. 11/21/2024
    - Hard Disk Drive
        - Insights
            - more data inside than outside
        - Address Mapping
            - Constant Linear Velocity - keep same density read, must speed arm as arm extends outward
            - Constant Angular Velocity - keep motor speed constant, write fewer bits internally, waste capacity, easy
        - Physical Realities
            - Energy to move actuator
            - Dealing with inertia - read a sector at a time
        -  Disk Scheduling
            - Read/Write Buffers
            - First Come First Served - processed in the order that they arrive, queue of sector destinations, reverse direction has a cost i.e. moving up and moving down, largest delay
            - Shortest Seek Time First - request closes to the current head position is served next, can cause starvation, optimal
            - Elevator Algorithm SCAN - disk head moves in one direction large to small, serviving all requests in its path, then reverses small to large, some requests may wait longer
            - C-SCAN - disk head moves one direction, serving all requests in its path, then returns to start without reading and starts again
            - LOOK - 
    - Solid State Drives
        - Read from multiple blocks at a time
        - Gate - NAND
        - NVM Device Controller
            - Valid Page -> Invalid Page -> Valid Page
            - Read - no cost
            - Write - lifespan reduction
            - Wear Leveling - avoid putting things in the same place and instead spread data writes across new pages

12. 12/05/2024
    - Final Topics to Review
        - Disk Scheduling
        - RAID
    - Solid State Drivers
        - NAND - Flash Memory Based Drives, easier to manufacture and its cheaper, NOR is faster
        - Speed - Parallelization of R/W
        - Challenegs
            - Write - Pages 4KB
            - Read - Blocks 128-256KB
            - Issue - Book Keeping and Write Amplification
                - Mark -> Copy -> Erase
            - Write Amplification - write triggers garbage collection/compaction
                - One or more blocks must be read, erased, and rewritten before the write can proceed
            - Garbage Collection - recover space when needed
            - New SATA Command Trim - allows the OS to tell the SSD that specific blocks are invalid and may be garbage collected
            - Wear Leveling - writing reduces the flash cell longevity, try to even out writes to cells
        - SSD Controllers
        - Flavors of NAND Flash Memory
            - Multi-Level Cell - higher capacity and cheaper, need error correction, 3000-5000 write cycles, more power
            - Single Level Cell - one bit per flash cell, lower capacity and more expensive, 10000 - 100000
    - File System Implementation
        - Question: how do we impose a structure on something that stores bits?
        - Question: why do most data storage systems prefer Linux?
            - Linux is open source, could be updated without powering down, file system
        - Question: what is the logical way of organizing a large chunk of data?
            - Main Folder -> Offices -> Groups -> Projects -> People
            - Issue - max depth and max name size...
        - Flow: Application -> Logical File System -> File Organization Module -> Basic File System -> I/O Control -> Devices
        - Format File Control Block (Metadata)
            - File Permission 
            - File Dates
            - File Owner, Group, Access Control List
            - File Size
            - File Data Blocks
        - In-Memory File System Structures - directory structure file control block + user space + kernel memory + secondary storage
        - Virtual File System - object oriented way of implementing file systems
        - Question: Why do we like indirection? Gives flexibility as to how to solve the problem - single system call and use polymorphism to achieve ends
        - Directory Implementation
            - Linear list of file names with pointer to the data blocks
            - Question: What happens when data becomes too large? time consuming to perform actions
            - Hash Table - issue could be collision and it is fixed size
        - Allocation Methods
            - Contiguous - each file occupies a set of contiguous blocks
                - Simple, random access, wasteful of space, files cannot grow
                - Question: What if you do not have contiguous blocks?
            - Extant - add a layer of indirection on contiguous i.e. multiple contiguous groups make up the file
            - Linked - store starting location and address of the next LL, simple, no wasted space, need to step through sequence to get to end, NO RANDOM access
            - Indexed (Modern) - allows random access, maximum use of space, and initial formatting of disk
                - Index Table - for each file, what are the associated blocks?
                    - RAM - find block and offset for specific data
                    - Downside - must manage larger list and metadata
                - No external fragmentation as the blocks are fixed size
                - Two Indexes - outer index and block of index table to map to specific file
                - Question: What is file is small? Can store file and metadata directly in the control block
                - Indirects
                    - Direct Block - actual file or first level index i.e. on pointer
                    - Double Indirect - two levels of pointers
                    - Triple Indirect - three levels of pointers
                - Question: how do we mark what is available?
                    - Bit Vector, one bit for each block that says in use or not in use, can add single bits for general idea of used and free
                - Question: What is performance problem of indexed when writing large amounts of data?
                    - Issue - disk is going to block, use direct memory access or caching
                    - Issue - did not write cache to hard drive
                    - Log Structured File System (Journaled)
                        - Record each update to the file system as a transaction
                        - All transactions are writen to a log
                        - Commited is when a transaction is writen to the log, however, file system update may not yet be done
                        - When the file system is written to, the transaction is removed from the log
                        - If the file system crashes, all remaining transaction in the log must still be performed
                        - Priority - write the log/journal first to persistent memory ASAP
        - Sun Network File System (NFS) - access a file as if it is part of the local file system, requires RPC and TCP/IP
            - OS Layer - Unix File System Interface
            - Virtual File System - API
            - NFS Service Layer - NFS Protocol