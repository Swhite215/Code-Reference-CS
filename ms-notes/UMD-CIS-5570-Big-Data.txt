Big Data

1. 01/06/2025 - Introduction
    - Course Objectives
        - Big Data Concepts
        - Technologies
        - Processing and Techniques
    - What is Data Mining?
        - Big Data - no standard term
        - Generators - almost everyone and every device
        - Note - Data has changed over time
        - Goal - extract knowledge and model from data
        - Goal - discover patterns and models that are valid, useful, unexpected, and understandable
    - Three Vs
        - Volume - size (gigabytes, terabytes, petabytes)
        - Velocity - speed
        - Variety - types
    - Directions in Modeling
        - Statistical Modeling - describes the distribution of data e.g. Gaussian is mean and standard deviation
        - Predictive Methods - use some variabels to predict unknown or futrue values of other variables
        - Descriptive Methods - find human interpretable patterns that describe data
            - PageRank - represent each website with a score, if a website has many hyperlinks to it, the score increases
            - Clustering - create groups of clusters of data points, with each cluster revealing something about the group
        - Feature-Based Model - look for the most extreme example of phenomen and represent the data by these items
            - Frequent Itemsets - makes sense for data that consists of baskets of small sets of items e.g. market basket
            - Similar Items - find pair of sets that have a relatively large fraction of their elements in common
    - Machine Learning Classification
        - Topic - movies and tv shows
        - Data - Instance - Record - Entry - Data Point
        - Features - length, genre, seasons
        - Goal - map rows and columns into open space, where every column is an axis in your space, orthogonal to each other
        - Decision Boundary - separation between classifications based on features
        - Prediction - input feature values and map them to the open space and then based on decision boundary return classification
        - Training Set - produces decision boundary
        - Test Set - unknown data that will be classified
        - Overfit - performance on test set is poor, and performance on training set is extremely high
    - Analytic Answers
        - Total Information Awarenmess - a plan to mine all data, including credit card receipts, hotel records, travel data, and many other
        - Bonferroni's Principle - if you look in more places for interesting patterns than your amount of data will support, you are bound to find crap
        - Example
            - Goal - we want to find unrelated people who on two different days were both at the same hotel
            - 10^9 people being tracked i.e. 10,000,000,000
            - 1,000 days are being reviewed
            - Each person stays in a hotel 1% of the time
            - A hotel holds 100 people
            - There are 10^5 hotels
            - What is the probability that a person stays in a hotel on any day? 0.01
            - What is the probability that two people stay in a hotel on any day? 0.01 * 0.01 = 0.0001
            - What is the probability that these two people stay in the same hotel on any day?  0.0001 / 10^5 = 1x10^-9
            - What is the probability that these two people stay in the same hotel on two days? 10^-9 squared which is 10^-18
            - n chose m, n days how many m combinations? (n! / (n-m)!m!)
                - m is 2 that is two people
                - n! / (n-2)!2!
                - n(n-1)(n-2)(n-3).../(n-2)(n-3)....*2
                - n^2-n/2
                - n^2/2
                - (10^9)^2/2 = 5*10^17
                - (10^3)^2/2 - 5*10^5
                - Goal - number of possible combinations of two days, times number of possible combinations of two people, times probability
            - 250,000 suspicious pairs of people
    - Things Useful to Know
        - TF.IDF - the formal measure of how concentrated into relatively few documents are the occurences of a given word - if you see how frequent a word is e.g. the or a.
        - Inverse Document Frequency
            - Frequency TF - for word I in document j, frequency of i in document j divided by max kfkj i.e. most frequent word in the document 
            - IDF - log(total number of documents divided by how many documents have the word i in them)
            - For the article  "the", T.IDF should be high, logn(1) is 0, not important
        - Hash Functions - takes a hash key value as an argument and produces a bucket number, which is an integer. normally in the range 0 to B-1, where B is the number of buckets
            - What is the ideal value of B? if all even numbers, the bucket is the same as results are 0
            - Prime number, which cannot be factored, are ideal for number of buckets
        - Base E of Natural Logarithms (Review)
        - Power Laws (Review)

2. 01/13/2025 - Hadoop
    - Things Useful to Know
        - Power Laws - linear relationship between logarithms of two variables, if x is the horizontal axis and y is the vertical axis, then the relationship is log base 10 y = 6-2 log base 10x
            - X axis - rank of the book, Y axis - number of sales
            - How to find the formula?
                - 1a. Determine slope, delta y over delta x (y=ax+b) [LINEAR]
                - 1b. Determine slope, log y = alogx + b, log delta y over log delta x [LOGARITHMIC]
                    - log(1) - log(1,000,000) / log(1000) - log(1)
                    - 0 - 6 / 3 - 0 = -6/3 = -2
                - 1c. Determine y Intercept b
                    - logy = -2logx + b
                    - log(1) = -2log(1000) + b
                    - 0 = -2(3) + b
                    - 0 = -6 + b
                    - 6 = b
                - 1d. Form Equation
                    - logy = -2logx + 6
                - 1e. Formula is linear relationship between logarithms of two variables
            - How do we take advantage of e to get the actual relationship
                - e to the power of log y = e to the power of (-2logx + 6)
                - y = e to the power of -2logx X e to the power of c (CONSTANT)
                - y = e to the power of logx to the power of -2 X e to the power of c
                - y = cx^-2
                - 1,000,000 = c(1)^-2
                - 1,000,000 = c
                - y = 10^6x^-2
    - Data
        - Challenges - usage, quality, context, streaming, scalability
        - Data Modalities - ontologies, structured, networks, text, multimedia, signals
        - Operators - collect, prepare, represent, model, reason, visualize
    - Data Mining Cultures
        - Databases - large scale data, simplex queries
        - Machine Learning - small data, complex models
        - Theory - randomized algorithms
        - Culture
            - DB - analytic processing i.e. queries that examine large amounts of data
            - ML - inference of models
    - Learning Objectives
        - Working with high dimensional data
            - Locality sensitive hashing, clutering, dimensionality reduction
        - Working with infinite/never ending data
            - Filtering data streams, web advertising, queries on streams
        - Working with labeled data
        - Working with MapReduce
        - Working with Data Streams
        - Working with Machine Learning
            - Support Vector Machine, decisions trees, perceptron, kNN
    - Applications and Tools
        - Recommender Systems
        - Market Basket Analysis
        - Duplicate Document Detection
        - Linear Algebra (Recommender System)
        - Dynamic Programming (Frequent ItemSets)
        - Hashing (LSH, Bloom Filters)
    - MapReduce and the New Software Stack
        - What is Hadoop? open source data storage and processing platform
        - Setting Up Hadoop
            - Hadoop Binaries - local, Cloudera's DEMO VM, Cloud
            - Data Storage - local filesystem or hadoop distributed file system, cloud
            - Map Reduce - local, cloud
            - Other Libraries and Tools - vendor tools, libraries
        - Philosophy
            - Objective - complete work on a large set of data
            - Actions
                - Divide work i.e. data across machines
                - Perform action on data
                - Combine results to produce final value
        - Architectures
            - Single Node Architecture - CPU, memory, and disk are on the same device, data loaded from disk to memory, algorithm is run against all data, results are written to storage
                - Issue(s) - What if you cannot store all data on disk? What if you cannot store all data in memory?
            - Cluster Architecture
                - Rack - 16-64 nodes (CPU, memory, disk), 1Gbps between nodes
                - Switch - connects racks and other switches, 2-10 Gbps between racks
        - Motivation
            - Data - 20+ billion web pages, each web page is 20kb = 400TB of data
            - Read - if reading data 30-35MB/sec from disk it would take ~4 months to read the web
            - Today's Paradigm - cluster of nodes with ethernet connecting them
        - Large Scale Computing
            - Challenges
                - How do you distribute computation?
                - Which machine should do what work?
                - How do we write the code for this problem?
                - How do we deal with machine failures?
            - Idea and Solution
                - #1 - Redundancy for Reliability
                    - Issue - copying data over a network takes time
                    - Idea - bring computation close to the data, store files multiple times for reliability and redundancy, algorithm runs on this machine
                    - Solutions - MapReduce, Google File System, Hadoop Distributed File System                
                - #2 - Storage Infrastructure
                    - Issue - if nodes fail, how to store data persistently?
                    - Idea - Distributed File System i.e. global file name space
                    - Usage Pattern - huge files, data rarely updated in place, reads and appends are common
                    - Note - Hadoop isn't necessarily great for real time
                - #3 - Distributed File System
                    - Idea - Chunk Servers - file is split into contiguous chunks 16-64mb, each chunk is replicated, try to keep replicas in different racks, each chunk server is a compute server
                    - Idea - Master Node - stores metadata about where files are stored, might be replicated
                    - Idea - Client Library - talks to master to find chunk servers, connects directly to chunk servers to access data
                    - Why store on different racks? if a switch fails, access to no data stored on a particular rack
        - Programming Model: MapReduce
            - Example Task
                - Objective - count the number of times each distinct word appears in the file
                - Data - huge amount of data, assume all web pages
                - Case 1 - file is too large for memory, but all word,count pairs fit in memory
                - Case 2 - storing words and counts would be large, task is naturally parallelizable
            - Overview: MapReduce
                - Essential Actions
                    - READ DATA
                    - Map - extract something you care about (keys)
                    - GROUP BY KEY - sort and shuffle
                    - Reduce - aggregate, summarize, filter, or transform
                    - WRITE RESULT
                - Map - Function (YOU)
                    - Input Key Value Pairs - Document, key = file name, value = lines in file
                    - Output Intermediate Key Value Pairs - key = word, value = 1
                - Reduce - Function (YOU)
                    - Sort to Key Value Groups - the, the, the, game, game, game, name, name
                    - Reduce to Output Key Value Pairs - aggregate the values for each key i.e. the - 3, game - 3, name - 2
                - Other - System (HADOOP)
            - Overview #2: Map Reduce
                - Input - Set of Key Value Pairs
                - Map(k,v) - takes input key value pair and outputs a set of intermediate key-value pairs (k',v') (Key = Document, Value = Words)
                - Reduce - takes intermediate key-value pair (k', <v'>) and ouputs a a set of key value pairs (k', v'') (Key = Word, Value = 1)
                - Reduce(k, v) - takes key value pair and outputs a set of output key value pairs (key = word, value = aggregate count)
            - Example Word Counting: MapReduce
                - #1 - Obtain Big Document and split into chunks
                - #2 - Map takes file and outputs word and value of 1
                - #3 - Reduce - Group by Key, collect all pairs with the same key
                - #4 - Reduce - take grouped keys and aggregate them together
            - Note on Global Count
                - Issue - how do I aggregate values together to determine global count?
                - Solution - hash function will take intermediate key value pairs and send them to reduce nodes, and in the reduce nodes grouping is done and reduce is applied
                - Each word is the input to the hash function, producing a unique number
                - X MOD B, x is intermediate key, B is number of buckets or reduce nodes
                - X MOD 3, output is either 0, 1, or 2, intermediate key goes to a particular node
                - Whichever map node the X occurs on, due to the hash, it will be sorted to the same reduce node, all unique words must go to same bucket which is same reduce node
            - Pseudo Code
                - map(key, value) - key = document, value = text
                    - for each word in value, emit(word, 1)
                - reduce(key, values) - key = word, values = iterator
                    - result = 0, for each count in values: result += v, emit(key, result)
            - MapReduce Environment Features
                - Partitioning the input data
                - Scheduling the program's execution across a set of machines
                - Performing the group by key step
                - Handling machine failures
                - Managing required inter-machine communication
            - MapReduce a Diagram - Slide 25
            - MapReduce Node Diagarm - Slide 26
                - Map Nodes - 3, Reduce Nodes - 2
        - Data Flow - Map Reduce
            - Input and Output are stored on Distributed File System i.e. Hadoop Distributed File System
            - Intermediate results are stored on local File System of Map and Reduce workers
                - Why? It would involve duplicating and take processing power and network bandwidth
            - Final Output is often input to another MapReduce task
        - Coordination Master
            - Task Status (idle, in progress, completed)
            - Idle Tasks - get scheduled as workers become availbale
            - When a map task completes, it sends the master location and size, Master pushes this info to reducers
            - Ping workers periodically to detect failures
        - Failures
            - Map Worker Failure - all map tasks are reset
            - Reduce Worker Failure - only in progress tasks are reset to idle
            - Master Failure - MapReduce task is aborted and client is notified
        - How many Map and Reduce Jobs?
            - Make M much larger than the number of nodes in the cluster
            - One Distributed File System chunk per map is common
            - Usually R is smaller than M, because output is spread across R files, if R is large, intermediate map files explode and cause high network traffic
        - Task Granularity and Pipelining
            - Master - Assign Tasks
            - Worker 1 - Map 1 and Map 3
            - Worker 2 - Map 2
            - Worker 3 - Reduce 1
            - Worker 4 - Reduce 2
        - Refinement: Combiners
            - Opportunity - perform aggregation earlier
            - Action - on the same Map, combine values of all keys of a single mapper to reduce data needing to be copied and shuffled
            - Note - combiners can only be used if Reduce is commutative and associative
                - Commutative - A + B = B + C (Addition, Subtraction)
                - Associative - (A + B) + C = A + (B + C) (Addition, Subtraction)
                - Average - Sum = 1 + 2 + 3 / 3 and Sum = 4 + 5 / 2 can be S1 + S2 / C1 and C2
                - Median - no...
        - Refinement: Partition Function
            - Sometimes it is useful to override the hash function
            - Example; Host Size
                - Host = url, size, date
                - Goal - for each host, find the total number of bytes of all URLs
                - Override the hash function use the hostname of the url instead of the entire URL, to ensure all urls with same host name are sent to each machine
        - Assignment: Language Model
            - Statistical Machine Translation - need to count number of times every 5 word csequence occurs in a large corpus of documents
                - Map
                    - Key1 - the dog and the cat, value = 1
                    - Key2 - dog and the cat played, value = 1
                - Reduce
                    - Combine the counts

3. 01/28/2025   
    - Three Example to Use Map Reduce
        - Matrix Vector Multiplication
            - Review
                - Matrix M - i rows and j columns
                - Vector V - j rows
                - Output Vector X - i rows, one column
                - X1 = M11 * V1 +  M12 * V2 + M13 * V3 ....
                    - Xi = Summation Mij * Vj
                - X2 = M21 * V1 + M22 * V2 + M23 * V3 ....
            - Map Reduce
                - Inputs
                    - Input Matrix: i, j and mij
                    - Value = mij * vj
                    - Intermedaite Key Value Pair(i, Mij*Vj)
                - Output Intermediate - keys are something you care about, and hash function will ensure each intermediate key goes to same node
                    - Output: i = N, i is common
                    - Why? Reduce Node will add all togetherd
        - Join by Map Reduce
            - Join - compute the natural join R(A,B)  S(B,C) or two relations
                - R and S are each stored in files
                - Tuple is two values (a,b) or (b,c)
            - Inputs
                - R - a,b and bit if R
                - S - b,c and bit if S
            - What shouild the intermediate key be? b so a and c are joined
                - From R intermediate output is (b, (a, R))
                - From S intermediate output is (b, (c, S))
            - Reduce Function - join the values together
                - Key List - (b, [(a1, R), (a2, R), (c1, S), (c2, S)])
            - What scenario will cause failure?
                - If B is the same value, this will cause all values sent to reduce node to be the entire dataset, likely causing a memory issue
        - Matrix Matrix Multiplication
            - Notes
                - Matrix M (i rows, j columns)
                - Matrix N (j rows, k columns)
                - Output Matrix P (i rows, k columns)
            - P1 - M11 * N11 + M12 * N21 + M13 * N31
                - Fixed (i and k), j changes
            - Pik = j summation of Mij * Njk
            - MapReduce One Pass
                - Input Key Value Pairs
                    - M - i, j and Mij and M bit
                    - N - j, k, and Njk and N bit
                - Intermediate Key
                    - i, k, the same I and k should go to the same reduced node
                - Intermediate Value 
                    - j, Mij, M Bit
                    - j, njk, N bit
                - Reduce Node - multiplication and addition
                    - Multiple any M and Ns that have the same j
                    - Add any M and N that have the same i,k
                - Map Function
                    - key = (i, k) and value = M, j, mij 
                    - key = (i,k) and value = N, j, njk
            - MapReduce Two Pass i.e. MapReduce -> MapReduce
                - First MapReduce - Multiplication
                    - Intermediate Key is j
                        - M - key = j and value is M, i, mij
                        - N - key = j and value is N, k, nkj
                    - Reduce - multiple same mij and nkj that share j
                        - Output Key - (i,k), value = mij * njk
                - Second MapReduce - Addition
                    - Map - Identity Function - Pass to Reduce
                    - Reduce - add any M and N that have the same (i,k)
                        - Output (i,k) and sum
        - Reducer Size
            - Issue - B is all the same, all data goes to same reducer
                - Solution
                    - Reducer Size, maximum number of inputs a reducer can have denoted by = q i.e. the length of the value list
                        - If q is small, than you force a lot of parallel processing
                    - Replication Rate - average number of key value pairs generated by the mappers - denoted by r
                    - If I have I input, p reducers which each receives q (maximum input), each mapper producers r
                        - What is R? divide output by input
                        - Ouput = p * q
                        - r = Output divided by input i.e. pq/i
        - Spark
            - Transformations - map, filter, join, union, intersection, distinct
                - Build RDD from another RDD
                - Do not occur till an Action has been called
            - Actions - count, collect, reduce, save
                - Trigger transformation and then...
                - Applied to an RDD to produce a value
            - General DAGs
                - RDD -> Transformation -> RDD
                - Supports general task graphs, pipeline functions where possible, cache aware data reuse and locality
        - Hadoop vs. Spark
            - Spark - speed, flexbility, ease of use, ease to program, real time
            - Why Hadoop?
                - Hadoop is better when it comes to very very large datasets
                - Spark requires a lot of memory
        - To Do List
            - Chapter 2 - Mining Massive Dataserts
            - Work on Word Count Program
            - Project
        - General Overview of Running Hadoop on Cloudera
            - Copy Code and Instructions to VM
            - Open New Project in Eclipse and Add Files
                - Add External Jars
                    - usr/lib/hadoop/ALL JARs
                    - usr/lib/hadoop/lib/ALL JARs
                    - usr/lib/hadoop/client0.2/ALL JARs
                - Create New Class named WordCount - (default package)
                - Paste Code into WordCount.java
            - Add Input File
                - Clouder's Home -> Workspace -> Project
                    - Create folder input
                    - Create file input/TestWC.txt
                    - Type whatever...
            - Run Program (Green Button) as Java Application
            - Conditional: Select WordCount if prompted
            - Refresh to View the Output Folder
            - Run Again - DELETE OUTPUT FOLDER BEFORE RUNNING AGAIN
            - Create Jar File
                - Folder -> Export Java -> Jar -> myproject.jar
            - Run on VM Cluster
                - CREATE INPUT FOLDER IN CLUSTER: hadoop fs -mkdir -p user/cloudera/input
                - PUT INPUT FILE IN INPUT FOLDER: hadoop fs -put /home/cloudera/Desktop/TestWC.txt /user/cloudera/input
                - RUN PROGRAM: hadoop jar /home/cloudera/workspace/myproject.jar WordCount /user/cloudera/input /user/cloudera/output
                - OPEN VISUALIZER: localhost:8088
                - GET AND STORE OUTPUT: hadoop fs -get /user/cloudera/output
                - SUBMIT OUTPUT FOLDER:
                - SUBMIT EXECUTION RESULTS FROM VISUALIZER: localhost:8088

4. 02/03/2025
    - Frequent Itemsets
        - Goal - find items that appear frequently together e.g. supermarket or restaurant orders or amazon shopping carts
        - Approach - process the sales data collected with barcode scanners to find dependencies among items
        - Classic Rule
            - If someone buys diaper and milk, then he/she is likley to buy beer
            - If someone buys beer, it is not true they will likely buy diapers
        - Market Based Model
            - Items - things that are sold, products
            - Basket - set of items or products
            - Discover - association rules i.e.people who bought item or basket tend to buy other item or basket
        - Examples
            - Baskets = sentences, Items = documents, detect documents that have many similar sentences in common
            - Baskets = documents, Items = words, detect words that go together in the same documents
            - Baskets = patients, Items = drugs and side effects, detect combination of drugs that result in certain side effects
                - Extension - absence of an item needs to be observed as well as presence
        - General Many to Many Mappin 
            - We are loking about connections among items NOT baskets
        - Outline
            - First Define
                - Determine Frequent Itemsets
                    - Goal - find set of items that appear together frequently in baskets
                    - Support for Itemset I - number of baskets containing all items in I
                        - Support for Milk and Bread - number of baskets with milk and bread - probability out of total number of baskets
                    - Question - how frequent is frequent?
                    - Support Threshold s - set of items that appear in at least s baskets are called frequent itemsets - given or derived
                    - Example (EXAM)
                        - Support Threshold s = 3 baskets
                        - Items - m = 5, c = 5, b = 6, {m,b} = 4, {c, b} = 3,, {c, j} = 3 
                - Create Association Rules
                    - If a basket contains all of, then it is likely to contain j
                    - Confidence - support of I union J divided by support of I
                        - Confidence of milk implies diapers is support of milk and diapers divided by support of milk
                        - Upper Bound - 1 i.e. 100%
                    - Note - not all high confidence rules are interesting, e.g. is a particular item frequent
                    - Interest - confidence of the rule minus the probability of the rule
                         - Interest = confidence of milk implies diapers minus probability of baskets that contain milk by itself
                         - If interest is low, little to no relation
                         - Threshold usually above 0.5 or lower than -0.5
                         - Negative means inverse relationship that is if you purchase something you are likely not to purchase another item
                    - Example
                        - Association Rule {m,b} -> c
                        - Confidence i.e. confidence of milk and bread implies coke
                            - Union of {m,b,c} divided by {m,b} i.e. .25 / .5 = 1/4 / 1/2 = .5
                            - Union of {m,b,c} divided by {m,b} i.e. 2 / 4 = 1/2 = .5
                        - Interest i.e. confidence of the rule minus the probability of the rule
                            -  .5 - 5/8 = -0.125 - NOT INTERESTING
                    - Example - Find all association rules with support >= s and confidence >= c and interest > x
                        - If high support and high confidence, then both will be frequent
            - Mining Association Rules
                - Determine Frequent Itemsets - consumes memory
                - Create Association Rules
                    - For every subset A of I, generate a rule A implies whatever remains
                    - Measure confidence and interest and determine which ones to keep
                    - Variant 1 - single pass to compute the rule confidence c(A,B -> C,D) == supoort(A,B,C,D) / support(A,B)
                    - Variant 2 - If (A,B,C -> D) is below confidence so is (A,B) -> (C,D)
                        - Why? support(A,B) is larger, producing smaller confidence
                - Example
                    - Support Threshold - 3
                    - Confidence Threshold - 0.75
                    - Determine Frequent Itemsets
                        - {m}, {c}, {b}, {j}
                        - {m,b}, {m,c}, {c,b}, {c,j}
                        - {m,c,b}
                    - Create Rules
                        - b implies m - support(b,m) / support(b) = 4 / 6
                        - m implies b - support(m,b) / support(m) = 4 / 5
                        - b implies c = support(b,c) / support(b) = 5 / 6
                    - Derive Confidence
                    - Derive Interest
                - Prune/Eliminate Rules
                    - Maximal Frequent Itemsets - no immediate (one more item) superset is frequent
                    - Closed Itemsets - no immediate superset has the same count > 0 - do not care about support threshold
                    - Closed FrequentItemSets - no immediate superset has the same count > 0 - should be equal or greater than support threshold
            - Finding Frequent Itemsets
                - Note - typically data is kept in flat files rather than in a database system
                    - Stored on disk, basket-by-basket, baskets are small but there are many baskets and many items
                        - Use k nested loops to generate all sets of size k
                            - Pair - 2 nested loops - for x = i to n; for y = j to m; return {i,j}
                            - Trios - 3 nested loops
                - Note - the true cost of mining disk resident data is usually the number of disk I/Os
                - Note - in practice, association rule algorityhms read the data in passes - all baskets read in turn
                - Note - we measure the cost as the number of passes
                - Note - main memory is the critical resources
                    - As we read baskets, increment, save to disk, loop - this is an issue
                - The hardest problem often turns out to be finding the frequent pair of items
                    - n choose 2 = n^2/2
                    - Approach - generate all the itemsets but only count those itemsets that in the end turn out to be frequent
                - Naive Algorithm - read file once, generate all pairs, count support for all pairs, determine confidence
                    - N^2/2 pairs are generated
                    - Each count is 4 bytes of memory
                    - Memory needed is N^2/2 * 4 = 2n^2 bytes
                    - If n is 100,000 then you need 10,000,000,000 * 2 = 20,000,000,000 i.e. 20GB of memory
                    - Memory must exceed n^2 items

5. 02/10/2025
    - Frequent Items Sets
        - Counting Pairs in Memory
            - Triangular Matrix
                - Notes - pairs diagonal is empty and order does not matter i.e. upperhalf triangle is all needed
                - Size = n^2 * Memory
                - Action
                    - Allocate Memory - 2n^2 bytes
                    - Loop through baskets and increment the count
                    - Each count consumes 4 bytes of memory
                - Benefit
                    - Items - n
                    - Size - n^2, only use half so n^2/2
            - Table of Triples
                - Action
                    - Loop through baskets, each time it finds a pair, it appends it to the table
                - Size = n^2 * Memory
                - Why would this be used? Triangular Matrix - allocate memory in advance whether the pair exists or not
                - Better if number of existing pairs to be less than 1/3 of the total number of possible pairs
            - Questions
                - Question - can we use both approaches?
                - Question - can we use triangular matrix? memory is already allocated for n^2
                - Question - can we use table of triples? yes
            - A Priori Algorithm
                - Montonicity
                    - if a set of items I appears at least s time, so does every subset J of I
                    - Contrapositive - if item I does not appear in s baskets, then no pair including i can appear in s baskets                                              
                - Note - two pass approach limits the need for main memory, this is an advantage in terms of memory
                - Note - lowers the number generated by only working with frequent singletons
                - Approach
                    - First Pass - read entire data set and count all singletons. using support threshold determine frequent singleton
                    - Second Pass - read entire data set and create pairs from frequent singletons, if a singleton is NOT frequent, neither will its pair be...
                - Pairs - N*^2 / 2 - should be LESS than those produced by naive i.e. it can handle larger data sets
                - Trick - re number frequent items sets 1, 2, and keep a table relating new numbers to original item numbers
                - Example 1:18:00 - if previous frequent pairs have already been excluded, triples with those pairs should also be as well
                - High Frequency
                    - Frequent Triples - Three Passes
                        - Pass 1 - Singletons -> Filter by Support Threshold -> Frequent Singletons
                        - Pass 2 - Candidate Pairs -> Filter by Support Threshold -> Frequent Pairs
                        - Pass 3 - Candidate Triples -> Filter by Suppot Threshold -> Frequent Triples
                    - Frequent Quad - Four Passes
                - What should support threshold be? 1%
            - Park Chen Yu PCY Algorithm
                - Action One
                    - During first pass, count items AND generate pairs from singletons, hashing them into hash table with odd number of buckets
                    - NOTE - DO NOT PUT THE PAIR IN THE BUCKET, just increment the count by one each time
                    - Result - all pairs are created and a hash function is applied, pairs are not kept
                - Action Two
                    - Filter singletons by support threshold to get frequent singletons
                - Action Three
                    - Bitmap - Vector with 0's and 1's
                    - Number of bits are number of buckets
                    - Bit is 1 if frequent i.e. over threshold
                    - Bit is 0 if no frequent i.e. under threshold
                - Action Four
                    - Take frequent singletons AND their pair must hash to a one in the bit table, if both apply, then they are a candidate pair
                    - Frequent Bucket or Bit does not tell you which is frequent
                    - However, if bucket count is lower than threshold, it is impossible any of them will be frequent
                    - Generated candidate pairs is smaller, allowing you to work with larger datasets easier, in-frequent buckets are ideal to know
                - Action Five
                - Question
                    - If only ij and mp map to bucket 0, then the count represents the total number of times I have seen them
                    - What is the compression rate?
                        - One number is 4 bytes i.e. 32 bits, but bit vector only needs a single bit so 32:1
                - Passes
                    - Pass 1 - maintain a hash table with as many buckets as needed
                        - Generate a pair, hash the pair to a bucket, add 1 to th count for th ebucket
                - Notes
                    - If a bucket contains a frequent pair, then the bucket is surely frequent
                    - For a bucket with a total count less than threshold, it is sure to not be frequent
                - Main Memory Details
                    - Storing Counts
                        - Triangular Matrix - no... best case is renumber indexes, all memory is allocated at start
                        - Table of Triples - 
                    - Question
                        - In what scenario is PCY better than a priori?
                            - PCY is using 12, A Priori is using 4
                            - If 2/3 of candidate pairs are eliminated than PCY is better

6. 02/17/2025
    - Quick Review
        - Naive Approach
            - Passes - 1
            - Memory - 2n^2
            - Storage
                - Triangular Matrix
                - Table of Triples
        - A Priori
            - Passes - 2
                - Pass 1 - Singletons
                - Pass 2 - Frequent Singletons -> Candidate Pairs -> Frequent Pairs
            - Storage
                - Triangular Matrix - Must do reindexing trick
                - Table of Triples  
        - PCY Algorithm
            - Passes - Review Above
            - Question
                - Does a frequent bucket mean a frequent
                - How do you determine buckets? Take remaining memory after first pass, divide by four bytes to get a reasonable bucket number
            - Memory
                - Must use Table of Triples - 12 bytes for each pair 3x larger than Triangular Matrix
                - To save memory, should eliminate 2/3 of candidate pairs
    - Multistage Algorithm
        - Three Passes
            - Pass 1 - Item Counts and Produce First Hash Table and Bitmap of Buckets
            - Pass 2 - Second Hash Table
                - Conditions - singleton is frequent and has one result should be 1
                - Action - hash the pair with a second hash function and create a second set of buckets with counts
                - Action - create second bitmap, 0 and 1 based on new buckets 
            - Pass 3 - Frequent Item, Bitmap 1 and Bitmap 2, Counts of Candidate Pairs
                - Candidate Pair
                    - Each singleton in pair should be frequent by itself
                    - Pair should hash to 1 in first bit map - we do not keep the information so hash must be checked again
                    - Pair should hash to 1 in second bit map
            - Memory - REVIEW
            - Note - can increase the number of hash tables
            - Note - hash functions must be different
            - Why over PCY?
                - Multistage Algorithms works by eliminating pairs, allowing you to tackle larger datasets.
            - Issue - in multistage, there is a point of diminishing returns, since the bit vectors eventually consume all of main memory
    - Multihash
        - Idea - use several independent hash functions and produce both hash tables and bitmaps in the first pass
        - Issue - multiple hash tables with lower number of buckets may result in more frequent buckets i.e. requiring more work later on, counts about double, infrequent buckets are the helpful ones because they help you eliminate
        - Note - if numbers in infrequent buckets are low, than when doubled likely to be okay as it is under threshold
        - Benefit - the bit vectors occupy exactly what on PCY bitmap does, but too many hash functions makes all counts >= support threshold
    - Algorithms w/ Less 2 Passes
        - Issue - some frequent itemsets may be missed - false negative - frequent in whole not in sample - cannot be eliminated
        - Issue - false positive - frequent in sample but not in whole
        - Simple Algorithm
            - Actions
                - Action - take a random sample of the market baskets
                - Action - run a-priori or another in main memory
                - Action - count candidate pairs to identify frequent pairs, if you count all pairs then it is the naive approache
            - Benefits
                - Dont pay for disk I/O each time we increase the size of itemsets
                - Pass - read or write from disk, this is no longer happening
                - Reduces support threshold proportionally to match the sample size
                - This pattern eliminates 
            - Questoin - how does this algorithm reduce false positives? Do a second pass to verify the frequenct on entire dataset
            - Question - how do we reduce false negatives? Lower the support threshold
        - SON (Savasere, Omiecinski, and Navathe)
            - Actions
                - Divide data into multiple samples
                - Repeatedly read the subset and using a proportionally reduced support threshold and identify candidate pairs
                - On second pass, count all the candidate item sets and determine which are frequent in the entire set
                - Monotonicity - an item cannot be frequent in the entire set of baskets unless it is frequent in at least one subset
            - Note
                - This lends itself to Hadoop or Spark HDFS etc.
                - Hadoop
                    - Divide into chunk
                    - Each chunk processed on each node with a lwoered threshold
                    - Output of each reduce will list candidate pairs
                    - Each machines sends its own list of candidate pairs to each other node
                    - Each node will count the candidate pairs in its own chunk
                    - Count for each candidate pairs sent to reduce node for aggregation
            - Benefits
                - You will not have any false negatives
        - Toivonen
            - Note - less than two passes and does not need distributed computing
            - Issue - you may have to peform it several times an unknown number of times
            - Actions
                - Lower the support threshold more than a proportional amount to try to avoid false negatives
                - Identify all singletons
                - From the sample, identify candidate pairs AND negative border (if an item set is not frequent but all its immediate subsets are frequent)
                    - ABCD is in negative border if, ABC, ACD, BCD, and ABD are frequent
                - Full pass on entire dataset to eliminate false positives, considering both candidate pairs and negative border
                    - Deaing with canddidates and the negative border that is not frequent in the sample but may be
            - Benefits
                - If all the item sets in the negative border turn out NOT to be frequent, you are good, no false negatives or false postiives
            - Issue
                - If any set in the negative border if frequent, you must repeat because 
            - Note - when processing the sample by A-Priori, each member of Ck is either in Lk or in the negative border, never both

7. 03/10/2025
    - Finding Similar Items
        - Example Problems
            - Pages with similar words
            - Customers who purchased similar products
            - Images with similar features
            - Entity resolution
        - Similar Documents
            - Goal - given a body of documents, find pairs of documents with a lot of text in common
                - Example - mirror website, plagiarism
                - Comparisons - N choose 2, n2 / 2 or n2  - n / 2
            - Techniques
                - Shingling - convert documents, emails, etc to sets
                    - k-shingle or k-gram is a sequence of k characters that appears in the document
                        - Example - k = 2, doc = abcab, Set is {ab, bc, ca}
                        - Changing a word only affects k-shingles within distance k-1 from thw word
                        - Reordering paragraphs only affects the 2k shingles that cross paragraph boundaries 
                        - Example
                            - The dog which chased the cat
                            - The dog that chased the cat
                    - Practice - k = 8, 9, or 10
                        - character = 4 bytes, 10 * 4 = 40 bytes
                    - Tokens - to save space, we can hash them to 4 bytes, called tokens
                        - Hashing can cause collisions, 
                - Jaccard Similarity - measure similarity of sets before min hashing
                    - Value - intersection of set divided by union
                    - Action - theoretically construct a matrix, practically you will run out of memory
                        - First Column - universal set of shingles i.e. all unique shingles
                        - Further Columns for Documents - 1 if shingle is in document
                    - Formula
                        - Variable - a = 1,1, b = 1,0, c = 0,1, d = 0,0
                        - Simularity(C1, C2) = a / (a + b + c) or Number of Times 1,1 divided by (1,1 + 1,0 + 0,1)
                - Minhashing - convert large sets to short signatures i.e. list of integers, while preserving similarity
                    - Experimental - Permute the Rows - will run out of memory
                    - Example
                        - Hash Function
                        - Input Matrix - look for first time of c having a 1 in what row
                            - Sequence In Order - look for first time of c having a 1 in what row
                            - Sequence in Reverse Order - look for first time of c having a 1 in what row
                            - Sequence in Random Order - look for first time of c having a 1 in what row
                            - Which should it be? Does not matter, just be consistent
                        - Signature Matrix
                    - Preserving Similarity - compression can mean information is lost, if similar pre hash then similar post hash
                    - Shingling -> Compare Jaccard Similarity -> Minhashing -> Compare Signature Similarity
                        - Property - similarity of signature matrix is same
                    - Issue
                        - 1 billion shingles, for each permutation, 100 permutations, 100 billion, 4 bytes, 400GB of ram
                - Minhashing Feasible Way
                    - Instead of 100 permutations, use 100 hash functions
                    - Intuition
                    - Order
                        - Start Signature Matrix
                        - Number of Hash = Rows 
                    - Output - compressed rows of signatures
                    - SIgnature Similarity
                - Locality Sensitive Hashing - produce candidate pairs
                    - Goal - find documents with jaccard similarity at least s (threshold original similarity)
                    - General Idea - use a function that tells whether x and y is a candidate pair

8. 03/17/2025
    - Locality Sensitive Hashing
        - Goal - find documents with Jaccard similarity at least s (threshold) - 80%
        - General Idea - use a function f(x,y) that tells whether x and y is a candidate pair, a pair of elements whose similarity must be evaluated
        - For Min-Hash
            - Hash columns of signature matrix to many buckets, each pair that is matched to the same bucket is a candidate pair
        - Steps
            - Divide Signature Matrix Rows into Bands (R Rows)
            - For each band, apply a hash function with k buckets, k should be large as possible to avoid missing
                - Why? if you have enough k buckets, only same should hash to same bucket
            - 
        - Review - Example of Bands Compression
        - Issue - Jaccard and Signature similarity may not be the same
        - Question
            - Assumptions
                - C1 and C2 are similar 80%
                - Bands 20
                _ Rows Per Band 5
                - Total Rows 100
                - Total Columns 10000
            - Probability C1 and C2 have one row is similar? 80%
            - Probability C1 and C2 are not similar? 1-.8 = 20%
            - Probability C1 and C2 are smiliar in one band? .8^5
            - Probability C1 and C2 are not equal in band? 1 - (.8 ^ 5)
            - Probability C1 and C2 are not equal in all bands? (1 - (.8 ^ 5)) ^ 20
            - Probability C1 and C2 are equal in one band? 1 - ((1 - (.8 ^ 5)) ^ 20) (DETECTED)
            - Insight - 99.9XX percentage you will detect
        - Question
            - Assumptions
                - C1 and C2 are similar 30%
                - Bands 20
                _ Rows Per Band 5
                - Total Rows 100
                - Total Columns 10000
            - Probability C1 and C2 have one row is similar? .3
            - Probability C1 and C2 are not similar? 1 - .3
            - Probability C1 and C2 are smiliar in one band? .3 ^ 5
            - Probability C1 and C2 are not equal in band? 1 - (.3 ^ 5)
            - Probability C1 and C2 are not equal in all bands? (1 - (.3 ^ 5)) ^ 20
            - Probability C1 and C2 are equal in one band? 1 - ((1 - (.3 ^ 5)) ^ 20)
    - LSH Tradeoff
        - Decisions
            - Number of min hahses i.e. rows in signature matrix
            - Number of bands
                - Less Bands = Higher False Negatives, less chances to hash to same bucket
                - More Bands = Higher False Positives, more chances to 
            - Number of rows per bands
        - Idea
            - If similarity is below threshold, hope 0% chance to hash to bucket
            - If similarity is above threshold, hope 100% chance to hash to bucket
    - Three Steps
        - Shingling: Convert documents to sets
        - MinHashing - Convvert large sets to short signatures while preserving similarity
        - Locality Sensitive Hashing - focus on pairs of signatures likely to be from similar documents
    - Distance Measures
        - Idea
            - Similar points are close
        - Axioms
            - d is a distance measure if it is a function from pairs of points to real numbers
                - distance must be greater than or equal to 0
                - distance of zero means point is same
                - distance from point to point is same in both directions
                - distance from x to y must be greater than distance x to z plus z to y
        - Euclidean Distance
            - L2 Norm = square root of the sum of the squares of the differences between x and y in each dimension
                - Trianble 5, 4, 3
                    - sqrt((9 - 5)^2 + (8-5)^2)
            - L1 Norm = sum of the differences in each dimension (manhattan distance)
                - Trianble 5,4,3
                - 4 + 3 = 7
        - Cosine Distance - for vectors
            - angle between the vectors
            - p1 * p2 / |p1||p2| = output
            - cosine(output) = angle
        - Jaccard Distance - for sets
            - 1 minus Jaccard Similarity
                - Jaccard Similarity - (2 / 5) - always equal to or less than 1
                - Distance - 1 - (2/5) = 3/5 - always greater than or equal to 0
        - Edit Distance - for strings
            - number of insertions and deletes to chane one string to another
            - length of s1 + length of s2 - 2 (Longest Common Subsequence)
            - s1 = 5
            - s2 = 6
            - LCS = 4
            - 5 + 6 - 2(4) = 3
        - Hamming Distance - for bit vectors
            - number of positions in which they defer
    - Recommender Systems: Content Based Systems & Collaborative Filtering
        - Example
            - Customer X buys Metallica, Megadeath
            - Customer Y buys Metallica, Recommend Megadeath
        - Scarcity to Abundance
        - The Long Tail - choose products by rank up to a point where sales are worthwhile to pursue, Amazon can do them all, enjoying profit from all
        - Types of Recommendations
            - Editorial and Hand Curated - essential, favorite, etc.
            - Simple Aggregates - top 10, most popular, recent uploads
            - Tailored to Individuals - netflix, amazon
        - Formal Model
            - X - set of customers
            - S - set of items
            - Utility Function - take x and S and produce ratings
            - R - set of ratings, totally ordered, 0-5
        - Key Problems
            - Gathering known ratings
                - Explicit - ask people to rate items
                - Implicit - learn ratings from user actions
            - Extrapolate unknown ratings from known ones
                - Problem - utility matrix is sparse
                - Problem - cold start i.e. new items have no ratings and new users have no history
            - Evaluate extrapolation methods - measure success/performance of recommendation methods
        - Approaches
            - Content-Based
                - Main Idea - recommend items to customer x similar to previous items rated highly by x
                    - Only focus on the customer
                - Plan of Action
                    - User Likes
                    - Item Profiles
                    - Create User Profile
                    - Compare Distance of User Profile to Item Profiles
            - Collaborative
        
9. 3/24/2025
    - Content Based
        - Side Note
            - TF - term frequency of term i in doc j
                - frequency of term / count of most frequent word in the Document
            - IDF - Inverse Document Frequency
                - log(total number of docs / number of docs that mention term i)
            - TF-IDF Score = TF x IDF
        - User Profiles
            - Boolean Utility Matrix
                - Items are movies, only feature is Actor
                - Suppose
                    - User X has watched 5 movies
                    - 2 movies featuring Actor A
                    - 3 movies featuring Actor B
                    - Feature A's Weight = 2/5
                    - Feature B's Weight = 3/5
                - 1. Item Profiles
                    - M1 - Actor A = 0 or 1, Actor B = 0 or 1 = [1,0]
                    - M2 - Actor A = 0 or 1, Actor B = 0 or 1 = [1,0]
                    - M3 - Actor A = 0 or 1, Actor B = 0 or 1 = [0,1]
                    - M4 - Actor A = 0 or 1, Actor B = 0 or 1 = [0,1]
                    - M5 - Actor A = 0 or 1, Actor B = 0 or 1 = [0,1]
                - 2. User Profile = mean of item profiles
                    - Linear Combination
                    - X = 2/5A + 3/5B
                - 3. Measure Distance Cosine - angle small = recommend, angle large = do not recommend
            - Star Ratings
                - Suppose
                    - A's movies are rated 3 and 5
                    - B's movies rated 1, 2, and 4
                    - Average Rating = 3 + 5 + 1 + 2 + 4 = 15 / 5 = 3
                - Normalized Rating to User Profile
                    - A: 3 - 3 = 0
                    - A: 5 - 3 = 2
                    - A Weight = (0 + 2)/2 = 1
                    - B: 1 - 3 = -2
                    - B: 2 - 3 = -1
                    - B: 4 - 3 = 1
                    - B Weight = (-2 + -1 + 1)/3 = -2/3
                    - IMPORTANT = X = 1A - 2/3B
                - Distance - Measure User Vector to Every Movie Vector
                    - cos(user vector, item profile) = user profile * item profile / magnitude of X and magnitude of i
                        - Magnitude of X = sqrt(1^2 - (2/3)^2)
        - Advantages of Content Based
            - Focus on the user and what you like
        - Disadvantages
            - Other users are not considered
            - Features might be difficult
            - Overspecialization
    - Collaborative Filtering
        - Idea
            - Consider User X
            - Find set N of other users whose ratings are similar to x's rating
            - Estimate x's rating based on ratings of users in N
        - Finding Similar Users
            - Let rx be the vector of user x's ratings
            - Jaccard Similarity - Intersection / Union
                - Intersection = 2
                - Union = 4
                - JS = 2/4 = .5
                - Issue - rating values are ignored
            - Cosine Distance
                - Similarity Metric - Users
                    - Intuition - we want similarity between A and B to be greater than similarity between A and C
                    - Jaccard Similarity
                        - sim(a,b) = intersection / union = 1/5 = 
                        - sim(a,c) = intersection / union = 2/4 = 
                    - Cosine Similarity
                    - Missing Ratings are treated as negative, modify to Centered Cosine Similarity or Pearson Correlation Coefficient
                        - Subtract average of x from all x, average of y from all y
                - From Similarity Metric to Recommendation 
                    - Estimate rating of user x to i item - take ratings of similar users and sum them and divide by total
                - Similarity Metric - Items
                    - Find similar items
                    - Estimate rating for item i based on ratings for similar items
                    - Use similarity metrics and prediction functions as in user user model
                    - Rating of User X to Item I
                        - Summation of items j that belongs to the neighborhood of the items rated by x
        - Item to Item Collaborative Filtering
            - Estimate rating of movie 1 by user 5, neighborhood of 2 (most two similar items to movie 1)
            - Step One - Find Similarity Between Movie 1 and Movie 2-6
                - Pearson Correlation or Centered Cosine Similarity
                    - Subtract average of item from all ratings
                    - Perform Centered Cosine Similarity
                    - Identify Two Highest Similarity
                    - (Similarity Times Rating + Similarity Times Rating)/(Similarity + Similarity)
                    - Skip - if user has no rating, dont perform
        - Item to Item vs User to User
            - Item to item - complexity is reduced, items are limited in aspects and scope, humans are complex and varied
        - Pros/Cons of Collaborative Filtering
            - Pros
                - Works on any kind of item
                - No feature selection is needed
            - Cons
                - Cold Start - need enough users to find a match
                - Sparsity - user/ratings matrix is sparse
                - First Rater - cannot recommend an item that has not been rated
                - Popularity Bias - tends to recommend popular items
    - Hybrid Methods
        - Add Content Based Methods to Collaborative Filtering
        - Global Baseline
            - Goal - get Joe's rating
            - Input
                - Movie Mean
                - Sixth Sense 0.5 above average
                - Joe rates .2 stars below average so 3.5
                - Baseline - global mean + movie average - joe variance of average = 4 stars
        - Local Neighborhood - 1 Neighborhood
            - Joe rated Signs 1 star
            - Sixth Sense - 4 - 1 = 3 stars
        - Common Practice
            - Subtract global baseline from similar movies
    - Remarks and Practical Tips
        - How many computations to check? Number of movies - O(|X|)
        - Pre Compute - O(K*|X|)
        - How?
            - Locality Sensitive Hashing - near neighbor search in high dimensions
        - Evaluation
            - Hide existing ratings = Test Set
            - Root Mean Square Error - REVIEW
            - 0/1 Model
                - Coverage - number of items/users for which system can make prediction
                - Precision - accuracy of predictions
                - Receiver Operating Characteristic - tradeoff curve between false positives and false negatives
        - Problems with Error
            - Narrow focus on accuracy sometimes misses the point
                - Prediction Diversity
                - Prediction Context
                - Order of Predictions
            - Practice
                - Precision at Top 10
                - Rank Correlation 

10. 3/31/2025
    - Mining Data Streams
        - Stream Management - important when input rate is controlled externally
            - Property - infinite, non-stop, non-stationary
        - Stream Model
            - Input Elements - enter atr a rapid rate at one or more input 
            - System cannot store all at once
        - Queries
            - Ad-Hoc - one time
            - Standing - repeatedly
        - System
            - Ad-Hoc Queries/Standing Queries
            - Input
            - Processor
            - Storage
                - Limited Working Storage
                - Archival Storage
            - Output
        - Applications
            - Mining Query Streams
            - Mining Click Streams
            - Mining Social Network News Feeds
            - Sensor Networks
            - Telephone Call Records
            - IP Packets Monitored at a Switch
        - Processing Methods
            - Sliding Window - N most recent elements received
            - Case - N is so large that the data cannot be stored in memory or even on disk
            - Standing Query - what is the average of the integers in the window O(n)
                - Can we avoid doing all computations each time the window changes?
                    - Store sum, and add and subtract before division
                - Issue - entire window must be held in main memory
            - Counting Bits
                - Problem - given a stream of 0s and 1s, how many 1s are in the last k bits?
                - Solution - store n bits as window, what if we cannot store N bits?
                - Solution - approximate the answer
                    - Uniformity Assumption
                        - Counter - # of zeroes - 
                        - Counter - # of ones
                        - S = 1's, Z = 0's, N
                        - How many 1's have I seen?
                            - S/S+Z = P(1)
                            - (S+Z) = ones and zeroes you have seen
                        - How many 1's in remaining N bits?
                            - P(1) * N = estimate of ones
            - Exponential Windows - doesn't quite work
                - Idea - summarize exponentially increasing regions of the stream looking backward
                - Idea - drop small regions if they begin at the same point as a larger region
                - Memory - sizes of blocks and how many ones are in the block, log(n)
                - Numbers = bits, how many bits to represent number, log(n) of number
                - Total - log(n) * log(n) or O(logn^2)
                - Idea - as long as 1s are fairly evenly distributed, the error due to the unknown region is small no more than 50%
                - The error could be unbounded, starting with all zeroes and next group all ones they would be missed
            - DGIM Method (Datar, Gionis, Indyk, Motwani)
                - Idea - summarize blocks with specific number of 1s, ignore 0s
                - Idea - let block sizes increase exponentially
                - Issue - where does N end if we are not counting 0?
                - Solution - timestamp, when the bit occurred
                - Store size of block which is #1s and timestamp
                - Note - guarantees answer never off by more thasn 50%
                - Representing a Stream
                    - Either one or two buckets with the same power of 2 number of 1s
                    - Buckets do not overlap in timestamps
                    - Buckets are sorted by size - earlier buckets are not smaller than later buckets
                    - Buckets disappear when their end time is > N time units in the past
                - Bucketized Streams
                    - How many 1s in the window? 36
                - Updating Buckets
                    - Whern a new bit comes in and is 1, drop the last bucket
                    - When a new bit comes in and is 1, create a new bucket of size 1
                        - If there are now three buckets of size 1, combine oldest two into a bucket of size 2
                        - If there are now three buckets of size 2, combine oldest two into a bucket of size 4
                        - Example 1:03 on Lecture 10
                - How to Query?
                    - Sum the size of buckets but the last, add half the size of the last bucket
                - Error Bound Proof
                    - Why is error 50%?
                    - Thought
                        - Last bucket has size 2^r
                        - Assume 2^r-1 half of its 1s are still witihn the window, we make an error of at most 2^r-1
                    - Get Maximum Error
                        - Do we need to assume we have one bucket of each size or two of each size?
                            - Worst Case one bucket of each size increases the error
                        - 1 + 2 + 4 + ... + (2^r-1) = (2^r) - 1
                        - Why can you subtract 1? The first bit in a bucket must be a one, I am not working with zeroes
        - More Stream Mining
            - Bloom Filters
                - Filtering Chunks - supose we have a dataset stored in a distributed file system, spread over many chunks e.g. blocks of 64mb
                    - We want to find a particular value V, looking at as few chunks as possible
                    - Bloom Filter on each chunk is a relatively short sequence of bits that provides an answer to the question, is V in there?
                        - BF no, then there is definitely no v there
                        - BF yes, then there is a possibility
                - Order
                    - All Bloom Filters bits are 0
                    - When input x arrives, we set to 1 the bits the hash function of x produces
                - Example (REVIEW)
                    - Use N = 11 bits for out filter
                    - Stream elements are integers
                    - Use Two Hash Functions (ANY)
                        - h1(x) = value
                        - h2(x) = value
                    - 1. Start with 00000000000
                        - Number = 25
                        - Binary 25 = 11001
                            - h1(25) 
                                - Odd Numbered = [1,3,5] = 101
                                - 101 = 5
                                - 5 mod 11 = 5 position
                            - h2(25)
                                - Event Numbered = [2,4] = 10
                                - 10 = 2
                                - 2 mod 11 = 2 position
                            - 00000000000 becomes 0010010000000
                        - New Number 159
                - New Element - compute h(y) for each hash function y, if one of them says 0, no way. If one is 1, it is possible due to collisions
            - Look Up
                  - h1(25) 
                    - Odd Numbered Bits = [] = BitString
                    - Bit String = Integer
                    - Integer mod 11 = Position
                 - h2(25)
                    - Even Numbered Bits = [] = BitString
                    - Bit String = Integer
                    - Integer mod 11 = Position
                - If either position is 0, no way
            - Performance of Bloom Filters
                - False Positives - 1's
                - False Positives - Hash Functions
                - Probability = (fraction of 1's) ^ # of hash functions
            - Throwing Darts
                - Turning random bits from 0 to 1 is like throwing d darts at t targets at random
                - P(hit) = 1/t
                - P(miss) = 1 - (1/t)
                - P(all miss) = (1-(1/t))^d
                - p(all miss) = e^-d/t
                    - d = darts, t = targets
                - Example
                    - 1 billion bits - targets
                    - 5 hash fucntions
                    - 100 million elements
                    - Darts = 5 * 100m = 500 million
                    - P(miss) = e^-500million/1billion
                    - P(miss) = .607 - 60% of bits are zeroes, about 39% are 1s, this is due to collisions
                    - Density of 1's  = .393 or 1 - P(miss)
                    - False Positive = .393^hash functions = .393^5 = .00937