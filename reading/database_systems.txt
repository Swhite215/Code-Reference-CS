Title: Database Systems: Design Implementation and Management
Authors: Coronel, Morris, and Rob

Summarization of Content

1. Database Systems
    a. Data vs. Information
        - Data - raw facts
        - Information - interpretation of data
        - Knowledge - body of information and facts about a specific subject
        - Data Management - discipline that focuses on the proper generation, storage, and retrieval of data
    b. Introducing the Database
        - Database - a shared, integrated computer structure that stores collections of data and metadata
            - Data - raw facts
            - Meta-Data - information about the data
        - Database Management System (DBMS) - collection of programs that manages database structure and controls access to the stored data
        - Role and Advantages of the DBMS
            - Role - intermediary between the user and the database
            - Advantages
                - Improved Data Sharing
                - Improved Data Security
                - Better Data Integration
                - Minimized Data Inconsistency
                - Improved Data Access
                - Improved Decision Making
                - Increased End-User Productivity
        - Types of Databases
            - Single or Multi-User Databases
            - Workgroup Database < 50 users
            - Enterprise Database > 50 users
            - Centralized Database vs. Decentralized Databases
            - Discipline Specific Database
            - Operational Database - supports businesses day to day
            - Online Transaction Processsing Transactional or Production Database
            - Analytical Databases - focuses primarily on storing historical data and business metrics used exclusively for tactical and strategic decision making
            - Data Warehouse - specialized database that stores data in a format optimized for decisions support
            - Online Analytical Processing OLAP - set of tools that work together to provide advanced data analysis environment
            - Business Intelligence - comprehensive approach to capture and process business data
            - XML Database
            - Social Media
            - NoSQL - describes database systems that are not based on traditional relationship database model
            - Big Data
            - In-Memory
            - Very Large Databases VLD
            - Cloud Databases
        - Types of Data
            - Unstructured - raw state
            - Structured Data - result of formatting unstructured data to facilitate storage, use, and generation of information
            - Semi-structured Data - data already processed to some extent
            - Extensible Markup Language - language used to represent and manipulate data elements
    c. Why Database Design is Important
        - Database Design - activities that focus on the design of the database structure that will be used to store and manage data
    d. Evolution of File System Data Processing
        - Manual File Systems
        - Computerized File Systems
            - Data Processing Specialist
        - File System Redux: Modern End-User Productivity Tools
    e. Problems with File System Data Processing
        - Lengthy Development Time
        - Difficulty of Getting Quick Answers
        - Complex System Administration
        - Lack of Security and Limited Data Sharing
        - Extensive Programmming
        - Structural and Data Dependence
            - Structural Dependence - access to a file is dependent on its structure
            - Data Dependence - access is altered by change of data characteristics
            - Logical Data Format - how the human views the data
            - Physical Data Format - how the computer must work with the data
        - Data Redundancy
            - Islands of Information - scattered data locations
            - Redundancy - same data is stored unnecessarily at different places
            - Issues
                - Poor Data Security
                - Data Inconsistency
                - Anomalies
        - Lack of Design and Data-Modeling
    f. Database Systems
        - System - organization of components that define and regulate the collection, storage, management and use of data within a database environment
        - Components
            - Hardware - physical system devices
            - Software
                - Operating System, Application Programs and Utility Software
            - People
                - System Administrators
                - Designers
                - End Users
            - Procedures - instructions and rules that govern the design and sue of the database systems
            - Data
        - DBMS Functions
            - Data Dictionary Managment - stores definitions of data elements and their relationships
            - Data Storage Management - creates and manages the complex structures required for data storage
            - Performance Tuning - activities that make the database perform more efficiently
            - Data Transformation and Presentation - transforms entered data to conform to required data structures
            - Security Management - creates a security system that enfroces user security and data privacy
            - Multiuser Access Control
            - Backup and Recovery Management
            - Data Integrity Managment - promotes and enforce integrity rules
            - Database Access Langauges and Application Programming Interface
                - Structured Query Language - specify what must be done with having to specify how
            - Database Communication Interfaces
            - Why a spreadsheet is not a database: pg. 25
        - DBMS Disadvantages
            - Increased Costs
            - Management Complexity
            - Maintaining Currency
            - Vendor Dependence
            - Frequent Upgrade/Replacement Cycles

2. Data Models
    a. Data Model
        - Definition - relatively simple representation, usually graphical, of more complex real world data structures
        - Implementation Ready Data Model Components
            - A description of the data structure that will store the end-user data
            - A set of enforceable rules to guarantee the integrity of the data
            - A data manipulations methodology to support the real world data transformations
    b. Data Model Basic Building Blocks
        - Entity - a person, place, thing, or event about which data will be collected and stored
            - Each entity occurrent should be unique and distinct
        - Attribute - characteristic of an entity, e.g. a CUSTOMER entity would be described by such attributes as customer last name, customer first name, etc.
        - Relationship - describes an association among entities e.g. an agent SERVES many customers
            - One to Many 1:M - e.g. a painter paints many paintings
            - Many to Many M:M - e.g. an employee may learn many job skills and many job skills can be learned by many employees
            - One to One 1:1 - e.g. a person has only one driver license
        - Constraint - restriction place on the data to help ensure data integrity
    c. Business Rules
        - Definition - a brief, precise, and unambiguous description of a policy, procedure, or principlae within a specific organization
        - Examples
            - A customer may generate many invoices
            - An invoice is generated by only one customer
            - A training sessions cannot be scheduled for fewer than 10 employees or for more than 30 employees
        - Important of Identifying and Documenting Business Rules
            - They help to standardize the company's view of data
            - They can be communication tols between users and designers
            - They allow the designer to understand the nature, role, and scope of the data
            - They allow the designer to understand business processes
            - They allow the designer to develop appropriate relationship participation rules and constraints and to create an accurate data model
        - Naming Conventions - ENTITY_ATTRIBUTE
    d. Evolution of Data Models
        - First Generation - 1960s to 1970s - File System - VMS/VSAM
        - Second Generation - 1970s - Hierarchical and Network - IMS, ADABAS
            - Hierarchical Design - upside down tree
            - Segment - recvord type
            - Network Model - represent complex data relationships more effectively
                - Schema - conceptual organization of the entire database as viewed by the database administrators
                - Subschem - defines the portion of the database seen by the application programs that actually produce the desired information from the data within the database
                - Data Manipulation Language - defines the environment in which data can be managed and is used to work with the data in the database
                - Data Definition Language - enables the database administrator to defined the schema components
        - Third Generation - Mid 1970s - Relational - DB2, Oracle, MS SQL Server, MySQL
            - Introduced by E.F. COdd of IBM - A Relational Model of Data for Large Shared Databanks
            - Relation/Table - matrix composed of intersecting rows and columns, each row is tuple
            - Relational Database Management System RDBMS - performs the basic functions provided by the hierarchical and network DBMS systems, hides the complexities of the relational data model from the user
            - Relational Diagram - representation of the relational database's entities, the attributes within those entitities, and the relationship between those entities
            - Entity Relationship Model - uses graphical representations to model database components
                - Entity - anything about which data will be collected and stored, represented as a rectangle
                - Relationships - decsribes association among data
                - Types of Notation pg. 43
                    - Original Chen Notation
                    - Crow's Foot Notation
                    - Class Diagram Notation
        - Fourth Generation - Mid 1980s - Object Oriented, Object/Relational - Versant, DB2\
            - Object Oriented Data Model OODM - data and their relationships are contained in a single structure known as an object, semantic data model
                - Object - abstraction of a real world entity
                - Attribute - describe the properties of an object
                - Class - collection of similar objects with shared structure and behavior
                - Method - represents a real world action
                - Hierarchy - resembles an upside down tree in which each class has only one parent
                - Inheritance - ability of an object within the class hierarchy to inherit the attributes and methods of the classes above it
                - Unified Modeling Langauge - language based on OO concepts that describes a set of diagrams and symbols you can use to graphically model a system
                - UML Class Diagrams - used to represent data and their relationships within the larger UML object oriented system modeling language
            - Object Oriented Database Management System OODMBS
        - Fifth Generation - Mid 1990s - XML,, Hybrid DBMS - dbXML, MS SQL Server
            - Extended Relational Data Model - adds OO model features within the inherently simpler relational database structure
            - Extensible Markup Language - standard for the efficient and effective exchange of structured, semi-structured, and unstructured data
        - Emerging - Late 2000s to Present - Key-Value Store, Column Store - BigTable, CouchDB, MongoD
            - Big Data - refers to a movement to find new and better ways to manage large amounts of generated data and derive business insights from it while simultaenously providing high performance and scalability at a reasonable cost
            - NoSQL Databases
                - Support Distributed Database Architectures
                - Provides High Scalability, Availability, and Fault Tolerance
                - Supports Very Large Amounts of Sparse Data - large number of attributes but the number of actual data instances is low
                - Geared Toward Performance Rather Than Transaction Consistency - eventual consitency - data is not guaranteed to be consistent across all copies of the data immediately after an update
                - Key-Value
    e. Additional Models
        - External Model - end users' view of the data environment
        - Conceptual Model - represents a global view of the entire database by the entire organization
            - Software Independence - model does not depend on the DBMS software used to implement the model
            - Hardware Independence - model does not depend on the hardware used in the implementation of the model
        - Internal Model - maps the conceptual model to the DBMS, representation of the database as seen by the DBMS
            - Logical Independence - changing the internal model does not affect the conceptual model
        - Physical Model - operates at the lowest level of abstraction, decsribing the way data are saved on storage media such as disks or tapes 
            - Requirements - definition of both the physical storage devices and the physical access methods required to reach the data within those storage devices

3. The Relational Database Model
    a. A Logical View of Data
        - Tables and Their Characteristics
            - A table is perceived as a two-dimensional structure composed of rows and columns
            - Each table row (tuple) represents a single entity occurence within the entity set
            - Each table column represents an attribute, and each column has a distinct name
            - Each intersection of a row and column represents a single data value
            - All values in a column must conform to the same data format
            - Each column has a specific range of values known as the attribute domain
            - The order of the rows and columns is immaterial to the DBMS
            - Each table must have an attribute or combination of attributes that uniquely identifies each row
    b. Key 
        - Definition - consists of one or more attributes that determine other attributes
        - Dependencies
            - Determination - is the state in which knowing the value of one attribute makes it possible to determine the value of another
            - Functional Dependence - the value of one or more attributes determines the value of one or more other attributes
            - Determinant - the attribute whose value determines another is called the determinant
            - Dependent - the attribute whose value i sdetermined by the other attributes
            - Full Functional Dependence - refer to functional dependencies in which the entire collection of attributes in the determinant is necessary for the relationship
        - Types of Keys
            - Composite Key - key that is composed of more than one attribute
            - Key Attribute - attribute that is part of a key
            - Superkey - key that can uniquely identify any rows in the table
            - Candidate Key - superkey without any unncessary attributes
            - Entity Integrity - condition in which each row in the table has its own unique identity
            - Null - absence of any data value, and should never be allowed to be any part of the primary key
            - Foreign Key - primary key of one table that has been placed into another table to create common attribute
            - Referential Integrity - condition in which every reference to an entity instance by another entity instance is valid
            - Secondary Key - defined as a key that is used strictly for data retrieval purposes
    c. Integrity Rules
        - Entity Integrity
        - Referential Integrity
        - Flags - indicates the absence of some value
    d. Relational Set Operators
        - Relational Algebra - defines the theoretical way of manipulating table contents using the eight relational operators
            - SELECT - yields values for all rows found in a table that satisfy a given condition
            - PROJECT - yields all values for selected attributes
            - JOIN - allows information to be combined from two or more tables
                - Natural Join - links tables by selecting only the rows with common values in their common attributes
                - Order - Product -> Select -> Project
                - Equijoin - links tables on the basis of an equality condition that compares specified columsn of each table
                - Thetajoin - Not equijoin...
                - Inner Join - only returns matched records from the tables that are being joined
                - Outer Join - matched pairs would be retained, and any unmatchedvalues in the other table would be left null
                - Left Outer Join - yields all of the rows of one table, including those that do not have a matching value in the other table
                - Right Outer Join - yields all of the rows in the other table, including those that do not have a matching value in one table
            - INTERSECT - yield only the rows that appear in both tables
            - UNION - combines all rows from two tables, excluding duplicates
            - DIFFERENCE- yields all rows in one table that are not found in the other table
            - PRODUCT - yield all possible pairs of rows from two tables
            - DIVIDE - uses one two column table as the dividend and one single column table as the divisor
        - Closure - the use of relational algebra operators on existing relations produce new relation
    e. The Data Dictionary and System Catalog
        - Data Dictionary - provides a detailed description of all tables in the database created by the user and designer
        - System Catalog - detailed system data dictionary that describes all objects within the database, including data about table names, table creator, and creation date, number of columns, data type of each column, index filenames, index creators, users, and access privileges
        - Homonyms - indicates the use of the samed name to label different attributes, these should be avoided
        - Synonym - use of a different name to describe the same attribute, these should be avoided
    f. Relationships within the Relational Database
        - 1:M - supported
        - 1:1 - supported
        - M:N - not supported directly in the relational environment
            - Composite/Bridge/Associative Entity - link many to many tables
    g. Data Redundancy Revisited
    h. Indexes - orderely arrangedment used to logically access rows in a table
        - Index Key - index's reference point
        - Unique Index - index key can only have one pointer value associated with it
    i. Codd's Relational Database Rules
        - Information - all information in a relational database must be logically represented as column values in rows within tables
        - Guaranteed Access - Every value in a table is guaranteed to be accessible through a combination of table name, primary key value, and column name
        - Systematic Treatment of Nulls - Nulls must be represented and treated in a systematic way, independent of data types
        - Dynamic Online Catalog Based On the Relational Model - The metadata must be stored and managed in a ordinary way, that is, within tables within the database
        - Comprehensive Data Sublanguage - must support one well-defined, declarative langauge
        - View Updating - Any view that is theoretically updatable must be updatable through the system
        - High Level Insert, Update, and Delete - the database must support set level inserts, updates, and deletes
        - Physical Data Independence - Application programs and ad hoc facilities are logically unaffected when physical access methods or storage structures are changed
        - Logical Data Independence - Application programs and ad hoc facilities are logically unaffected when changes are made to the table structures that preserve the original table values e..g. changing order of columns
        - Integrity Independence - All relational integrity constraints must be definable in the relational language and stored in the system catalog
        - Distribution Independence - The end users and application programs are unaware of and unaffected by the data location
        - Nonsubversion - users must not be allowed to bypass the integrity rules of the database
        - Rule Zero - All preceedeing rules are based on the notion that to be considered relational, a database must use its relational facilities exclusively for management

4. Entity Relationship Modeling
    a. Entity Relationship Model
        - ERD - represents conceptual database as viewed by the end user
        - Notations - Chen, Crow's Foot and UML
        - Entities - entity set i.e. an entire table
        - Attributes - characteristics of entities
            - Required - attribute that must have a value
            - Optional - attribute that doesn't require a value
        - Domains - set of possible values for a given attribute
        - Identifiers - one or more attributes that uniquely identify each entity instance
        - Composite Identifier - primary composed composed of more than one attribute
        - Composite and Simple Attributes
            - Composite - attribute that can be further subdivided to yield additional attributes
            - Simple - attribute that cannot be subdivided
        - Single Valued Attributes - attribute that can only have a single value
        - Multivalued Attributes - attributes that can have many values
        - Derived Attribute - attribute whose value is calculated from other attributes
        - Relationships - association between entities
        - Connectivituy and Cardinality
            - Connectivity - relationship classification
            - Cardinality - minimum and maximum numnber of entity occurences associated with one occurence of the related entity
        - Existing Dependence - entity can exists in the database only when it is associated with another related entity occurence
        - Existence Independent - entity can exist apart from all of its related entities - strong or regular entity
        - Weak Relationships/Non-Identifying - exists if the primary key of the related entity does not contain a primary key component of the parent entity
        - Stong Relationships/Identifying - exists when the primary key of th related entity contains a primary key component of the parent entity
        - Relationship Participation
            - Optional - one entity occurrence does not require a corresponding entity occurrence in a particular relationship - course does not have to have classes
            - Mandatory - one entity occurrence requres a corresponding entity occurence in a particular relationship - class must be taught be professor
        - Relationship Degree - indicates the number of entities or participants associated with a relationship
            - Unary - association is maintained within a single entity
                - employee requires another employee to be manager
            - Binary - two entities are associated - most common
                - professor teaches one or more classes
            - Ternary - three entities are associated
                - doctor writes prescriptions
                - patient receives prescriptions
                - drug appears in prescriptions
        - Recursive Relationships - a relationship can exists between occurrences of the same entity set
        - Associative Entities -  composite or bridge entity faciliates turning M:N relationship into two 1:M relationships
    b. Developing an Entity Relationship Diagram
        - Iterative Process Involving
            - Create a detailed narrative of the organization's description of operations
            - Identify the business rules based on the description of operations
            - Identify the main entities and relationships from the business rules
            - Develop the initial ERD
            - Identify the attributes and primary keys that adequately decsribe the entities
            - Revise and review the ERD
            - Example - Tiny College pg. 129
    c. Database Design Challenges: Conflicting Goals
        - Design Standards - standards guide you in developing logical structures that minimize data redundancies
        - Processing Speed - means minimal access time, which may be achieved by minimizing the number and complexity of logically desirable relationships
        - Information Requirements - dictate data transformations, and may expand the number of entities and attributes within the design

5. Advanced Data Modeling
    a. Extended Entity Relationship Model
        - EERM - result of adding more semantic constructs to the original entity relationship
        - Entity Supertypes and Subtypes
            - Benefits
                - Avoids unnecessary nulls in attributes when some employees have characteristics that are not shared by other employees
                - Enables a particular employee type to participate in relationships that are unique to that type
            - Supertype - generic entity type that is related to one or more entity subtypes
                - Contains common characteristics
            - Criteria
                - There must be different, identifiable kinds or types of the entity in the user's environment
                - The different kind or types of instances should each have one or more attributes that are unique to that kind of type of instance
        - Specialization Hierarchy - depicts the arrangement of higher-level entity supertypes and lower-level entity subtypes
            - Supertype - Employee
            - Possible Subtypes - Pilot, Mechanic, and Accountant
            - Specialization Hierachy Provides the Means To:
                - Support attribute inheritance
                - Define a special supertype attribute known as the subtype discriminator
                - Define disjoint/overlapping constraints and complete/partial constraints
        - Inheritance - enables an entity subtype to inherit the attributes and relationships of the supertype
            - Employee HAS Dependent
            - Employee Subtype Pilot - So Pilot HAS Dependent
        - Subtype Discriminator - attribute in the supertype entity that determines to which subtype the supertype occurrence is related
            - Employee Entity has EMP_TYPE
        - Disjoint and Overlapping Constraints
            - Disjoint/Non-Overlapping - subtypes that contain a unique subset of the supertype entity set
            - Overlapping - subtypes that contain nonunique subsets of the supertype entity, that is each entity instance of the supertype may appear in more than one subtype
                - STUDENT and EMPLOYEE are overlapping subtypes of the supertype PERSON
                - PROFESSOR and ADMINSTRATOR are overlapping subtypes of the supertype EMPLOYEE
        - Completeness Constraint - specifies whether each entity supertype occurence must also be a member of at least one subtype
            - Partial Completeness - means that not ever supertype occurrence is a member of a subtype
            - Total Completeness - means that every supertype occurrence must be a member of at least one subtype
        - Specializations and Generalization
            - Specialization - top-down process of identifying lower-level, more specific entity subtypes from a higher-level entity supertype
            - Generalization - bottom-up process of identifyuing a higher-level, more generic supertype from lower-level entity subtypes
    b. Entity Clustering
        - Entity Cluster - virtual entity type used to represent multiple entities and relationships in the Entity Relationship Diagram
            - Formed by combining multiple interrelated entities into a single , abstract entity object
    c. Entity Integrity: Selecting Primary Keys
        - Natural Keys and Primary Keys
            - Natural Key/Natural Identifier - real-world, generally accepted identifier used to distinguish real world objects
        - Primary Key Guidelines
            - Main Function - uniquely identify an entity instance or row within a table
            - Secondary Function - primary keys and foreign keys are used to implement relationships among entities
        - When to Use Composite Primary Keys
            - #1 - As identifiers of composite entities, in which each primary key combination is allowed only once in the M:N relationship
            - #2 - As identifiers of weak entities, in which the weak entity has a strong identifying relationship with the parent entity
        - Desirable Primary Key Characteristics
            - Unique - PK must uniquely identify each entity instance
            - Nonintelligent - PK should not have embedded semantic meaning other than to uniquely identify each entity instance
            - No Change Over Time
            - Preferrable Single Attribute
            - Preferably Numeric
            - Security-Compliant
        - When to Use Surrogate Primary Keys - primary key created by the database desinger to simplify the identification of entity instances
            - #1 - Primary key doesn't exist in the real world or the existing natural key might not be a suitable primary key
    d. Design Cases: Learning Flexible Database Design
        - Case 1 - Implementing 1:1 Relationships
            - Basic Rule - put the primary key of the "one" side (the parent entity) on the "many" side (the dependent entity), as a foreign key
            - 1:1 Options
                - #1 - Place a foreign key in both entities
                - #2 - Place a foreign key in one of the entities
        - Case 2 - Maintaining History of Time-Variant Data
            - Time-Variant Day - refer to data whose values change over time and for which you must keep a history of the data changes
            - Multivalue Attribute Situation
                - Create a new entity in a 1:M relationship with the original entity - EMP_SALARY, multiple entires per employee in EMP_SAL_HISTORY
        - Case 3 - Fan Traps
            - Design Trap - occurs when a relationship is improperly or incompletely identified and is therefore represented in a way that is not consistent with the real world
            - Fan Trap - occurs when you have one entity in two 1:M relationships to toehr entities, thus producing an association among the other entities that is not expressed in the model
                - Issue - DIVISION is 1:M with TEAMS and 1:M with PLAYERS
                - Solution - DIVISION is 1:M with TEAMS and TEAMS is 1:M with PLAYERS
        - Case 4 - Redundant Relationships

6. Normalizations of Database Tables
    a. Database Tables and Normalization
        - Normalization - is a process for evaluating and correcting table structures to minimize data redundancies, thereby reducing the likelihood of data anomalies
        - Normal Forms - 1NF < 2NF < 3NF < 4NF
        - Denomalization - produces a lower normal form
    b. The Need for Normalization
        - Situation #1 - after the initial ERD is complete, the designer can use normalization to analyze the relationships among the attributes within each entity and determine if the structure can be improved through normalization
        - Situation #2 - modify existing data structures that can be in the form of flat files, spreadsheets, or older database structures
    c. The Normalization Process
        - Goal: ensure each table conforms to the concept of well-formed relations - in other words, tables that have the following characteristics
            - Each table represents a single subject
            - No data item will be unnecessarily stored in more than one table (minimum controlled redundancy)
            - All nonprime attributes in a table are dependent on the primary key - the entire primary key and nothing but the primary key
            - Each table is void of insertion, update, or deletion anomalies, which ensures the integrity and consistency of the data
        - Normal Forms
            - First Normal Form 1NF - Table format, no repeating groups, and PK identified
            - Second Normal Form 2NF - 1NF and no partial dependencies
            - Third Normal Form 3NF - 2NF and no transitive dependencies
            - Boyce-Codd Normal Form BCNF - Every determinant is a candidate key
            - Fourth Normal Form 4NF - 3NF and no independent mutlivalued dependencies
                - Candidate Key - minimal irreducible superkey
                - Primary Key - candidate key selected to be the primary means used to identify the rows in the table
            - Goal - data modeler wants to ensure that all tables are at least in third normal form 3NF
        - Determination and Functional Dependnence
            - Functional Dependence - The attribute B is fully functionally dependent on the attribute A if each value of A determines one and only one value of B
            - Functional Dependence - Attribute A determines Attributw B if all of the rows in the table that agree in value for Attribute A also agree in value for Attribute B
            - Fully Functional Dependence - If Attribute B is functioanlly depedendent on a composite key A but not on any subset of that composite key, the Attribute B is fully functionally dependent on A
            - Partial Dependencuy - exists when there is a functional dependence on which the determinant is only part of the primary key e.g. (A, B) -> (C,D), B -> C and (A,B) is the primary key, then B -> C is a partial dependency
            - Transitive Dependency - exists when there are functional dependencies such that X -> Y, Y -> Z, and X is the primary key. Transitive becauser X determines the value of Z via Y.\
            - Repeating Group - derives its name from the fact that a group of multiple entries of the same type can exist for a single key attribute occurrence
        - Guide
            - Conversion to First Normal Form
                - Step 1: Eliminate the Repeating Groups
                - Step 2: Identify the Primary Key - May Be Composite
                - Step 3: Identify All Dependencies - Dependency Diagram
            - Conversion to Second Normal Form
                - Step 1: Make New Tables to Eliminate Partial Dependencies
                - Step 2: Reassign Corresponding Dependent Attributes
            - Conversion to Third Normal Form
                - Step 1: Make New Tables To Eliminate Transitive Dependencies
                - Step 2: Reassign Corresponding Dependent Attributes
    d. Improving the Design
        - Evaluate PK Assignment
        - Evaluate Naming Conventions
        - Refine Attribute Atomicity
        - Identify New Attributes
        - Identify New Relationships
        - Refine Primary Keys as Required for Data Granularity
        - Maintain Historical Accuracy
        - Evaluate Using Derived Attributes
    e. Higher Level Normal Forms
        - The Boyce-Codd Normal Form - every determinant in the table is a candidate key
        - Fourth Normal Form - 3NF and has no multivalued dependencies
    f. Normalization and Database Design
        - Good Design Principles and Procedures Alongside Normalization Procedures
            - Develop an ERD through iterative process
                - Identify relevant entities, attributes, and their relationships
            - Use Normaliztion to focus on the characteristices of specific entities
    g. Denormalization
    h. Data Modeling Checklist
        - Business Rules
            - Properly document and verify all business rules with the end users
            - Ensure that all business rules are written precisely, clearly, and simpl.
            - Identify the source of all business rules, and ensure that each business role is justified, dated, and signed off by an approving authority
        - Data Modeling
            - Naming Conventions
                - Entity Names
                    - Should be nouns that are familiar to business and should be short and meaningful
                    - Should document abbreviations, synonyms, and aliases for each entity
                    - Should be unique within the model
                    - For composite entities, may include a combinmation of abbreviated names of the entities linked through the composite entity
                - Attribute Names
                    - Should be unique within the entity
                    - Should use the entity abbreviation as a prefix
                    - Should be descriptive of the characteristic
                    - Should use suffixes such as _ID, _NUM, _CODE for the PK attribute
                    - Should not be a reserved word
                    - Should not contain spaces or special characters
                - Relationship Names
                    - sShould be active or passive verbs that clearly indicate the nature of the relationship
            - Entities
                - Each entity should represent a single subject
                - Each entity should represent a set of distinguishable entity instnace
                - All entities should be in 3NF or higher, any below 3NF should be justified
                - The granularity of the entity instance should be clearly defined
                - The PK should be clearly defined and support the selected data granularity
            - Attributes
                - Should be simple and single valued
                - Should document default values, constraints, synonyms and aliases
                - Derived attributes shoudl be clearly identified and include sources
                - Should not be redundant unless this is required for transaction accuracy, performance, or maintaining a history
                - Nonkey attributes must be fully dependent on the PK attribute
            - Relationships
                - Should clearly identify relationship participants
                - Should clearly define participation, connectivity, and document cardinality
            - ER Model
                - Should be validated against expected processes: inserts, updates, and deletes
                - Should evaluate where, when, and how to maintain a history
                - Should not contain redundant relationships except as required
                - Should minimze data redundancy to ensure single place updates
                - Should conform to the minimal data rule. All that is needed is there, and all that is there is needed

7. Introduction to Structured Query Language
    a. Introduction to SQL
        - SQL Data Definition Commands
            - CREATE SCHEMA AUTHORIZATION - Create a database schema
            - CREATE TABLE - Creates a new table in the user's database schema
            - NOT NULL - Ensures that a column will not have null values
            - UNIQUE - Ensures that a column will not have duplicate values
            - PRIMARY KEY - Defines a primary key for a table
            - FOREIGN KEY - Defines a foreign key for a table
            - DEFAULT - Defines a default value for a column (when no value is given)
            - CHECK - Validates data in an attribute
            - CREATE INDEX - Creates an index for a table
            - CREATE VIEW - Creates a dynamic subset of rows and columns from one or more tables
            - ALTER TABLE - Modifies a table's definitions (adds, modifies, or deletes attributes or constraints)
            - CREATE TABLE AS - Creates a new table based on a query in the user's database schema
            - DROP TABLE - Permanently deletes a table (and its data)
            - DROP INDEX - Permanently deletes an index
            - DROP VIEWQ - Permanently deletes a view
        - SQL Categories
            - Data Definition Language - SQL includes commands to create database objects such as tables, indexes, and views
            - Datat Manipulation Language - SQL includes commands to insert, update, delete, and retrieve data within the database tables
        - SQL Data Manipulation Commands
            - INSERT - Insert rows into a table
            - SELECT - Selects Attributes from rows in one or more tables or views
                - WHERE - Restricts the selection of rows based on a conditional expression
                - GROUP BY - Groups the selected rows based on one or more attributes
                - HAVING - Restricts the selection of grouped rows based on a condition
                - ORDER BY - Orders the selected rows based on one or more attributes
            - UPDATE - Modifies an attribute's values in one or more table's rows
            - DELETE - Deletes one or more rows from a table
            - COMMIT - Permanently saves data changes
            - ROLLBACK - Restores data to their original values
            - Comparison Operators - =,<,>,<=,>=,<>
            - Logical Operators - AND, OR, NOT
            - Special Operators
                - BETWEEN - Checks whether an attribute value is within a range
                - IS NULL - Checks whether an attribute value is null
                - LIKE - Checks whether an attribute value matches a given string pattern
                - IN - Checks whether an attribute value matches any value within a value list
                - EXISTS - Checks whether a subquery returns any rows
                - DISTINCT - Limits values to unique values
            - Aggregate Functions - used with SELECT to retun mathematical summaries on columns
                - COUNT - Returns the number of rows with non-null values for a given column
                - MIN - Returns the minimum attribute value found in a given column
                - MAX - Returns thae maximum attribute value found in a given column
                - SUM - Returns the sum of all values for a given column
                - AVG - Returns the average of all values for a given column
    b. Data Definition Commands
        - Create the Database
        - Create The Database Schema - CREATE SCHEMA
        - Data Types
            - PK - Primary Key
            - FK - Foreign Key
            - CHAR - Fixed length character data (1 to 255 characters)
            - VARCHAR - Variable length character data (1 to 2000 characters)
            - NUMBER - Numeric data
                - NUMBER(9,2) - numbers that have two decimal places an dup to nine digits long
            - INT - Integet values
            - SMALLINT
            - DATE
            - DECIMAL
        - Create Table Structures - CREATE TABLE
        - Create SQL Constraints
            - NOT NULL, UNIQUE, DEFAULT, CHECK, CONSTRAINT
        - Create SQL Indexes
            - CREATE INDEX - improves the efficiency of searches and avoids duplicate column values
    c. Data Manipulation Commands
        - Adding Table Rows - INSERT
            - Inserting Rows with Null Attributes
            - Inserting Rows with Optional Attributes
                - INSERT INTO TABLE (OPTIONAL_VALUES) VALUES (VALUES)
        - Saving Table Changes - COMMIT - any changes made to the table contents are not saved on disk until you close the database
        - Listing Table Rows - SELECT
        - Updating Table Rows - UPDATE
        - Restoring Table Contents - ROLLBACK
        - Deleting Table Rows - DELETE
        - Inserting Tbale Rows with a SELECT Subquery
            - INSERT INTO tablename SELECT columnlist FROM tablename;
    d. SELECT Queries
        - Selecting Rows with Conditional Restrictions
            - SELECT columnlist FROM tablelist WHERE conditionlist
        - Using Comparison Operators on Dates
            - SELECT columnlist FROM tablelist WEHERE DATE >= '20-Jan-2012
        - Arithmetic Operators: The Rule of Precedence
            - Perform operations within parentheses
            - Perform power operations
            - Perform multiplication and divisions
            - Perform additions and substractions
        - Logical Operators: AND, OR, and NOT
        - Special Opertions: BETWEEN, IS NULL, LIKE, IN, EXISTS
    e. Additional Data Definition COmmands
        - Changing a Column's Data Type - ALTER TABLE tablename MODIFY (columnlist DATE_TYPE)
        - Changing a Column's Data Characteristics - ALTER TABLE tablename MODIFY (columnlist DECIMAL(10,2))
        - Adding a Column - ALTER TABLE tablename ADD (columnlist DATA_TYPE)
        - Dropping a Column - ALTER TABLE tablename DROP columnlist
        - Advanced Data Updates - UPDATE tablename SET columnlist = VALUE WHERE columnlist = VALUE
        - Copying Parts of Tables - INSERT INTO target_tablename SELECT source_columnlist FROM source_tablename
        - Adding Primary and Foreign Key Designations - ALTER TABLE tablename ADD PRIMARY_KEY(column)
        - Deleting a Table from the Database - DROP TABLE
    f. Additional SELECT Query Keywords
        - Ordering a listing - ORDER BY
        - Listing Unique Values - UNIQUE
        - Aggregate Functions - COUNT, MAX and MIN, SUM, AVG
        - Grouping Data - GROUP BY
    g. Joining Database Tables
        - SEELCT columnlist FROM tablenname WHERE PRIMARY_KEY = FOREIGN_KEY
        - Joining Tables with an Alias
        - Recursive Joins

8. Advanced SQL
    a. SQL Join Operators
        - Conditions
            - Natural Join - two rows have common values in common columns
            - Equality or Inequality - two rows meet a given join condition
            - Outer Join - have common values in common columns or have no matching values
        - Joins
            - Inner Join - traditional join in which only rows that meet a given criterion are selected
            - Outer Join - returns not only the matching rows but the rows with unmatched attribute values for one table or both tables to be joined
            - Cross Join - performs a relational product off two tables
            - Natural Join - returns all rows with matchings values in the matching columns and eliminates duplicate columns
            - Join Using Clause - returns only the rows with matchings values in the column indicated in the USING clause
            - Join ON Clause - return only the rows that meet the indicated join condition
    b. Subqueries and Correlated Queries
        - Characteristics
            - A subquery is a query, SELECT statement, inside a query
            - A subquery is normally expressed inside parentheses
            - The first query in the SQL statement is known as the outer query
            - The query inside the SQL statement is known as the inner query
            - The inner query is executed first
            - The output of an inner query is used as the input for the outer query
            - The entire SQL statement is sometimes referred to as a nested query
        - Types
            - WHERE Subqueries - uses an inner SELECT subquery on the right side of a WHERE comparison (single value)
            - IN Subqueries - uses an inner SELECT subquery on the right side of a WHERE comparison (multiple values)
            - HAVING Subqueries - uses an inner SELECT subquery on the right side of a HAVING comparison
            - Multirow Subquery Operators: ANY and ALL
            - FROM Subqueries - SELECT subquery within FROM statements
            - Correlated Subqueries - subquery that executes once for each row in the outer query
    c. SQL Functions
        - Date and Time Functions - take one parameter of a datter or character data type and return a value
            - YEAR - returns a four digit year
            - MONTH - returns a two digit month
            - DAY - returns the number of the day
            - DATE - returns today's date
            - DATEADD - adds a number of selected time periods to a date
            - DATEDIFF - subtracts two dates
            - TO_CHAR - returns a character string or a formatted sting from a date value
            - TO_DATE - returns a date value using a character string and a date format mask
            - SYSDATE - returns today's date
            - ADD_MONTHS - adds a number of months or years to a date
            - LAST_DAY - returns the date of the last day of the month given in a date
        - Numeric Functions
            - ABS - returns absolute value of a number
            - ROUND - rounds a value to a specified precision
            - CEIL/CEILING/FLOOR - returns the smallest integer greater than or equal to a number or returns the largbest integer equal to or less than a number
        - String Functions
            - Concatenation - concatenates data from two different character columns and returns a single column
            - UPPER and LOWER - returns a string in all capital or all lowercase letters
            - SUBSTRING - returns a substring or part of a given string parameter
            - LENGTH - returns the number of characters in a single value
        - Conversion Functions
            - TO_CHAR - returns a character string from a numeric value
            - TO_CHAR - returns a character string or a formatted character string from a date value
            - TO_NUMBER - returns a formatted number from a character string
            - DECODE - compares an attribute or expression with a series of values and returns an assocaited value or a default value
    d. Relational Set Operators
        - UNION - combines rows from two or more queries without including duplicate rows - query UNION query
        - UNION ALL - combines rows from two or more queries including duplicates - query UNION ALL query
        - INTERSECT - combine rows from two queries, returning only the rows that appear in both sets
        - MINUS - combines rows from two queries and returns only the rows that appear in the first set but not in the second - query MINUS query
        - Syntax Alternatives - IN or NOT IN to obtain results similar to INTERSECT or MINUS
    e. Virtual Tables: Creating a View
        - View - virtual table based on a SELECT query
        - Base Tables - tables on which the view is based
        - CREATE VIEW viewname AS SELECT query
        - Updatable Views
            - Batch Update Routine - pools multiple transactions into a single batch to update a master table field in a single operation
            - Updatable View - can be used to update attributes in any base table used in the view
    f. Oracle Sequences
    g. Procedural SQL
        - Persistent Stored Module - block of conde containing standard SQL statements and procedural extensions that is stored and executed at the DBMS server
        - Procedural Language SQL - langauge that makes it possible to use and store procedural code and SQL statements within the database and to merge SQL and traditional programming constructs, such as variables, conditional processing, basic loops, and error trapping
        - Anonymous PL/SQL Block
        - PL/SQL Basic Data Types
            - CHAR, VARCHAR, NUMBER, DATE, %TYPE
        - Triggers - procedural SQL code that is automatically invoked by the RDBMs upon the occurrence of a given data manipulation event
            - Invoked before or after a data row is inserted, updated, or deleted
            - Associated with a database table
            - Each database table may have one or more triggers
            - Executed as part of the transaction that triggered it
            - Uses
                - Enforce constraints that cannot be enforced at the DBMS design and implementation levels
                - Add functionality by automating critical actions and providing appropriate warnings and suggestions for remedial action
                - Can be used to update table values, insert records in tables, and call other stored proceedures
                - Auditing
                - Automatic generation of derived column values
                - Enforcement of business or security constraints
                - Creation fo replica tables for backup purposes
            - Definition
                - Trigger Timing - BEFORE or AFTER
                - Trigger Event - Statement that causes the trigger to execute - INSERT, UPDATE, or DELETE
                - Trigger Level
                    - Statement-Level Trigger
                    - Row-Level Trigger
                - Trigger Action
        - Stored Procedures - named collection of procedural and SQL statments
        - PL/SQL Processing with Cursors
            - Cursor - special construct used in procedural SQL to hold the data rows returned by a SQL query
                - Implicit Cursor - automatically created in procedural SQL when the SQL statement returns only one value
                - Explicit Cursor - created to hold the output of a SQL statement that may return two or more rows (but neven zero or only one)
        - PL/SQL Stored Functions - basically a named group of procedural and SQL statements that returns a value
    h. Embedded SQL - term used to refer to SQL statements contained within an application programming language

9. Database Design
    a. The Information System - provides for data collection, storage, and retrieval, comkposed of people, hardware, software, database(s), application programs and procedures
        - Systems Analysis - process that establishes the need for an information system and its extent
        - Systems Development - process of creating an information system
        - Application - Data and thre code by which the data is transfromed into information
        - Database Development - describes the process of database design and implementation
    b. The Systems Development Life Cycle SDLC - traces the history of an information system
        - Planning Phase - yields a general overview of the company and its objectives
            - Should the existing system be continued?
            - Should the existing system be modified?
            - Should the existing system be replaced?
            - Feasibility Study Focus
                - Technical aspects of hardware and software requirements
                - System Cost
                - Operational Cost
        - Analysis Phase - audit of user requirements
            - What are the requirements of the current system's end users?
            - Do those requirements fit into the overall information requirements?
        - Detailed System Design Phase - designer completes the design of the system's processes
            - Includes - screens, menus, reports, and other devices that help make the system a more efficient information generator
        - Implementation Phase - hardware, DBMS software, and application programs are installed, and the database design is implemented
        - Maintenance Phase
            - Corrective Maintenance - responding to system errors
            - Adapative Maintenance - responding to changes in the business environment
            - Perfective Maintenance - enhancing the system
        - Computer Aided Systems Engineering - help produce better systems within a reasonable amount of time and at a reasonable cost
    c. The Database Life Cycle
        - Database Initial Study Phase
            - Analyse Company Situation
                - Company Objectives, Operations, and Structure
                - What is the organization's general operating environment, and what is its mission within that environment?
                - What is the organization's structure?
            - Define Problems and Constraints
                - How does the existing system function?
                - What input does the system require?
                - What documents does the system generate?
                - By whom and how is the system output used?
            - Define Databse System Specification 
                - Objectives, Scope, and Boundaries
                    - What is the proposed system's initial objective?
                    - Will the system interface with other existing or future systems in the company?
                    - Will the system share the data with other systems or users?
                    - Scope - defines the extewnt of the design according to operational requirements
                    - Boundaries - external limits to the system
        - Database Design Phase
            - Goal - making sure the final product meets user and system requirements
            - Views - Business and Designer
            - Process
                - Conceptual Design
                    - Data analysis and requirements
                    - Entity Relationship modeling and normalization
                    - Data model verification
                - DBMS Selection
                - Logical Design
                    - Map conceptual model to logical model components
                    - Validate logical model using normalization
                    - Validate logical model integrity constraints
                    - Validate logical model against user requirements
                - Physical Design
                    - Define data storage organization
                    - Define integrity and security measures
                    - Determine performance measures
            - Produce - instructions dealing with creation of tables, attributes, domains, views, indexes, security constraints, and storage and performance guidelines
        - Implementation and Loading Phase
            - Install the DBMS
                - Virtualization - technical that creates lofical representations of computing resources that are independent of the underlying physical computing resources
            - Create the Database
            - Load or Convert the Data
        - Testing and Evaluation Phase
            - Test the Database
                - Physical Security
                - Password Security
                - Access Rights
                - Audit Trails
                - Data Encryption
                - Diskless Workstations
            - Fine Tune the Database
            - Evaluate the Database and Its Application Programs
                - Backups
                    - Full - all database objects are backed up in their entirety
                    - Differential - only the objects that have been updated or modified sinces the last full backup are backed up
                    - Transaction Log - backs up only the transaction log operations that are nto reflected in a previous backup copy of the database
                - Database Failures
                    - Software, Hardware, Programming Exceptions, Transactions, and External Factors
        - Operation Phase
        - Maintenance and Evolution
            - Preventive Maintenance - back up
            - Corrective Maintenance - recovery
            - Adadptive Maintenance - enhancing performance, adding entities, etc.
            - Access Permissions
            - Database Statistics
            - Security Audits
            - Usage Summaries
    d. Conceptual Design - first stage in the database design procss
        - Goal - design a database that is independent of database software and physical details
        - Output - conceptual data model that describes the main data entities, attributes, relationships, and constraints of a given problem domain
        - Minimal Data Rule - All that is needed is there, and all that is there is needed
        - Steps
            - Data Analysis and Requirements
                - What kind of information is needed?
                - Who will use the information? How is the information used? What are the various end user data views?
                - Where is the information to be found? How is the information extracted once it is found?
                - What data elements are needed to produce the information? What are the data attributes? What relationships exist amongst the data? What is the data volume? How frequently are the data used?  What data transformations are used?
                - Develop and gather end user data views
                - Directly observe the cuyrrent system
                - Interface with the systems design group
                - Description of Operations - a formal document that provides precise, up to date, and throroughly reviewed description fo the activities that define an organization's operating environment
                - Buisness Rule Benefits
                    - Help to standardize company's view of data
                    - Constitute a communications tool between users and designers
                    - Allow the designer to understand the nature, role, and scope of the data
                    - Allow the designer to understand business processes
                    - Allow the designer to develop appropriate relationship participation rules and foreign key constraints
            - Entity Relationship Modeling and Normalization
                - Identify, analyze, and refine the business rules
                - Identify the main entities using results from Step 1
                - Define the relationships among entities using results of Step 1 and Step 2
                - Define attributes, primary keys, and foreign keys for each of the entities
                - Normalize the entities
                - Complete the initial ER diagram
                - Validate the ER model against the end users' information and processing requirements
                - Modify the ER model
            - Data Model Verification
                - Tests against End-user data views, all required transactions, access rights and security, and business imposed data requirements and constraints
                - Module - information system component that handles a specific business function
                - Module Benefits
                    - modules can be delegated to design groups within teams, speeding up development work
                    - modules simplify the design work
                    - module can be prototyped quickly
                    - modules allow for incremental deployment of the system
                - ER Model Verification Process
                    - Identify the ER model's central entity
                    - Identify each module and its components
                    - Identify each module's transaction requirements
                    - Verify all processes against system requirements
                    - Make all necessary changes
                    - Repeat Steps 2-5 for all modules
                - Objectives
                    - Ensure Module's Cohesivity i.e. the entities must be strongly related, and the module must be complete and self-sufficient
                    - Address Moduel Coupling i.e. the exten to which modules are independent of one another, goal is low coupling, indicating that they are independent of other modules
            - Distributed Database Design
    e. DBMS Software Selection
        - Important Factors
            - Cost
            - DBMS Features and Tools
            - Underlying Model - hierarchiacal, network, relational, object/relational, or object-oriented
            - Portability
            - DBMS Hardware Requirements
    f. Logical Design - second stage in the database design process
        - Steps
            - Map the conceptual model to logical model components
                - Map strong entities
                - Map supertype/subtype relationships
                - Map weak entities
                - Map binary relationships
                - Map higher-degree relationships
            - Validate the logical model using normalization
            - Validate the logical model integrity constraints
            - Validate the logical model against user requirements
    g. Physical Design - process of determining the data storage organization and data access characteristics of the database to ensure its integrity, security and performance
        - Steps
            - Define data storage organization
                - Determine the locaiton and physical storeage organization for each table
                - Identify indexes and th type of indexes to be used for each table
                - Identify the views and the type of views to be used on each table
            - Define integrity and security measures
                - Define user and security groups and roles
                - Assign security controls
            - Define performance measurements
    h. Database Design Strategies
        - Top-Down Design - starts by identifying the data sets and then defines the data elements for each of those sets
        - Bottom-Up Design - identifies the data elements and then groups them together in data sets
    i. Centralized vs. Decentralized design
        - Centralized - productive hwen the data component has a relatively small number of objects and procedures
        - Decentralized - might be used when the system's data component has a consideral number of entities and complex relations on which very complex operations are performed

10. Transaction Managment and Concurrency Control
    a. What is a Transaction?
        - Transaction - any action that reads from or writes to a database
            - A simple SELECT statement to generate a list of table contents
            - A series of related UPDATE statements to change the values of attributes in various tables
            - A series of INSERT statements to add rows to one or more tables
            - A combination of SELECT, UPDATE, and INSERT statements
        - Consistent Database State - one in which all data integrity constraints are satisfied
        - Database Request - is the equivalent of a singel SQL statement in an application program or transaction
        - Transaction Properties
            - Atomicity - requires that all operations of a transaction be completed
            - Consistency - indicates the permanence of the database's conistent state, a transaction takes a database from one consistent state to another
            - Isolation - means that the data used during the execution of a transaction cannot be used by a second transaction until the first one is completed
            - Durability - ensures that once transaction changes are done and commited, they cannot be undone or lost, even in the event of a system failure
            - Serializability - ensures that the schedule for the concurrent execution of the transactions yields consistent results
        - Transaction Management with SQL
            - ANSI Standards
                - COMMIT - all changes are permanently recorded within the database
                - ROLLBACK - all changes are aborted and the databse is rolled back to its previous consistent state
                - End of program is reached, equivalent to COMMIT
                - Program is aborted and database is rolled back to its previous consistent state, equivalent to ROLLBACK
        - Transaction Log - keeps track of all transactions that update the database
            - Stores
                - A record for the beginning of the transaction
                - For each transaction component (SQL statement)
                    - The type of operation being performed (INSERT, UPDATE, DELETE)
                    - The names of the objects affected by the transaction (table name)
                    - The "before" and "after" values for the fields being updated
                    - Pointers to the previous and next transaction log entries for the same transaction
                - The ending COMMIT of the transaction
    b. Concurrency Control - coordinating the simultaneous execution of transactions in a multiuser database system
        - Objective - ensure the serializability of transactions in a multiuser database environment
        - Main Problems
            - Lost Updates
                - Occurs when two concurrent transactions, T1 and T2, are updating the same data element and one of the updates is lost (overwritten by the other transaction)
            - Uncommitted Data
                - Occurs when two transactions, T1 and T2, are executed concurrently and the first transaction is rolled back after the second transaction has already accessed the uncommitted data, thus violating the isolation property of transactions
            - Inconsistent Retrievals
                - Occurs when a transaction accesses data before and after one or more other transactions finish working with such data\
        - The Scheduler - special DBMS process that established the order in which the operations are executed within concurrent transactions
            - Actions based on concurrency control algorithsm such as locking or timestamping methods
            - Serializable schedule - in which the intervleaved exeuction of transations yields the same results as if the transaction were executed in serial order
    c. Concurreny Control with Locking Methods
        - Lock - guarantees exclusive use of a data item to a current transaction
            - Pessimistic Locking - use of locks based on the assumption that conflict between transactions is likely
            - Lock Manager - responsible for assigning and policiing the locks used by the transaactions
        - Lock Granularity - indicates the level of lock use.
            - Database Level - the entire database is locked, thus preventing the use of any tables in the databse by transaction T2 while T1 is being executed - good for batch, unsuitable for multisuer DBMS
            - Table Level - the entire table is locked, preventing access to any row by transaction T2 while T1 is using the table
            - Page Level - DBMS locks an entire diskpage
                - Diskpage or Page - equivalent of a disck block which is a directly addressable section of a disk
            - Row Level - DBMS allows concurrent transactions to access different rows of the same table evne when the rows are located on the same page
            - Field Level - allows concurrent transactions to access the same row as long as they require the use of different field (attributes) within that row
        - Lock Types
            - Binary Lock - two states, locked or unlocked
                - locked - no other transaction can use that object
                - unlocked - any transaction can lock that object for its use
            - Shared/Exclusive Locks
                - Exclusive - exists when access is reserved specifically for the transaction that locked the object
                - Shared - exists when concurrent transactions are grantd read access on the basis of a common lock
            - Two-Phase Locking to Ensure Serializability
                - Phase One - Growing Phase - a transaction acquires all required locks without unblocking any data
                - Phase Two - Shrinking Phase - a transaction releases all locks and cannot obtain a new lock
                - Rules
                    - Two transactions cannot have conflicting locks
                    - No unlock operation can preceded a lock operation in the same transaction
                    - No data are affected until all locks are obtained
            - Deadlocks - occurs when two transactions wait indefinitely for each other to unlock data
    d. Conccurency Control with Timestamping Methods
        - Timestamping - assigns a global, unqiue timestamp to each transaction which produces an explicit order in which transactions are submitted to the DBMS system
            - Uniqueness - ensures that no equal timestamp values can exists
            - Monotonicity - ensures that timestamp values always increase
        - Wait/Die and Wound/Wait Schemes - which transaction is rolled back and which continues executing
            - Wait/Die - older waits for the younger
                - If the transaction requesting the lock is the older of the two transactions, it will wait until the other transaction is completed and the locks are released
                - If the trasnsaction requesting the lock is the younger of the two transactions, it will die (roll back) and is rescheduled using the same timestamp
            - Wound/Wait - older transaction rolls back the younger transaction and reschedules it
    e. Concurrency Control with Optimistic Methods
        - Optimistic - based on the assumption that the majority of database operations do not conflict
            - Read Phase - the transaction reads the database, executes the needed computations, and makes the updates to a private copy of the database values. All update operations are recorded in a temporary update file, which is not accessed by the remaining transactions
            - Validation Phase - the transaction is validated to ensure that the changes made will not affect the integrity and consistency of the database. If positive, the transaction goes to the write phase, else, transaction is restared and changes are discarded
            - Write Phase - the changes are permanently applied to the database
    f. Database Recovery Management
        - Database Recovery - restores a database from a given state to a previously consistent state
            - Atomic Transaction Property - all portions of the transaction must be treated as a single, logical unit of work in which all operations are applied and completed to produce a consistent database
        - Critical Events
            - Hardware/Software Failure
            - Human Incidents
            - Natural Disasters
        - Transaction Recovery
            - Write-Ahead Log Protocol - ensures that transaction logs are always written before any database data are actually update, ensures in the case of ailure that the database can later be recovered to a consistent state using the data in the transaction log
            - Redundant Transaction Logs - ensure that a physical disk failure will not impair the DBMS's ability to recover data
            - Buffers - temporary storage areas in primary memory used to speed up disk operations
            - Checkpoints - operations in which the DBMS writes all of its updated buffers to disk
        - Deferred-Write Technique - the transaction oeprations do not immediately update the physical database
            - Recovery Process
                - Identify the last checkpoint in the transaction log
                - For a transaction that started and was commited before the last checkpoint, nothing needs to be done
                - For a transaction that performed a commit operation after the last checkpoint, the DBMS uses the transaction log records to redo the transaction and update the database
                - For any transaction that has a ROLLBACK operation after the last checkpoint, nothing needs to be done
        - Write-Through Technique - the database is immediately updated by the transaction operations during the transaction's execution
            - Recovery Phase
                - Identify the last checkpoint in the transaction log
                - For a transaction that started and was commited before the last checkpoint, nothing needs to be done
                - For a transaction that was committted after the last checkpoint, the DBMS redoes the transaction, applying changes from oldest to newest
                - For a transaction that had a ROLLBACK,  the DBMS uses the transaction log recrords to ROLLBACK and undo the operations, using the "before" values in the transaction log, applying changes from newest to oldest

11. Database Performance Tuning and Query Optimizations
    a. Database Performance Tuning Concepts
        - Interaction Sequence
            - Client application generates a query
            - Query is sent to the DBMS server
            - DBMS server executes the query
            - DBMS sends resulting data to client
        - Database Performance Tuning - referes to a set of actitivities and procedures desgined to reduce the response time of the database system
        - Mantra - No amount of fine-tuning will make a poorly designed datatbase perform as well as a well-designed database
        - Performance Tuning: Client and Server
            - SQL Performance Tuning - generate a SQL query that returns the correct answer in the least amount of time, using the minimum amount of resources at the server end
            - DBMS Performance Tuning - configured to respond to clients' requests in the fastest way possible
        - DBMS Architecture
            - Data File - where data is stored
            - Table Space/File GRoup - logical grouping of several data files that store data with similar characeristics
            - Data Cache/Buffer Cache - shared, reserved memory area that stores the most recently accessed data blocks in RAM
            - SQL Cache/Procedure Cache - shared, reserved memory area that stores the most recently executed SQL statements or PL/SQL procedures including triggers and functions
            - I/O Request - low level data access operation that reads or writes data to and from computer devices
            - Listener - listens for clients' requests and handles the processing of the SQL request to other DBMS processes
            - User - DBMS creates a user process to manage each client session
            - Scheduler - process organizes the concurrent execution of SQL requests
            - Lock Manager - manages all locks placed on database objects, including disk pages
            - Optimizer - analyzes SQL queries and finds the most efficient way to access the data
        - Database Query Optimization Modes
            - Selection of the optimum execution order
            - Selection of sites to be accessed to minimze communication costs
            - Operation Modes
                - Automatic Query Optimization - DBMS
                - Manual Query Optimization - user
            - Algorithms
                - Static Query Optimization - takes place at compilation time
                - Dynamic Query Optimization - takes place at execution time
            - Date Type
                - Statiscally Based Query Optimization Algorithm - uses statistical information about the database
                    - Dynamic Statistical Generation Mode - DBMS automatically evaluates and updates the statistics after each access
                    - Manual Statistcal Generation Mode - statistics must be updated periodically through a user selected utilitysuch as RUNSTAT
                - Rule Based Query Optimization - based on a set of user defined rules to determiune the best query access strategy
        - Database Statistics - number of measurements about database objects, such as number of processors used, processor speed, and temporary space available
    b. Quiery Processing
        - DBMS Query Process Phase
            - Parsing - DBMS parses the SQL query and chooses the most efficient access/execution plan
                - Query Optimizer - analyzes SQL query and finds the most efficient way to access the data
                - Parsing Steps
                    - Validated for syntax compliance
                    - Validated against the data dictionary to ensure that table  names and column names are correct
                    - Validated against the data dictionary to esnure that the user has the proper access rights
                    - Analyzed and decomposed into more atomic components
                    - Optimized through transformationm into a full equivalent but more efficient SQL query
                    - Prepared for execution by determing the most efficient execution or access plan
            - Execution - DBMS executes the SQL query using the chosen execution plan
            - Fetching - DBMS fetches the data and sends the result set back to the client
            - Query Processing Bottlenecks - delay introduced in the processing of an I/O operation that causes the overall system to slow down
                - Common Bottlenecks - CPU, RAM, Hard Disk, Network, Application Code
    c. Indexes and Query Optimization
        - Types
            - Data Sparsity - refers to the number of different values a column could have
            - Hash Index - based on an ordered list of hash values
            - B-Tree Index - ordered data structure organized as an upside-down tree
            - Bitmap Index - uses a bit array to represent the existence of a value or condition
                - Example One - lots of unique values that repeat a relatively small number of times, use B-tree
                - Example Two - small number of unique values that repeat a relatively large number of times, use bitmap
    d. Optimizer Choices
        - Modes
            - Rule-Based Optimizer - uses preset rules and points to determine the best approach to execute a query
            - Cost-Based Optimizer - uses sophisticated algorithms based on statistics about the objects being accessed to determine the best appraoch to execute a query
    e. SQL Performance Tuning
        - Index Selctivity
            - When an indexed column appears by itself in the search criteria of a WHERE or HAVING clause
            - When an indexed column appears by itself in a GROUP BY or ORDER BY clause
            - When a MAX or MIN function is applied to an indexed column
            - When the data sparsity on the indexed column is high
    f. Query Formulations
        - Identify what columns and computations are required
        - Identify the source tables
        - Determine how to join the tables
        - Determine what selection criteria are needed
        - Determine the order in which to display the output
    g. DBMS Performance Tuning
        - Configurable Parameters
            - Data Cache
            - SQL Cache
            - Sort Cache
            - Optimizer Mode
        - Recoomendations
            - Use RAID Redundant Array of Independent Disks
            - Minimize disk contentions
            - Put high usage tables in their own table space
            - Assign separate data files in separate storage volumes for the indexes, system, and high usage tables
            - Take advantage of the various table storage organizations available
            - Partition tables based on usage
            - Use denormalized tables where appropriate
            - Store computed and aggregate attributes in tables

12. Distributed Database Management Systems
    a. The Evolution of Distributed Database Management Systems
        - DDBMS - governs the storage and processing of logically related data over interconnected computer systems in which bot hdata and processing are distributed among several sites
        - DB Requirements
            - Rapid ad hoc data access
            - Distributed data access to support geograhically dispersed business units
        - DB Requirement Factors
            - The growing acceptance of the internet as a platform for data access and distribution
            - The mobile wireless revolution
            - The accelerated growth of companies using applications as a service
            - The increased focus on mobile business intelligence
        - Centralized DB Problems
            - Performance degradation because of a growing number of remote locations over greater distances
            - High costs associated with maintaining and operating large central mainframes
            - Reliability problem created by a dependence on a single site
            - Scalability problem associated withh physical limits imposed by a single location
            - Organizational rigidity imposed by the database
    b. DDBMS Advantages and Disadvantages
        - Advantages
            - Data are located near the site of greatest demand
            - Faster data access
            - Faster data processing
            - Growth facilitation
            - Improved communication
            - Reduced operating costs
            - User-friendly interface
            - Less danger of a single-point of failure
            - Processor independence
        - Disdavantages
            - Complexity of management and control
            - Technological difficulty
            - Security
            - Lack of standards
            - Increased storage and infrastructure requirements
            - Increased training cost
            - Costs
    c. Distributed Processing and Distributed Databases
        - Distributed Processing - database's logical processing is shared among two or more physically independent sites that are connected through a network
        - Distributed Database - store a logically related database over two or more phhysically independent sites
        - Database Fragments - multiple compose a database
    d. Characterisitcs of Distributed Database Management Systems
        - Application Interface
        - Validation to analyze data requests for syntax correctness
        - Transformation to decompose complex requests into atomic data request components
        - Query optimization to find the best access strategy
        - Mapping to determine the data location of local and remote fragments
        - I/O interface to read or write data from or to permanent local storage
        - Formatting to prepare data for presentation to the end user
        - Security to provide data privacy at bothh local and remote databases
        - Backup and recovery to ensure tehh availability and recoverability of the database in case of a failure
        - DB administration features for the DBA
        - Concurrency control to manage simultaneous data access and to esnure data consistency across database fragments
        - Transaction management
    e. DDBMS Componenets
        - Computer Workstations
        - Network Hahrdware and Software
        - Communications Media
        - Transaction Processor - requests data
        - Data Processor - stores and retrieves data located at the site
    f. Levels of Data and Process Diistribution
        - Single-Site Processing, Single-Site Data
        - Multiple-Site Processing, Single-Site Data
        - Multiple-Site Processing, Multiple-Site Data
    g. Distributed Database Transparency Features
        - Distribution Transparency - allows a distributd database to be treated as a single logical database
        - Transaction Transparency - allows a transaction to update data at more than one network site
        - Failure Transparency - ensures that the system will continue to operate in the event of a node or network failure
        - Performance Transparency - allows the system to perform as if it were a centralized DBMS
        - Heterogenity Transparency - allows the integration of several different local DBMSs under a common, or global, schhema
    h. Distribution Transparency
        - Fragmentation Transparency - the end user or programmer does not need to know that a database is partitioned
        - Location Transparency - the end user must specify the database fragment names but does not need to specify where those fragments are located
        - Local Mapping Transparency - exists when the end user must specify both the fragment name and location
    i. Transaction Transparency - emsures datanase transactions will maintain the distributed database's integirty and consistency
        - Distributed Requests amd Distributed Transactions
            - Remote Request - a single SQL statement access the data that are to be processed by a single remote database processor
            - Distributed Transaction - can reference several different local or remote DP sites
            - Distributed Request - lets a single SQL statement reference data located at several different local or remote DP sites
        - Distributed Concurrency Control
        - Two-Phase Commit Proposal - guarantees that if a portion of a transaction operation cannot be committed, all changes made at the othher sites will be undone to maintain a consistent database state
            - DO-UNDO-REDO Protocol
                - DO - performs the operation and records the "before" and "after" values in the transaction log
                - UNDO - reverses an operation, using the log entries written by the DO portion of the sequence
                - REDO - redoes an operation, using the log entries written by the DO portion of thhe sequence
            - Phase 1: Preparation
            - Pahse 2: The Final COMMIT
    j. Performance and Failure Transparency
        - Cost of Requests
            - Access Time, Communication Cost, CPU Time Cost
        - Important Points
            - Data Distribution, Data Replication, and Network and Node Availability
    k. Distributed Database Design
        - Data Fragmentation
            - Horizontal Fragmentation - split on entire rows
            - Vertical Fragmentation - split on attribute columns
            - Mixed Fragmentation - a combination of horizontal and vertical
        - Data Replication - storage of data copies at multiple sites serverd by a computer network
            - Push Replication - after data update, originating DP sends the changes to replica nodes to ensure data update immediately
            - Pull Replication - after data update, originating DP sends messages to replica nodes, who decide when to apply the updates
                - Replica Scenarios
                    - Fully replicated database - stores multiple copies of each database fragment at multiple sites
                    - Partially replicated database - stores multiple copies of some database fragments at multiple sites
                    - Unreplicated database - stores each database fragment at a single site
                - Replica Factors
                    - Database Size, Usage Frequency, Costs
        - Data Allocation
            - Centralized Data Allocation - entire database is stored at one site
            - Partitioned Data Allocation - database is divided into two or more disjoined parts and stored at two or more sites
            - Replicated Data Allocation - copies of one or more database fragments are stored at several sits
    l. The CAP Theorem
        - Consistency - all nodes should see the same data at the same time
        - Availability - a request is always fulfilled by the system
        - Partition Tolerance - system continues to operate even in the event of a node failure
        - BASE - Basically Available, Soft State, Eventually Consistent
    m. Date's 12 Commandments for Distributed Databases
        - Local Site Independence - each local site can act as an independent, autonomous, centralized DBMS
        - Central Site Independence - no site in the network relies on a central site or any other site
        - Failure Independence - system is not affected by node failures
        - Location Transparency - user does not need to know the location of data to retrieve those data
        - Fragmentation Transparency - fragmentation is transparent to the user, who sees only one logical database
        - Replication Transparency - DBMS transparently selects the database fragment to access, user only sees one logical database
        - Distributed Query Processing - query may be executed at several different DP sites
        - Distributed Transaction Processing - transaction may update data at several sites, and the transaction is executed transparently
        - Hardware Independence - system must run on any hardware platform
        - Operating System Independence - system must run on any operating system
        - Network Independence - system must run on any network platform
        - Database Independence - system must support any vendor's database product

13. Business Intelligence and Data Warehouses
    a. The Need for Data Analysis
    b. Business Intelligence - term that describes a comprehensive, cohesive, and integrated set of tools and processes used to capture, collect, integrate, store and analyze data withh the purpose of generating and presenting information to support business decision making
        - Framework To:
            - Collect and store operational data
            - Aggregrate operational data into decision support data
            - Analyze deciison support data to generate information
            - Present information to the end user to support business decisions
            - Make business decision, which in turn generate more data that works through the flow
            - Monitor results to evaluate outcomes of the business decisions
            - Predictr future behhaviors and outcomes withh a high degree of accuracy
        - Business Intelligence Architecture
            - ETL Tools - data extraction, transformation, and loading tools to collect, filter, integrate, and aggregrate internal and external data
            - Data Store - supported by data warehouse that stores data optimized for data analysis and query speed
            - Query and Reporting - performs data selection and retrieval
            - Data Visualization - presents data to teh end user in a variety of meaningful and innovative ways
            - Data Monitoring and Alerting - allow real-time monitoring of business activities
            - Data Analytics - performs data analysis and data-mining tasks using the data in the data store
            - Examples
                - Dashboards - Salesforce
                - Portals - Actuate
                - Data Analysis and Reporting - MicroStrategy
                - Data-Mining - SAP
                - Data Warehouse - Azure CosmosDB
                - OLAP Tools
                - Data Visualization - Tableau
        - Master Data Management - collection of concepts, techniques, and processes for thhe proper identification, definition, and management of data elements within an organization
        - Governance - method or process of government
        - Key Performance Indicators - quantifiable numeric or scale based measurements thhat assess the compnay's effectiveness or success in reach its strategic and operational goals
        - Reporting Styles
            - Advanced Reporting - insightful information
            - Monitoring and Alerting - monitor decision outcome
            - Advanced Data Analysis - help the end user discover relationships, patterns, and trends hhidden within the organization's data
        - Business Intelligence Benefits
            - Integrating Architecture
            - Common User Interface for Data Reporting and Analysis
            - Common Data Repository Fosters Single Version of Company Data
            - Improved Organizational Performance
        - Decision Support System - arrangment of computerized tools used to asssit managerial decision making
        - Business Technology Trends
            - Data Storage Improvements
            - Business Intelligence Applicances
            - Business Intelligence As A Service
            - Big Data Analytics
            - Personal Analytics
    c. Decision Support Data
        - Operational Data vs. Decision Support Data
            - Operational Data - optimized to support transactions thhat represent daily operations
            - Decision Support Data - gives tactical and strategic business meaning to the operational data
                - Time Span - cover a longer time frame
                - Granularity - presented from highly summarized to nearly atomic
            - Dimensionality - includs many dimensions and are interested in how the data relate over those dimensions
        - Decision Support Database Requirements
            - Database Schema
            - Data Extraction and Filtering
            - Database Size
    d. The Data Warehouse
        - Integrated - centralized, consolidated database thhat integrates data dervied from the entire organization and from multiple sources with diverse formats
            - All business entities, data elements, data characteristics, and business metrics are described in the same way throughout the enterprise
        - Subject-Oriented - arranged and optimized to provide answers to questions from diverse functional areas within a company
            - Organized and sumarized by topic, such as sales, marketing, finance, distribution, and transportation
        - Time-Variant - represent the flow of data through time
        - Non-Volatile - once data enter the warehouse, they are never removed
        - Data Mart - small, single subject data warehouse subset that provides decision support to a small group of people
        - Twelve Rules
            - The data warehouse and operational environments are separated
            - The data warehouse data are integrated
            - The data warehouse contains historical data over a long time
            - The date warehouse data are snapshot data captured at a given point in time
            - The data warehouse data are subject oriented
            - The data warehouse are mainly read-only with periodic batchh updates from operational data
            - The data warehouse development lifecycle is data driven instead of process driven
            - The data warehouse contains data with several levels of detail, current, old, lightly summarized and highly summarized
            - The data warehouse is characterized by read-only transactions to very large data sets
            - The data warehouse has a system that traces data sources, transformations, and storage
            - The data warehouse meta data identify and define all data elements
            - The data warehouse contains a chargeback mechanism for resource usage that enforces optimal use of the data by end users
    e. Star Schemas - modeling technique used to map multidimensional support data into a relational database
        - Facts - numeric measurements that represent a specific business aspect or activity
        - Dimensions - qualifying characteristics that provide additional perspectives to a given fact
        - Attributes - used to search, filter, or classify facts
        - Attribute Hierarchy - provides a top-down data organization that is used for two purposes: aggregation, and drill-down/roll-up data analysis
            - Example: Region -> State -> City -> Store
        - Performance Improving Techniques for the Star Schema
            - Normalizing Dimensional Tables
            - Maintaining Multiple Fact Tables to Represent Different Aggregation Levels
            - Denormalizing Fact Tables
            - Partitioning and Replicating Tables
    f. Data Analytics - subset of BI functionality that encompasses a wide range of mathematical, statistical, and modeling techniques with the purpose of extracting knowledge from data
        - Explanatory Analytics - focus on discovering and explaining data characteristics and relationships based on existing data
        - Predictive Analytics - focus on predicting future data outcomes with a high degree of accuracy
        - Data Mining - refers to analyzing massive amounts of data to uncover hidden trends, patterns, and relationships to form computer models to simulate and explain the findings and then to use such models to support business decision making
            - Data Preparation Phase - main data sets to be used are identified and cleansed of any data impurities
            - Data Analysis And Classification Phase - studies the data to identify common data characteristics or patterns
                - Data Groupings, classifications, clusters, or sequences
                - Data dependencies, links, or relationships
                - Data patterns, trends, and deviations
            - Knowledge Acquisition Phase - uses thhe result of data analysiss and classification phase
                - Neural Networks, Decision Trees, Rules Induction, Genetic Algorithms, Regression Trees, Memory-Based Reasoning, and Nearest Neighbor
            - Prognosis Phase - modeling, forecasting and prediction
        - Predictive Analytics - use of advanced mathematical, statistical, and modeling tools to predict future business outcomes with high degrees of accuracy
    g. Online Analytic Processing
        - Multi-Dimensional Data Analysis Techniques - data are processed and viewed as part of a multidimensional structure
        - Advanced Database Support/Features
            - Access to many different kinds of DBMSs, flat files, and internal and external data sources
            - Access to aggregrated data warehouse data as well as tothe detail data found in operational databases
            - Advanced data navigation features such as drill-down and roll-up
            - Rapid and consisteny query response times
            - The ability to map end-user requests to the appropriate data source and then to the proper data access language
            - Support for very large databases
        - Easy-to-Use End Usert Interface
        - OLAP Architecture
            - Graphical User Interface
            - Analytical Processing Logic
            - Data Processing Logic
        - Relational OLAP - provides OLAP functionality by using relational databases and familiar relational query tools to store and analyze multi-dimensional data
            - Multidimensionl data scheme support within the RDBMS
            - Data access language and query performance optimized for multidimensional data
            - Support for very large databases
        - Multidimensional OLAP - extends OLAP functionality to multi-dimensional database management systems
            - Data Cube - three-dimensional visualization where the location of eachh data value is a function of x-, y-, and z- axes in a three dimensional space
    h. SQL Extensions for OLAP
        - The ROLLUP Extentsion - use with GROUP BY clause to generate aggregates by different dimensions
        - The CUBE Extension - used withh GROUP BY clause to generate aggregates by the listed columns
        - Materialized Views - dynamic table that not only contains the SQL query command to generate the rows, it stores the actual rows

14. Database Connectivity and Web Technologies
    a. Database Connectivity - the mechanisms thhroughh which application programs connect and communicate with data repositories
        - Database Middleware - provides an interface between the application program and the database
        - Native SQL Connectivity - the connection interface that is provided by the database vendor and that is unique to that vendor e.g. Oracle SQL*Net or MSSQL Server
            - ODBC, DAO, and RDO
                - ODBC - Open Database Connectivity - MS implementation of the SQL Access Group Call Level Interface
                - Data Access Objects DAO - object oriented API used to access MS Access, MS FoxPro and dBase
                - Remote Data Objects RDO - higher-level object-oriented appliction used to access remote database servers
                - Dynamic Link Libraries - shared code that is linked to theh Windows operating environment
                - ODBC Architechture
                    - ODBC API
                    - Driver Manager - in charge of managing all database connections
                    - ODBC Driver that communicates directly with DBMS
            - OLE-DB - Object Linking and Embedding for Database - adds object oriented functionality for access to relational and nonrelational data
                - Consumers are objects that request and use data
                - Producers are objects that manage the connection with a data source and provide data to the consunmers
                    - Data Providers - provide data to other processes
                    - Service Providers - provide additional functionality to consumers
                - ActiveX Data Objects - high-level application orientd interface to interact withh OLD-DB, DAO, and RDO
            - ADO.NET - data access component of Microsoft's .NET application development framework
            - Java Database Connectivity JDBC - application programming interface that allows a Java program to interact with a wide range of data sources including relational databases, tabular data sources, spreadsheets, and text files
    b. Database Internet Connectivity
        - Enables
            - Rapid response to competitive pressures by bringing new services and products to market quickly
            - Increase customer satisffaction through the creation of Web-based support services
            - Allow anywhere, anytime data access using mobile smart devices via the internet
            - Yield fast and effective information dissemination through universal access from across the street or across the globe
        - Web to Database Middleware: Server-Side Extensions
        - Web Server Interfaces
            - Common Gateway Interface - uses script files that perform specific functions baxed on the client's parameters that are passed to the web server
            - Application Programming Interface
        - Web Browser
        - Client Side Extensions
            - Plug-ins - external application that is automatically invoked by the browser when needed
            - JavaScript, ActiveX, VBSCript
        - Web Application Servers - middleware application that expands the functionality of Web servers by linking thhemn to a wide range of services, such as databases, directory systems, and search engines
        - Web Database Development - deals with the process of interfacing databases with the Web browser
    c. Extensible Markup Language - metalanguage used to represent and manipulate data elements
        - Document Type Definition and XML Schemas
            - Document Type Definition - file that describes XML elements, composition of the database's logical model and defines the syntax rules or valid elements for each type of XML document
            - XML Schema - advanced data definition language that is used to describe the structure of XML data documents
                - Elements, data types, relationship types, ranges, and default values
        - XML Presentation 
            - Extensible Style Language Transformations XSLT - describes the general mechanism that is used to extract and process data from one XML document and enables its transformation within another document
            - XSL Stylesheets - define the presentation rules applied to XML elements
        - XML Applications
            - Business-to-Business Exchanges
            - Legacy System Integrations
            - Web Page Development
            - Database Support
            - Database Metadictionaries
            - XML Databases
            - XML Services
    d. Cloud Computing Services
        - Cloud Computing - a computing model for enabling ubiquitous, convenient, on-demand, network access to a shared pool of configurable computer resources e.g. networks, servers, storage, applications, and services
        - Cloud Services - services provided by cloud computing
        - Cloud Implementation Types
            - Public - managed exclusively by third party
            - Private - built by an organization for the sole purpose of servicing its own needs
            - Hybrid - mix of public and private
        - Characteristics of Cloud Services
            - Ubiquitous access via internet technologies
            - Shared infrastructure
            - Lower costs and variable pricing
            - Flexible and scalable services
            - Dynamic provisioning
            - Service orientation
            - Managed operations
        - Types of Cloud Services
            - Software as a Service - Google Docs
            - Platform as a Service - Azure App Service
            - Infrastructure as a Service - VM
        - Cloud Advantages
            - Low initial cost of entry
            - Scalability and elasticity
            - Support for mobile computing
            - Ubiquitous access
            - High reliability and performance
            - Fast provisioning
            - Managed infrastructure
        - Cloud Disadvantages
            - Issues of security, privacy and compliance
            - Hidden costs of implementation and operation
            - Data migration is difficult and lengthy
            - Complex licensing scheme
            - Loss of ownership and control
            - Organization culture
            - Difficult integration with internal IT system
        - SQL Data Services - cloud computing based data management service that provides relational data storage, access, and management to companies of all sizes without the typically high costs of in-house
            - Hosted Data Management
            - Standard Protocols
            - Common Programming Interface
            
15. Database Administration and Security
    a. Data as a Corporate Asset
        - Dirty Data - data that suffers from inaccuracies and inconsistencies, becomes an even greater threat
            - Lack of enforcement of integrity constraints, such has not null, uniqueness, and referential integrity
            - Data-entry errors and typographical errors
            - Use of synonyms and homonyms across systems
            - Nonstandard use of abbreviations in character data
            - Different decompositions of composite attributes into simple attributes across systems
        - Data Quality - comprehhensive approach to ensuring the accuracy, validity, and timeliness of data
        - Data Profiling Software - gathers statistics, analyzes existing data sources and metadata to determine data patterns
        - Master Data Management Software - coordinating common data acorss multiple systems
    b. The Need for a Database and Its Role in the Organization
        - DBMS Facilitate
            - Interpretation and presentation of data
            - Distribution of data
            - Data preservation and monitoring
            - Control over data duplication
        - Management Levels
            - Top - needs information for strategic decision making
            - Middle - needs information for tactical decision making
            - Operational - needs information for every day decision making
        - Enterprise Database - company's data representation that provide support for all present and expected future operations
    c. Introduction of a Database: Special Considerations
        - Technological - DBMS software and hardware - installing, configuring, and monitoring the DBMS to make sure it efficiently handles data storage, access and security
        - Managerial - Administrative Functions
        - Cultural - Corporate resistance to change - effect on people, functions, and interactions
    d. The Evolution of Database Administration
        - Information Systems Department
            - A service function to provide end users with datamanagement suppor 
            - A production function to provide end users with solutions for their information needs through intergated application or management information systems
        - Database Administrator - responsible for the control of the centralized and shared databases
            - Database planning, including the definition of standards, procedures, and enforcement
            - Database requirements gathering and conceptual design
            - Database logical and transaction design
            - Database physical design and implementation
            - Database testing and debugging
            - Database operations and maintenance, including installation, conversion, and migration
        - Data Administrator/Information Resource Manager
    e. Database Environment's Human Component
        - Data Adminstrator Activities and Characteristics
            - Performs strategic planning
            - Sets long-term goals
            - Sets policies and standards
            - Job is broad in scope
            - Focuses on the long term
            - Has a managerial orientation
            - Is DBMS independent
        - Database Administrator
            - Controls and supervises
            - Executes plans to reach goals
            - Enforces policies and procedures
            - Enforces programming standards
            - Job is narrow in scope
            - Focuses on the short term
            - Has a technical orientation
            - Is DBMS specific
        - DBA Managerial Role
            - End User Support
            - Policies, Standards, and Procedures
            - Data Security, Privacy and Integrity
            - Data Backup and Recovery
                - Tasks
                    - Periodic data and application backup
                    - Proper backup and identification
                    - Convenient and safe backup storage
                    - Physical protection of both hardware and software
                    - Personal access control to the software of a database installation
                    - Insurance coverage for the data in the database
                - Disaster Management - all of the DBA activites designed to secure data availability following a physical disaster or database integrity failure
                - Full Backup - produces a complete copy of the database
                - Incremental Backup - produces a backup of all data since the last backup date
                - Concurrent Backup - takes place while the user is working on a database
            - Data Distribution and Use
        - DBA Technical Role
            - Evaluating, selecting, and installing the DBMS and related utilities
                - DBMS Model
                - DBMS Storage Capacity
                - Application Development Support
                - Security and Integrity
                - Backup and Recovery
                - Concurrency Control
                - Performance
                - Database Administration Tools
                - Interoperability and Data Distribution
                - Portability and Standards
                - Hardware
                - Data Dictionary
                - Vendor Training and Support
                - Available Third Party Tools
                - Costs
            - Designing and implementing databases and applications
            - Testing and evaluating databases and applications
            - Operating theh DBMS, utilities, and applications
            - Training and supporting users
            - Maintaining the DBMS, utilities, and applications
        - DBA's Role in the Cloud
            - DBMS Installation and Updates
            - Server/Network Engagement
            - Backup and Recovery Operations
    f. Security
        - Key Ideas
            - Security - refers to activities and measures that ensure the confidentiality, integrity, and availability of an information system and its main asset, data
            - Confidentiality - deals with ensuring that data are protected against unauthorized access and use
            - Integrity - keeping data consistent and free of errors or anomalies
            - Availability - accessibility of data whenever required by authorized users and for authorized uses
        - Security Policies - collection of standards, policies, and procedures created to guarantee the security of a system and ensure auditing and compliance
        - Security Vulnerabilities - weakness in a system componnet that could be exploited to allow unauthorized access or cause service disruptions
            - Security Threat - imminent security violation
            - Security Breach - occurs when a security threat is exploited to endanger the integrity, confidentiality, or availability of the sytem
                - Preserverd - data recovery is not necessary
                - Corrupted - data recovery is necessarys
        - Database Security - refers to DBMS features and other related measures that comply with the organization's security requirements
            - Safeguards
                - Change default system passwords
                - Change default installation paths
                - Apply the latest patches
                - Secure installation folders with proper access rights
                - Make sure that only required services are running
                - Set up auditing logs
                - Set up session logging
                - Require session encryption
            - Authorization Management
                - Define each user to the database
                - Assign passwords to each user
                - Define user groups
                - Assign access privileges
                - View Definition
                - DBMS Access Control
                - DBMS Usage Monitoring
    g. Database Administration Tools
        - Data Dictionary - stores the definitions of data characteristics and relationships
            - Active Data Dictionary - automatically updated with every database access
            - Passive Data Dictionary - not automatically updated
        - CASE Tools - computer aided systems engineering
    h. Developing a Data Adminstration Strategy
        - Information Engineering - allows for the translation of the company's strategic goals into the data and applications that will help the company achieve those goals
        - Information Systems Architecture - servers as the basis for planning, development, and control of future information systems
        - Success Factors
            - Management Commitment
            - Thorough Analysis
            - End-user Involvement
            - Standards
            - Training
            - Pilot