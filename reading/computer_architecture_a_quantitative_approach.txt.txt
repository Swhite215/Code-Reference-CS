Title: Computer Architecture: A Quantitative Approach
Authors: John L. Hennessy and David A. Patterson

Summarization of Content

1. Chapter 1 - Fundamentals of Quantitative Design and Analysis
    a. Classes of Computers
        - Personal/Mobile Devices PMD
        - Desktop
        - Server
        - Warehouse
        - IoT/Embedded
    b. Parallelism
        - Data-Level Parallelism
            - Instruction Level Parallelism
            - Vector Architectures, Graphic Processor Units, and Multimedia Instruction Sets
            - Thread Level Parallelism
        - Task-Level Parallelism 
            - Thread Level Parallelism
            - Request Level Parallelism
        - Categories
            - Single Instruction, Single Data Stream SISD
            - Single Instruction, Multiple Data Stream SIMD
            - Multiple Instruction, Singel Data Stream MISD
            - Multiple Instruction, Multiple Data Stream MIMD
    c. Instruction Set Architecture Dimensions
        - Class - General Purpose Registers
        - Memory Addressing
        - Addresing Modes
        - Types and Sizes of Operands
        - Operations
        - Control Flow Instructions
        - Encoding an ISA
    d. Trends in Technology
        - Integrated Circuit Technology
        - Semiconductor DRAM
        - Semiconductor Flash
        - Magnetic Disks
    e. Performance Trends
        - Bandwidth or Throughput
        - Latency or Response Time
    f. Dependability
        - Mean Time to Failure
        - Mean Time Between Failures
        - Mean Time to Recover
    g. Benchmarks
        - Electronic Design News Embedded Microprocessor Benchmark Consortium
        - Standard Performance Evaluation Corporation
        - Transaction Processing Council
    h. Quantitative Principles of Computer Design
        - Parallelism
        - Locality
        - Common Case
        - Amdahl's Law
        - Processor Performance Equation
    i. Fallacies and Pitfalls
        - P: All exponential laws must come to an end.
        - F: Multiprocessors are a silver bullet.
        - P: Falling prey to Amdahl's Law.
        - P: A single point of failure.
        - F: Hardware enhancements that increase performance also improve energy efficiency, or at worst are energy neutral.
        - F: Benchmarks remain valid indefinitely.
        - F: Disks never fail.
        - F: Peak performance tracks observed performance.

2. Chapter 2 - Memory Hierarchy Design
    a. Memory Technology
        - Primary Memory
            - L1, L2, L3 Cache
        - Secondary Memory
            - HDD
            - SDD
            - Optical Drives CD/DVD
        - Caches Misses
            - Compulsory, Capacity, Conflict
        - Six Basic Cache Optimizations
            - Larger Block Size to Reduce Miss Rate
            - Bigger Caches to Reduce Miss Rate
            - Higher Associativity to Reduce Miss Rate
            - Multilevel Caches to Reduce Miss Penalty
            - Priority to Read Misses Over Writes to Reduce Miss Penalty
            - Avoiding Address Translation During Indexing of Cache to Reduce Hit Time
    b. Memory Technology and Optimizations
        - RAM
            - Dynamic RAM - DRAM
            - Static RAM - SRAM
            - Synchronous DRAM - SDRAM
            - Double Data Rate - DDR
            - Graphics SDRAM - GSDRAM
        - Flash Memory
            - EEPROM
        - Phase-Change Memory Technology
    c. Ten Advanced Optimizations of Cache Performance
        - Categories
            - Reducing the Hit Time
                - Small and Simple First Level Caches
                - Way Prediction
            - Increasing Cache Bandwidthq
                - Pipelined and Multibanked Caches
                - Non-Blocking Caches
            - Reducing the Miss Penalty
                - Critical Word First and Early Restart
                - Merging Write Buffer
            - Reducing the Miss Rate
                - Compiler Optimizations
            - Reducing the Miss Penalty or Miss Rate Via Parallelism
                - Hardware Prefetching
                - Compile Prefetching
            - Miscellaneous
                - High-Bandwidth Memory 
    d. Virtual Memory and Virtual Machines
        - Virtual Machine Monitor
            - Provides an environment for programs which is essentially identical with the original machine
            - Programs run in this environment show at worst only minor decreases in speed
            - VMM is in complete control of system resources
        - Protection via Virtual Memory
            - Kernel and User Mode
            - User Process Use w/ supervisor mode bit, exception enable/disable bit, and memory protection information
            - Mechanism to go to supervisor mode and back to user mode
            - Mechanisms to limit memory access without having to swap the process on disk on a context switch
        - Protection Via Virtual Machines
            - Increase importance of isolation and security
            - Failures in security and reliability of standard operating systems
            - Sharing of a single computer among unrelated users
            - Dramatic increases in the raw speed of processors
        - VM
            - All emulation methods that provide a standard software interface
            - Interest: VMs that provide a complete system-level environment at the binary instruction set architecture level
        - VM Benefits
            - Managing Software and Hardware
        - VMM Requirements
            - Guest hardware should behave on a VM exactly as if it were running on the native hardware
            - Guest software should not be able to directly change allocation of real system resources
    e. Pitfalls and Fallacies
        - F: Predicting Cache Performance of One Program from Another
        - P: Simulating Enough Instructions to Get Accurate Performance Measure of the Memory Hierarchy
        - P: Not Delivering High Memory Bandwidth in a Cache-Based System
        - P: Implementing a Virtual Machine Monitor on an Instruction Set Architecture that was not designed to be virtualizable
        
3. Chapter 3 - Instruction Level Parallelism
    a. Instruction Level Parallelism: Concepts and Challenges
        - Metric - Cycles Per Instruction
            - Pipeline CPI = Ideal Pipeline CPI + Structural Stalls + Data Hazard Stalls + Control Stalls
        - Approaches
            - Hardware to Exploit ILP Dynamically
            - Software to Exploit ILP Statically at Compile Time
        - Dependences (Data, Name, Control)
            - Data Dependences: Instruction is dependent on result or of an instruction dependent on another
            - Name Dependences: Two instructions use the same register or memory location
            - Control Dependences: Instruction should be executed in correct order
        - Data Hazards
            - WAW Write After Write
            - RAW Read After Write
            - WAR Write After Read
    b. Basic Compiler Techniques for Exposing ILP
        - Pipeline Scheduling and Loop Unrolling
            - Possible if all of a loops iteration are independent
    c. Reducing Branch Costs with Advanced Branch Prediction
        - Correlating Branch Predictors - Look at recent behavior of other branches
        - Tournament Predictors - Use of Global (most recent branch history) and Local Predictor (address of branch)
        - Tagged Hybrid Predictors - Prediction tables with varying lengths and tags
    d. Overcoming Data Hazards with Dynamic Scheduling
        - Hardwre reorders the instruction execution to reduce the stalls while maintaing data flow and exception behavior
        - Advantages: 
            - Code compiled with one pipeline in mind to run efficiently on a different pipeline
            - Enables handling some cases where dependences are unknown at compile time
            - Allows the processor to tolerate unpredictable delays such as cache misses
        - Out-Of-Order - Execution as ssoon as instruction and data operands are available
        - Tomasulo's Approach
            - Issue - Get instruction FIFO order, issue instruction to reservation station, rename registers (Avoids WAW and WAR)
            - Execute - Place operands into any awaiting reservation station, execute operation when ready (Avoids RAW)
            - Write Result - Write result on Common Data Bus to registers and into any reservation stations awaiting results, stores are buffered until value and address are available
    e. Dynamic Scheduling: Examples and Algorithms
        - Tomasulo's Advantages
            - Distribution of Hazard Detection Logic and Elimination fo Stalls for WAR and WAW hazards
    f. Hardware-Based Speculation
        - Speculation - To address control dependences - fetch, issue, and execute
        - Key Ideas
            - Dynamic Branch Prediction
            - Speculation - to allow the execution of instructions before control dependences are resolved
            - Dynamic Scheduling
        - Steps in Instruciton Execution
            - Issue - Get an instruction from queue
            - Execute - Execute when both operands are available
            - Write Result - When available, write result to CDB and then into Read Only Buffer and any reservation stations
            - Commit
    g. Exploiting ILP Using Multiple-Issue and Static Scheduling
        - Flavors
            - Statically Scheduled Superscalar Processors
            - Very Long Instruction Word Processors
            - Dynamically Scheduled Superscalar Processors
        - Goal - Decrease CPI below one
    h. Exploiting ILP Using Dynamic Scheduling, Multiple Issue, and Speculation
        - Use all three
    i. Advanced Techniques for Instruction Delivery and Speculation
        - Goal - High-bandwidth instruction stream (4-8) instructions every clock cycle
        - Branch Target Buffers
            - Branch-Prediction cache that stores the predicted address for the next instruction after a branch
        - Integrated Instruction Fetch Unit
            -  Feeds instructions to the pipeline
            -  Integrated Branch Prediction
            -  Instruction Prefetch
            -  Instruction Memory Access and Buffering
        - Speculation Implementation
            - Support - Register Renaming Vs. Reorder Buffers
            - Issue - Speculation has a cost
            - Inefficiency - Unused speculation results and undoing speculation
    j. Cross-Cutting Issues
        - Hardware vs. Software Speculation
    k. Multithreading: Exploiting TLP
        - Hardware Approaches
            - Fine-grained - switch between threads on each cycle
            - Coarse-grained - only switches threads on costly stalls
            - Simulaneous Multithreading - Allows multiple instructions from independent threads to be executed without regard to the depedencesm which are handled by the dynamic scheduling capability
    l. Fallacies and Pitfalls
        - F: It is easy to predict the performance and energy efficiency of two different versions of the same instruction set architecture, if we hold the technology constant
        - F: Processors with lower CPIs will always be faster
        - F: Processors with faster clock rates will always be faster
        - P: Sometimes bigger and dumber is better
        - P: And sometimes smarter is bettern than bigger and dumber
        - P: Believing that there are large amounts of ILP available, if only we had the right techniques


4. Chapter 4 - Data-Level Parallelism
    a. Introduction
        - Data-Level Parallelism
            - Scientific Computing
            - Media-Oriented Image and Sound Processing
            - Machine Learning
        - Variations of Single Instruction Multiple Data
            - Vector Architectures
            - SIMD Multimedia Instruction Set Extensions
            - Graphical Processing Units
    b. Vector Architecture
        - Order
            - Grab sets of data elements scattered across memory
            - Place them into large sequentail register files
            - Operate on data in the register files
            - Disperse the results back into memory
        - RV64V
            - Components
                - Vector Registers - holds a single vector
                - Vector Functional Units - pipelined with control unit to detect structural and data hazards
                - Vector Load/Store Unit - loads or stores a vector to or from memory
                - Scalar Registers - can also provide data as input to vector functional units
            - Unique Vector Instruction Set
        - How Vector Processes Work
            - Single-Precisison A * X + Y SAXPY
            - Double-Precisions A * X + Y DAXPY
        - Vector Execution Time
            - Dependencies
                - Length of operand vectors
                - Structural hazards among operations
                - Data dependences
            - Vocabulary
                - Convoy - set of vector instructions that could potentially execute together
                - Chime - unit of time taken to execute one convoy
                - Flexible Chaining - allows a vector instruction to chain to eseentially any other vector instruction, assuming we dont generate a structural hazard
            - Optimizations
                - Multiple Lanes: Beyond One Element per Clock Cycle
                - Vector-Length Registers: Handling Loops Not Equal to 32
                - Predicate Registers: Hanlding IF Statements in Vector Loops
                - Memory Banks: Supplying Bandwidth for Vector Load/Store Units
                - Gather Scatter: Handling Sparse Matrices in Vector Architectures
    c. SIMD Instruction Set Extensions for Multimedia
        - Goal: Taks with data types lesss than 32-bit
        - Omissions
            - No Vector Length Register
            - No Strided or Gather/Scatter Data Transfer Instructions
            - No Mask Registers
        - Popularity
            - Cost
            - Require little extra processor state 
            - Require less memory bandwidth
            - Dont deal with virtual memory issues from a single instructions 32 memory access
            - Cache concern with vector architectures
        - Programming Multimedia SIMD Architectures
            - Libraries or Assembly Language
        - Roofline Visual Performance Model
            - Arithmetic Intensity - ratio of FP operations per byte of memory accessed
            - Floating Point Performance - FLOPS - Floating Point Operations per Second
    d. Graphics Processing Units
        - Enables
            - Multithreading, MIMD, SIMD, and Instruction-Level
        - Programming the GPU
            - CUDA - Compute Unified Device Architecture
            - CUDA Thread - Lowest Level of Parallelism
            - Thread Block - Grouped Threads
            - Multithreaded SIMD Processor - Exeutes Thread Blocks
            - GPU - Handles Parallel Execution and Thread Management
        - NVIDIA/CUDA Terms
            - Grid - code that runs on a GPU that consists of a set of Thread Blocks
            - Thread Block Scheduler - Assigns Thread Blocks to Processors
            - SIMD Lanes - Parallel Funtional Units to Perform Operations
        - NVIDIA GPU Instruction Set Architecture
            - PTX - Parallel Thread Execution
            - PTX Format - opcode.type destination, a source, b source, c source
        - Conditional Branching in GPUs
            - PX Assembler Level - branch, call, return, exit, and per-thread-lane predicate registers
            - GPU Hardware Instruction Level - branch, jump, jump indexed, call, call indexed, return, exit, and special instructions
        - NVIDIA GPU Memory Structures
            - Private Memory per SI<D Lane
            - Local Memory per Multithreaded SMIMD Processesor
            - GPU Memory shared by the whole GPU and all Thread Blocks that is off-chip DRAM
        - Innovations in the Pascal GPU Architecture
            - Fast single-precision, double-precisions, and half-precision floating point arithmetic
            - High-bandwidth memory
            - High-speed chip-to-chip interconnect
            - Unified virtual memory and paging support
    e. Detecting and Enhancing Loop-Level Parallelism
        - Goal: Discovering the amount of parallelism that we can exploit in a program as well as hardware support
        - Loop-Carried Dependence - Data accesses in later iterations are dependent on data values produced in earlier iterations
        - Affine - A one-dimensional array index is affine if it can be written in the form a * i + b
    f. Cross-Cutting Issues
        - Energy and DLP: Slow and Wide Versus Fast and Narrow
            - Goal: Lower the voltage while dropping the clock rate and increasing execution resources
        - Banked Memory and Graphics Memory
            - High-bandwidth Memory - Stacked DRAMS
    g. Fallacies and Pitfalls
        - F: GPUs suffer from being coprocessors
        - P: Concentrating on peak performance in vector architectures and ignoring start-up overhead
        - P: Incresing vector performance, without comparable increases in scalar performance
        - F: You can get good vector performance without providing memory bandwidth
        - F: On GPUs, just add more threads if you don't have enough memory performance

5. Chapter 5 - Thread Level Parallelism
    a. Introduction
        - TLP achieved with multiple program counters and MIMDs
        - Multiprocessors - computers consisting of tigthly coupled processors whose coordination and usage are typically controlled by a single operating system and that share memory
        - Software Models
            - Parallel Processing - tighlty coupled set of threads collaborating on a single task
            - Request-Level Parallelism - execution of multiple, relatively independent processes that may originate from one or more users
        - Multiprocessor Architecture: Issues and Approaches
            - Symmetric (Shared Memory) Multiprocessors
                - Non-Uniform Cache Access NUCA
                - Uniform Memory Access UMA
            - Distributed Shared Memory
                - Non-Uniform Memory Access NUMA
        - Challenges of Parallel Processing
            - Limited Parallelism in Program
            - High Cost of Communication - Latency of Remote Access
    b. Centralized Shared-Memory Architectures
        - Use large multi-level cache to reduce memory bandwidth demands of a processor
        - Private Data - used by only one processor
        - Shared Data - used by multiple processors
        - Multiprocessor Cache Coherence
            - Processors could view different values for the same memory location if shared between them
            - Coherence - defines what values can be returned by a read
            - Consistency - determines when a written value will be returned by a read
        - Basic Schemes for Enforcing Coherence
            - Migration - moving data item to a local cache
            - Replication - duplicate of data taht can be read simultaneously
            - Cache Coherence Protocols - protocols to maintain coherence (Directory or Snooping)
        - Snooping Coherence Protocols
            - Write Invalidate Protocol - processor has exclusive access to a data item
            - Write Broadcast/Update Protocol - update all cached copies of a data item when that item is written
        - Basic Implementation Techniques
            - Use the bus to perform invalidates and have controllers listening who perform the necessary changes to a cached data item
            - States: Invalid, Shared, or Modified
        - Extensions to the Basic Coherence Protocol
            - Modified, Exclusive, Shared, or Invalid MESI
            - Modified, Owned, Exclusive, Shared or Invalid MOESI
        - Limitation in Symmetric Shared Memory Multiprocessors and Snooping Protocols
            - Snooping Bandwidth
            - Increasing Snooping Bandwidth
                - Duplicate Tags
                - Shared Outermost Cache
                - Directory at Outermost Cache
        - Implementing Snooping Cache Coherence
            - Issue - write and upgrade misses are not atomic
    c. Performance of Symmetric Shared-Mmeory Multiprocessors
        - Uniprocessor Miss Rate - capcaity, compulsory, and conflict
        - Coherence Misses
            - True Sharing Misses - arise from communication of data through the cache coherence mechanism
            - False Sharing - occurs when a block is invalidated and a subsequent reference causes a miss because some word in the block is written into
        - Commercial Workload
            - Online Transaction-Processing by TPC-B
            - Measure time spent Instruction, Capacity/Conflict, Compulsory, False Sharing and True Sharing
        - A Multiprogramming and OS Workload
            - Phases - Compiling the Benchmarks, Installing Object Files in Library, and Removing the Object Files
            - measure time spent User Execution, Kernel Execution, Synchronization Wait, Processor Idle
    d. Distributed Shared-Memory and Directory-Based Coherence
        - Goal: Keep the state of every block that may be cached replicated and distributed across processors
        - The Basics
            - States - Shared, Uncached, Modified
            - Message Types - Read Miss, Write Miss, Invalidate, Fetch, Fetch/Invalidate, Data Value Reply, Data Write-Back
            - Local Node - Where request originates
            - Home Node - Where memory location and directory entry of an address reside
            - Remote Node - Where copies exist
        - Example Directory Protocol
            - Cache Blocks transitions between Invalid, Shared, and Modified
            - Directory transitions between Uncached, Shared, and Exclusive
            - Directory Requests - Read miss, Write Miss and Data Write-Back
    e. Synchronization: The Basics
        - User-level software routings that rely on hardware-supplied synchronization instructions
        - Goal: Enable atomicity
        - Basic Hardware Primitives
            - Atomic Exchange - interchanges a value in a register for a value in memor
            - Test-And-Set - tests a value and sets it if the value passes the test
            - Fetch-And-Increment - retursnt he value of a memory location and atomically increments it
            - RISC-V Load Reserved and Store Conditional - load the contents onto memory address and then store the value or values that can be used to check if atomic operation occurred
        - Implementing Locks Using Coherence
            - Spin Locks - locks that a processor continously tries to acquire, spinning around a loop until it succeeds
    f. Models of Memory Consistency: An Introduction
        - Sequential Consistency - requires that the result of any exeuction be the sa,e as though the memory accesses executed by each processor were kept in order and the access among different processors were arbitrarily interleaved
        - Programmer's View
            - Synchronzied - if all accesses to shared data are ordered by synchronization operations
            - Data-Race-Free - no cases where variable may be updated without ordering by synchronization causing execution outcome to be unpredictable
        - Relaxed Consistency Models: The Basics and Release Consistency
            - Goal: Allow read and writes to complete out of order, but use synchronization operations to enforce ordering so that a synchronized pogram behaves as though the processor were sequential
            - Total Store Ordering/Processor Consistency - relaxing the W -> R
            - Partial Store Order - relaxing both the W -> R and W -> W
            - Release Consistency + Weak Ordering - relaxing W -> R, W -> W, R -> R, R -> W
    g. Cross-Cutting Issues
        - Compiler Optimzations and the Consistency Model
        - Using Speculation to Hide Latency in Strict Consistency Models
        - Inclusion and Its Implementation
    h. Fallacies and Pitfalls
        - P: Measuring performance of multiprocessors by linear speedup versus execution time
        - F: Amdahl's Law doesn't apply to parallel computers
        - F: Linear speedups are needed to make multiprocessors cost-effective
        - P: Not developing the software to take advantage of, or optimize for, a multiprocessor architecture

Chapter 6 - Warehouse-Scale Computers to Exploit Requet-Level and Data-Level Parallelism
    a. Introduction
        - WSC: Foundation of Internet services billions of people use every day
        - Goals
            - Cost-Performance
            - Energy Efficiency
            - Dependability via Redundancy
            - Network I/O
            - Interactive and Batch Processing Workloads
        - Differences with Server Architecture
            - Ample Parallelism
            - Operational Costs Count
            - Location Counts
            - Computing Efficiently at Low Utilization
            - Scale and the Opportunities/Problems Associated with Scale
    b. Programming Models and Workloads for WSC
        - MapReduce
            - Map - applies a programmer-supplied function to each logical input record
            - Reduce - collects the output of those distributed tasks and collapses them using another programmer-defined function
        - Scalable Storage Systems
            - Google File System
            - Colossus
            - Dynamo
            - BigTable
    c. Computer Architecture of WSC
        - Components
            - Rack - structure that holds the servers
            - Top of Rack Switch - network cable switch
            - Rack to Rack Switch
            - Cooling Systems (Fan and Water)
            - Power Systems AC Conversion
            - Networking Systems
        - Storage
            - Disks on Rack
            - Network-Attached Storage
        - Memory Hierarchy
            - Memory, Flash, and Disk
    d. Efficiency and Cost of WSC
        - Power Distribution
        - Cooling
            - Computer Room Air-Conditioning CRAC
        - Measuring Efficiency
            - Power Utilization Effectiveness
        - Costs of a WSC
            - Operational Expenditures OPEX
            - Capital Expenditures CAPEX
            - Cost per Watt per Year < $2
    e. Cloud Computing - The Return of Utility Computing
        - Advantages
            - Reduction in Storage Costs
            - Reduction in Administrative Costs
            - Reduction in Networking Costs
            - Data Potentially Safer in the Cloud
        - Amazon Web Services Example
            - Amazon Simple Storage Service
            - Amazon Elastic ComputerCloud
            - Smart Choices
                - Virtual Machines, Low Cost, Open Source Software, No Initial Guarantee of Service, No Contract Required
    f. Google Example
        - Power Distribution
            - High Voltage Lines - 110K AC V to Substations - 10K-35K V to Building Unit - 400-480 V to Indoor Power Cables - 240-277 V to Rack Power Converters -  48 V DC to Boards
        - Cooling
            - Run at Slightly Higher Temperatures 80F
            - Hot and Cold Air Aisles
            - Internal Fans and Large-Scale Fan-Coils
            - Cool Water Pipes
            - Cooling Towers
        - Racks
            - Network Swtiches, Power Conversion, Payload Bay, and Batter Backup
        - Networking
            - Clos Topology
    g. Fallacies and Pitfalls
        - F: Cloud computing providers are losing money
        - P: Focusing on average performance instead of 99th percentile performance
        - P: Using too wimpy a processor when trying to improve WSC cost-performance
        - P: Inconsistent measure of PUE by different companies
        - F: Capital costs of the WSC facility are higher than for the servers that it houses
        - P: Trying to save power with inactive low power modes versus active low power modes
        - F: Given improvements in DRAM and the fault tolerance of WSC systems software, there is no need to spend extra for ECC memory in a WSC
        - P: Coping effectively with microsecond delays as opposed to nanosecond or millisecond delays
        - F: Turning off hardware during periods of low activity improves cost-performance of a WSC

Chapter 7 - Doman Specific Architectures
    a. Guidelines for DSA
        - Use Dedicated Memories to Minimize the Distance Ocer Which Data is Moved
        - Invest the Resources Saved from Dropping Advanced Microarchtiectural Optimizations into more Arithmetic Units or Bigger Memories
        - Use the Easiest Form of Parallelism that Matches the Domain
        - Reduce Data Size and Type to the Simplest Needed for the Domain
        - Use a Domain-Specific Programming Language to Port Code to the DSA
    b. Deep Neural Networks
        - Machine Learning - Deep Neural Networks
        - Neuron - computes the sum over a set of products of weights or parameters and data values that is then put through a nonlinear function to determine its output
        - Rectified Linear Unit or ReLU - f(x) = max(x, 0)
        - Activation - output of nonlinear function
        - Operational Intensity - Operations per Weight
        - Traning vs. Inference
            - Training or Learning - the development phaes where the goal is to learn the wieghts associated with each edge in the neural network graph
            - Production - inference, predicting, scoring, implementation, evaluation, running, or testing
            - Supervised Learning - given a set of training data to learn from where the data is preprocessed to have correct labels
        - Popular DNNs
            - MultiLayer Perceptrons MLP
            - Convolutional Neural Networks CNN
                - Computer Vision
            - Recurrent Neural Networks RNN
                - Speech Recognition and Language Translation
                - Long Short-Term Memory LSTM
        - Batches - reuse the weights ocne they have been fetched from memory across a set of inputs
        - Quantization - conversion between fixed point and floating point for the inference phase
    c. Google's Tensor Processing Unit
        - Custom Application Specific Integrated Circuit ASIC
        - TPU Architecture
            - Coprocessor on the PCIe I/O Bus
            - Host server sends instruction over the PCIe bus directly to the TPU
            - Matrix Multiply Unit - ALUs to perform 8-bit multiple-and-adds on signed or unsigned integers
            - Accumulator - collects 16-bit products
        - TPU Instruction Set Architecture
            - Read_Host_Memory, Read_Weights, MatrixMultiple/Convolve, Activate, Write_Host_Memory
        - TPU Microarchitecture
            - Systolic Array - a two-dimensional collection of arithmetic units that each independently compute a partial result as a function of inputs from other arithmetic units that are considered upstream to each unit
        - TPU Software
            - TensorFlow
            - User Space Driver and Kernel Driver
    d. Microsoft Catapult
        - Data Center Accelerator
        - Hardware-Description Language Verilog or VHDL
        - Register Transfer Level RTL - shell (system library) and role (application logic)
        - Search Acceleration
            - Feature Extraction, Free-Form Expressions, Machine-Learned Scoring
            - 1 FGPA FE, 2 FPGA FFE, 1 FGPA Compression Stage, 3 FGPA Machine-Learned Scoring
        - Feature Functional Unit - collection of finite state machines that measure standard features in Search
        - Dynamic Programming Feature - creates a Microsoft proprietary set of features using dynamic programming
    e. Intel Crest
        - Data Center Accelerator for Training
    f. Pixel Visual Core
        - PMD DSA for Image Processing and Computer Vision
        - Sytem on a Chip SOC
        - Image Processing Units IPUs - analyze and modify an input image in contract to generating an output image
        - Two-Dimensional Array of Processing Elements - makes it easy to perform stencil computations
        - Image Signal Processor ISP - hardwired accelerator for enhancing input images
        - Pxiel Visual Core Software
            - Directed Acyclic Graph DAG of Kernels
        - Architecture Philosophy
            - Two-Dimensional > One-Dimensional
            - Closer > Farther
        - Halo - extended region with simplified PEs that leave out the ALUs
        - PE Array or Vector Array - main computation unit
        - Sheet Generator SHG - load-store unit to access blocks in memory of size 1x1 to 256x256
        - Scalar Lane SCL - vector lane tha adds instructions to handle jumps, branches, and interrupts, controls instruction flow to the vector array, and schedules all the loads and store for the SHG
        - Instruction Set Architecture
            - Compile Halide -> vISA -> pISA
        - Two-Dimensional Line Buffer - minimizes DRAM access
    g. Cross-Cutting Issues
        - Heteogenity and System on a Chip SOC
            - Intellectual Property Block to be integrated into the SOC
        - Open Instruction Set
            - RISC-V a viable free and open instruction set
    h. Fallacies and Pitfalls
        - F: It costs $100 million to design a custom chip
        - P: Performance counters added as an afterthought for DSA hardware
        - F: Architects tackling the right DNN tasks
        - F: For DNN hardware, inferences per second IPS is a fair summary performance metric
        - F: Being ignorant of architecture history when designing a DSA

Appendix A - Instruction Set Principles
    a. Classifying Instruction Set Architectures
        - Stack
        - Accumulator
        - Registers
            - Register-Memory - acccess memory as part of any instruction
            - Load-Store - access memory only with load and store instructions
            - Memory-Memory - keeps all operands in memory
        - General Purpose Register
            - Faster than memory and more efficient for a compiler to user
            - Two or Three Operands ALU
            - Zero to Three Memory Operands
    b. Memory Addressing
        - Little Endian - puts the byte whose address is "x ... x000" at the least significant position in the double world
        - Big Endian - puts the byte whose address is "x ... x000" at the most significant position in the double world
        - Addressing Modes (PC Relative Addressing)
            - Register, Immediate, Displacement, Register Indirect, Indexed, Direct or Absolute, Memory Indirect, Autoincrement, Autodecrement, Scaled
        - Displacement Addressing Mode
        - Immediate or Literal Addressing Mode
    c. Types and Size of Operands
        - Character 8 bits
        - Half Word 16 bits
        - Word 32 bits
        - Single-Precision Floating Point
        - Double-Precision Floating Point
        - Two's Complement Binary Numbers
        - Packed Decimal or Binary-Coded Decimal
    d. Operations in the Instruction Set
        - Arithmetic and Logical, Data Transfer, Control, System, Floating Point, Decimal, String, Graphics
    e. Instruction for Control Flow
        - Conditional Branches
            - Condition Code
            - Condition Register/Limited Comparison
            - Compare and Branch 
        - Jumps
            - Case or Switch, Virtual Function or Methods, High-Order Functions, Dynamically Shared Libraries
        - Procedure Calls
        - Procedure Returns
    f. Encoding an Instruction Set
        - Variable - allows virtually all addressing modes to be with all operations
        - Fixed - combines the operation and the addressing mode into the opcode
        - Hybrid - reduce the variability in size and provide multiple instruction lengths to reduce code size
    g. Cross-Cutting Issues: Role of Compilers
        - Instruction Set Architecture is a compile target
        - Goals - Correctness and Speed
        - Structure of Recent Compilers
            - Front end per Language
            - High-level Optimizations
            - Global Optimizer
            - Code Generator
        - Optimizations
            - High-Level
            - Local
            - Global
            - Register Allocation
            - Process-Dependent
        - Register Allocation
            - Graph Coloring - construct a graph representing the possible candidates for allocation to a register and then to use the graph to allocate registers
        - Architect Helping the Compiler Write
            - Provide Regularity
            - Provide Primitives
            - Simplify Trade-Offs Among Alternatives
            - Provide Instruction that Bind the Quantities Known at Compile Time as Constants
    h. Fallacies and Pitfalls
        - P: Designing a "high-level" instruction set feature specifically oriented to supporting a high-level language structure
        - F: There is such a thing as a typical program
        - P: Innovating at the instruction set architecture to reduce code size without accounting for the compiler
        - F: An architecture with flaws cannot be successful
        - F: You can design a flawless architecture
