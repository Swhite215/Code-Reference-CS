Title: Computer Architecture: A Quantitative Approach
Authors: John L. Hennessy and David A. Patterson

Summarization of Content

1. Chapter 1 - Fundamentals of Quantitative Design and Analysis
    a. Classes of Computers
        - Personal/Mobile Devices PMD
        - Desktop
        - Server
        - Warehouse
        - IoT/Embedded
    b. Parallelism
        - Data-Level Parallelism
            - Instruction Level Parallelism
            - Vector Architectures, Graphic Processor Units, and Multimedia Instruction Sets
            - Thread Level Parallelism
        - Task-Level Parallelism 
            - Thread Level Parallelism
            - Request Level Parallelism
        - Categories
            - Single Instruction, Single Data Stream SISD
            - Single Instruction, Multiple Data Stream SIMD
            - Multiple Instruction, Singel Data Stream MISD
            - Multiple Instruction, Multiple Data Stream MIMD
    c. Instruction Set Architecture Dimensions
        - Class - General Purpose Registers
        - Memory Addressing
        - Addresing Modes
        - Types and Sizes of Operands
        - Operations
        - Control Flow Instructions
        - Encoding an ISA
    d. Trends in Technology
        - Integrated Circuit Technology
        - Semiconductor DRAM
        - Semiconductor Flash
        - Magnetic Disks
    e. Performance Trends
        - Bandwidth or Throughput
        - Latency or Response Time
    f. Dependability
        - Mean Time to Failure
        - Mean Time Between Failures
        - Mean Time to Recover
    g. Benchmarks
        - Electronic Design News Embedded Microprocessor Benchmark Consortium
        - Standard Performance Evaluation Corporation
        - Transaction Processing Council
    h. Quantitative Principles of Computer Design
        - Parallelism
        - Locality
        - Common Case
        - Amdahl's Law
        - Processor Performance Equation
    i. Fallacies and Pitfalls
        - P: All exponential laws must come to an end.
        - F: Multiprocessors are a silver bullet.
        - P: Falling prey to Amdahl's Law.
        - P: A single point of failure.
        - F: Hardware enhancements that increase performance also improve energy efficiency, or at worst are energy neutral.
        - F: Benchmarks remain valid indefinitely.
        - F: Disks never fail.
        - F: Peak performance tracks observed performance.

2. Chapter 2 - Memory Hierarchy Design
    a. Memory Technology
        - Primary Memory
            - L1, L2, L3 Cache
        - Secondary Memory
            - HDD
            - SDD
            - Optical Drives CD/DVD
        - Caches Misses
            - Compulsory, Capacity, Conflict
        - Six Basic Cache Optimizations
            - Larger Block Size to Reduce Miss Rate
            - Bigger Caches to Reduce Miss Rate
            - Higher Associativity to Reduce Miss Rate
            - Multilevel Caches to Reduce Miss Penalty
            - Priority to Read Misses Over Writes to Reduce Miss Penalty
            - Avoiding Address Translation During Indexing of Cache to Reduce Hit Time
    b. Memory Technology and Optimizations
        - RAM
            - Dynamic RAM - DRAM
            - Static RAM - SRAM
            - Synchronous DRAM - SDRAM
            - Double Data Rate - DDR
            - Graphics SDRAM - GSDRAM
        - Flash Memory
            - EEPROM
        - Phase-Change Memory Technology
    c. Ten Advanced Optimizations of Cache Performance
        - Categories
            - Reducing the Hit Time
                - Small and Simple First Level Caches
                - Way Prediction
            - Increasing Cache Bandwidthq
                - Pipelined and Multibanked Caches
                - Non-Blocking Caches
            - Reducing the Miss Penalty
                - Critical Word First and Early Restart
                - Merging Write Buffer
            - Reducing the Miss Rate
                - Compiler Optimizations
            - Reducing the Miss Penalty or Miss Rate Via Parallelism
                - Hardware Prefetching
                - Compile Prefetching
            - Miscellaneous
                - High-Bandwidth Memory 
    d. Virtual Memory and Virtual Machines
        - Virtual Machine Monitor
            - Provides an environment for programs which is essentially identical with the original machine
            - Programs run in this environment show at worst only minor decreases in speed
            - VMM is in complete control of system resources
        - Protection via Virtual Memory
            - Kernel and User Mode
            - User Process Use w/ supervisor mode bit, exception enable/disable bit, and memory protection information
            - Mechanism to go to supervisor mode and back to user mode
            - Mechanisms to limit memory access without having to swap the process on disk on a context switch
        - Protection Via Virtual Machines
            - Increase importance of isolation and security
            - Failures in security and reliability of standard operating systems
            - Sharing of a single computer among unrelated users
            - Dramatic increases in the raw speed of processors
        - VM
            - All emulation methods that provide a standard software interface
            - Interest: VMs that provide a complete system-level environment at the binary instruction set architecture level
        - VM Benefits
            - Managing Software and Hardware
        - VMM Requirements
            - Guest hardware should behave on a VM exactly as if it were running on the native hardware
            - Guest software should not be able to directly change allocation of real system resources
    e. Pitfalls and Fallacies
        - F: Predicting Cache Performance of One Program from Another
        - P: Simulating Enough Instructions to Get Accurate Performance Measure of the Memory Hierarchy
        - P: Not Delivering High Memory Bandwidth in a Cache-Based System
        - P: Implementing a Virtual Machine Monitor on an Instruction Set Architecture that was not designed to be Virtualizable
3. Chapter 3 - Instruction Level Parallelism
    a. Instruction Level Parallelism: Concepts and Challenges
        - Metric - Cycles Per Instruction
            - Pipeline CPI = Ideal Pipeline CPI + Structural Stalls + Data Hazard Stalls + Control Stalls
        - Approaches
            - Hardware to Exploit ILP Dynamically
            - Software to Exploit ILP Statically at Compile Time
        - Dependences (Data, Name, Control)
            - Data Dependences: Instruction is dependent on result or of an instruction dependent on another
            - Name Dependences: Two instructions use the same register or memory location
            - Control Dependences: Instruction should be executed in correct order
        - Data Hazards
            - WAW Write After Write
            - RAW Read After Write
            - WAR Write After Read
    b. Basic Compiler Techniques for Exposing ILP
        - Pipeline Scheduling and Loop Unrolling
            - Possible if all of a loops iteration are independent
    c. Reducing Branch Costs with Advanced Branch Prediction
        - Correlating Branch Predictors - Look at recent behavior of other branches
        - Tournament Predictors - Use of Global (most recent branch history) and Local Predictor (address of branch)
        - Tagged Hybrid Predictors - Prediction tables with varying lengths and tags
    d. Overcoming Data Hazards with Dynamic Scheduling
        - Hardwre reorders the instruction execution to reduce the stalls while maintaing data flow and exception behavior
        - Advantages: 
            - Code compiled with one pipeline in mind to run efficiently on a different pipeline
            - Enables handling some cases where dependences are unknown at compile time
            - Allows the processor to tolerate unpredictable delays such as cache misses
        - Out-Of-Order - Execution as ssoon as instruction and data operands are available
        - Tomasulo's Approach
            - Issue - Get instruction FIFO order, issue instruction to reservation station, rename registers (Avoids WAW and WAR)
            - Execute - Place operands into any awaiting reservation station, execute operation when ready (Avoids RAW)
            - Write Result - Write result on Common Data Bus to registers and into any reservation stations awaiting results, stores are buffered until value and address are available
    e. Dynamic Scheduling: Examples and Algorithms
        - Tomasulo's Advantages
            - Distribution of Hazard Detection Logic and Elimination fo Stalls for WAR and WAW hazards
    f. Hardware-Based Speculation
        - Speculation - To address control dependences - fetch, issue, and execute
        - Key Ideas
            - Dynamic Branch Prediction
            - Speculation - to allow the execution of instructions before control dependences are resolved
            - Dynamic Scheduling
        - Steps in Instruciton Execution
            - Issue - Get an instruction from queue
            - Execute - Execute when both operands are available
            - Write Result - When available, write result to CDB and then into Read Only Buffer and any reservation stations
            - Commit
    g. Exploiting ILP Using Multiple-Issue and Static Scheduling
    h. Exploiting ILP Using Dynamic Scheduling, Multiple Issue, and Speculation
    i. Advanced Techniques for Instruction Delivery and Speculation
    j. Cross-Cutting Issues
    k. Multithreading: Exploiting TLP
    l. Fallacies and Pitfalls
